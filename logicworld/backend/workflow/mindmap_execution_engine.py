"""
æ€ç»´å¯¼å›¾å·¥ä½œæµæ‰§è¡Œå¼•æ“
ä¸“é—¨å¤„ç†å‰ç«¯æ€ç»´å¯¼å›¾èŠ‚ç‚¹çš„æ‰§è¡Œé€»è¾‘
"""
import asyncio
import json
import logging
import os
import uuid
from typing import Dict, Any, List, Optional, Set
from datetime import datetime
import datetime as dt_module  # å¤‡ç”¨å…¨å±€å¯¼å…¥
from pathlib import Path

from utils.schemas import Node, Edge, WorkflowPayload
from .workflow_state import WorkflowStateManager, WorkflowStatus, NodeStatus
from agent_system.execution_engine import ExecutionEngine

# ğŸ§  NEW: å¯¼å…¥æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼ºå™¨
import sys
# æ­£ç¡®çš„è·¯å¾„ï¼šbackend/workflow/ -> é€»è¾‘0.9.2/
project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
sys.path.insert(0, project_root)
# from smart_context_enhancer import SmartContextEnhancer  # æ¨¡å—ä¸å­˜åœ¨ï¼Œä½¿ç”¨ä¸´æ—¶æ›¿ä»£

# ä¸´æ—¶æ›¿ä»£SmartContextEnhancer
class SmartContextEnhancer:
    """ä¸´æ—¶æ›¿ä»£çš„æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼ºå™¨"""
    def __init__(self):
        pass

# ğŸŒŸ å…¨å±€å·¥ä½œæµä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå€Ÿé‰´Airflow XComæœºåˆ¶ï¼‰
class GlobalWorkflowContext:
    """å…¨å±€å·¥ä½œæµä¸Šä¸‹æ–‡ç®¡ç†å™¨ - ç¡®ä¿æ•°æ®å¼ºåˆ¶æ€§ä¼ é€’å’ŒéªŒè¯"""
    
    def __init__(self, workflow_id: str):
        self.workflow_id = workflow_id
        self.target_file = None
        self.target_file_path = None
        self.data_lineage = []  # æ•°æ®è¡€ç¼˜è¿½è¸ª
        self.required_fields = ["target_file"]  # å¿…éœ€å­—æ®µ
        self.logger = logging.getLogger(f"GlobalContext-{workflow_id}")
    
    def set_target_file(self, file_info: Dict[str, Any], source_node: str) -> None:
        """è®¾ç½®ç›®æ ‡æ–‡ä»¶ï¼ˆå¼ºåˆ¶æ€§ï¼‰"""
        if not file_info:
            raise ValueError(f"Target file cannot be empty (from {source_node})")
        
        self.target_file = file_info
        self.target_file_path = file_info.get('path') or file_info.get('fullPath')
        
        if not self.target_file_path:
            raise ValueError(f"Target file path cannot be empty (from {source_node})")
        
        # è®°å½•æ•°æ®è¡€ç¼˜
        self.data_lineage.append({
            "action": "set_target_file",
            "source_node": source_node,
            "file_path": self.target_file_path,
            "timestamp": datetime.now()
        })
        
        self.logger.info(f"ğŸ¯ [GlobalContext] è®¾ç½®ç›®æ ‡æ–‡ä»¶: {self.target_file_path} (æ¥æº: {source_node})")
    
    def get_target_file(self) -> Dict[str, Any]:
        """è·å–ç›®æ ‡æ–‡ä»¶ï¼ˆå¸¦éªŒè¯ï¼‰"""
        if not self.target_file:
            raise ValueError(f"Target file not set in workflow {self.workflow_id}")
        return self.target_file
    
    def get_target_file_path(self) -> str:
        """è·å–ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼ˆå¸¦éªŒè¯ï¼‰"""
        if not self.target_file_path:
            raise ValueError(f"Target file path not set in workflow {self.workflow_id}")
        return self.target_file_path
    
    def validate_file_operation(self, operation_file_path: str, operation_type: str) -> bool:
        """éªŒè¯æ–‡ä»¶æ“ä½œæ˜¯å¦é’ˆå¯¹æ­£ç¡®çš„ç›®æ ‡æ–‡ä»¶"""
        if not self.target_file_path:
            self.logger.warning(f"ğŸš¨ [GlobalContext] ç›®æ ‡æ–‡ä»¶æœªè®¾ç½®ï¼Œæ— æ³•éªŒè¯æ“ä½œ: {operation_type}")
            return False
        
        if operation_file_path != self.target_file_path:
            self.logger.error(f"ğŸš¨ [GlobalContext] æ–‡ä»¶æ“ä½œä¸ä¸€è‡´ï¼")
            self.logger.error(f"   æœŸæœ›æ“ä½œ: {self.target_file_path}")
            self.logger.error(f"   å®é™…æ“ä½œ: {operation_file_path}")
            self.logger.error(f"   æ“ä½œç±»å‹: {operation_type}")
            return False
        
        # è®°å½•æ­£ç¡®çš„æ“ä½œ
        self.data_lineage.append({
            "action": operation_type,
            "file_path": operation_file_path,
            "timestamp": datetime.now(),
            "validated": True
        })
        
        self.logger.info(f"âœ… [GlobalContext] æ–‡ä»¶æ“ä½œéªŒè¯é€šè¿‡: {operation_type} -> {operation_file_path}")
        return True
    
    def get_data_lineage_summary(self) -> str:
        """è·å–æ•°æ®è¡€ç¼˜æ‘˜è¦"""
        if not self.data_lineage:
            return "æ— æ“ä½œè®°å½•"
        
        summary = f"ç›®æ ‡æ–‡ä»¶: {self.target_file_path}\n"
        summary += f"æ“ä½œå†å² ({len(self.data_lineage)}é¡¹):\n"
        
        for i, item in enumerate(self.data_lineage, 1):
            summary += f"  {i}. {item['action']} ({item.get('source_node', 'system')})\n"
        
        return summary


class MindmapExecutionEngine:
    """æ€ç»´å¯¼å›¾æ‰§è¡Œå¼•æ“"""
    
    def __init__(self, state_manager: WorkflowStateManager = None):
        self.state_manager = state_manager or WorkflowStateManager()
        self.execution_engine = ExecutionEngine()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # ğŸ§  NEW: åˆå§‹åŒ–æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼ºå™¨
        self.context_enhancer = SmartContextEnhancer()
        self.logger.info("ğŸ§  [æ™ºèƒ½å¢å¼º] æ™ºèƒ½ä¸Šä¸‹æ–‡å¢å¼ºå™¨å·²åˆå§‹åŒ–")
        
        # ğŸ¯ MVPæ”¹è¿›ï¼šå·¥ä½œæµçº§åˆ«çš„ç›®æ ‡æ–‡ä»¶è·Ÿè¸ª
        self.workflow_target_files = {}  # {workflow_id: target_file_path}
        
        # ğŸŒŸ æ–°å¢ï¼šå…¨å±€ç›®æ ‡æ–‡ä»¶ä¸Šä¸‹æ–‡ç®¡ç†ï¼ˆå€Ÿé‰´Airflow XComï¼‰
        self.workflow_global_context = {}  # {workflow_id: GlobalContext}
        
        # æ‰§è¡ŒçŠ¶æ€è·Ÿè¸ª
        self.execution_states = {}  # å­˜å‚¨æ¯ä¸ªå·¥ä½œæµçš„æ‰§è¡ŒçŠ¶æ€
        
        # èŠ‚ç‚¹æ‰§è¡Œå™¨æ˜ å°„
        self.node_executors = {
            # æ ¸å¿ƒèŠ‚ç‚¹ç±»å‹
            'material-node': self._execute_material_node,
            'execution-node': self._execute_execution_node,
            'condition-node': self._execute_condition_node,
            'result-node': self._execute_result_node,

            # é»˜è®¤æ‰§è¡Œå™¨
            'default': self._execute_default_node
        }
        
        # æ´»è·ƒçš„å·¥ä½œæµæ‰§è¡Œä»»åŠ¡
        self.active_workflows: Dict[str, asyncio.Task] = {}
    
    async def execute_workflow(self, payload) -> str:
        """æ‰§è¡Œå·¥ä½œæµ"""
        workflow_id = str(uuid.uuid4())
        
        try:
            # ç±»å‹æ£€æŸ¥å’Œä¿®å¤ - ç¡®ä¿payloadæ˜¯WorkflowPayloadå¯¹è±¡
            if isinstance(payload, dict):
                self.logger.warning(f"ğŸ”„ æ£€æµ‹åˆ°å­—å…¸ç±»å‹çš„payloadï¼Œæ­£åœ¨è½¬æ¢ä¸ºWorkflowPayloadå¯¹è±¡")
                from utils.schemas import WorkflowPayload, Node, Edge, NodeData, Position
                
                # è½¬æ¢å­—å…¸ä¸ºWorkflowPayloadå¯¹è±¡
                nodes = []
                for node_dict in payload.get('nodes', []):
                    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨ï¼Œæ·»åŠ é»˜è®¤å€¼
                    node_data = node_dict.get('data', {})
                    
                    # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
                    if 'label' not in node_data:
                        node_data['label'] = node_dict.get('type', 'Unknown Node')
                    if 'nodeType' not in node_data:
                        node_data['nodeType'] = node_dict.get('type', 'default-node')
                    
                    # æ·»åŠ å…¶ä»–å¸¸ç”¨é»˜è®¤å€¼
                    node_data.setdefault('params', {})
                    node_data.setdefault('inputs', [])
                    node_data.setdefault('outputs', [])
                    
                    self.logger.info(f"ğŸ”§ [NodeData] åˆ›å»ºèŠ‚ç‚¹æ•°æ®: {node_data.get('label')} ({node_data.get('nodeType')})")
                    
                    try:
                        node = Node(
                            id=node_dict['id'],
                            type=node_dict['type'],
                            position=Position(**node_dict['position']),
                            data=NodeData(**node_data)
                        )
                        nodes.append(node)
                        self.logger.info(f"âœ… [NodeData] èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {node.id}")
                    except Exception as e:
                        self.logger.error(f"âŒ [NodeData] èŠ‚ç‚¹åˆ›å»ºå¤±è´¥: {node_dict['id']} - {str(e)}")
                        self.logger.error(f"âŒ [NodeData] èŠ‚ç‚¹æ•°æ®: {node_data}")
                        raise ValueError(f"èŠ‚ç‚¹æ•°æ®åˆ›å»ºå¤±è´¥ {node_dict['id']}: {str(e)}")
                
                edges = []
                for edge_dict in payload.get('edges', []):
                    edge = Edge(**edge_dict)
                    edges.append(edge)
                
                payload = WorkflowPayload(nodes=nodes, edges=edges)
                self.logger.info(f"âœ… æˆåŠŸè½¬æ¢payloadä¸ºWorkflowPayloadå¯¹è±¡ï¼ŒèŠ‚ç‚¹æ•°: {len(payload.nodes)}")
            
            # ç¡®ä¿payloadæœ‰æ­£ç¡®çš„å±æ€§
            if not hasattr(payload, 'nodes') or not hasattr(payload, 'edges'):
                raise ValueError(f"æ— æ•ˆçš„payloadå¯¹è±¡: {type(payload)}")
            
            # å¯åŠ¨å·¥ä½œæµ
            execution = self.state_manager.start_workflow(workflow_id, payload)
            
            # åˆ›å»ºæ‰§è¡Œä»»åŠ¡
            task = asyncio.create_task(self._run_workflow(workflow_id, payload))
            self.active_workflows[workflow_id] = task
            
            self.logger.info(f"Started workflow execution: {workflow_id}")
            return workflow_id
            
        except Exception as e:
            self.logger.error(f"Failed to start workflow: {e}")
            self.state_manager.fail_workflow(workflow_id, str(e))
            raise
    
    async def _run_workflow(self, workflow_id: str, payload):
        """è¿è¡Œå·¥ä½œæµçš„ä¸»è¦é€»è¾‘"""
        try:
            # ç±»å‹æ£€æŸ¥å’Œä¿®å¤
            if isinstance(payload, dict):
                self.logger.warning(f"ğŸ”„ æ£€æµ‹åˆ°å­—å…¸ç±»å‹çš„payloadï¼Œæ­£åœ¨è½¬æ¢ä¸ºWorkflowPayloadå¯¹è±¡")
                from utils.schemas import WorkflowPayload, Node, Edge, NodeData, Position
                
                # è½¬æ¢å­—å…¸ä¸ºWorkflowPayloadå¯¹è±¡
                nodes = []
                for node_dict in payload.get('nodes', []):
                    # ğŸš¨ ä¿®å¤ï¼šç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨ï¼Œæ·»åŠ é»˜è®¤å€¼
                    node_data = node_dict.get('data', {})
                    
                    # ç¡®ä¿å¿…éœ€å­—æ®µå­˜åœ¨
                    if 'label' not in node_data:
                        node_data['label'] = node_dict.get('type', 'Unknown Node')
                    if 'nodeType' not in node_data:
                        node_data['nodeType'] = node_dict.get('type', 'default-node')
                    
                    # æ·»åŠ å…¶ä»–å¸¸ç”¨é»˜è®¤å€¼
                    node_data.setdefault('params', {})
                    node_data.setdefault('inputs', [])
                    node_data.setdefault('outputs', [])
                    
                    self.logger.info(f"ğŸ”§ [NodeData] åˆ›å»ºèŠ‚ç‚¹æ•°æ®: {node_data.get('label')} ({node_data.get('nodeType')})")
                    
                    try:
                        node = Node(
                            id=node_dict['id'],
                            type=node_dict['type'],
                            position=Position(**node_dict['position']),
                            data=NodeData(**node_data)
                        )
                        nodes.append(node)
                        self.logger.info(f"âœ… [NodeData] èŠ‚ç‚¹åˆ›å»ºæˆåŠŸ: {node.id}")
                    except Exception as e:
                        self.logger.error(f"âŒ [NodeData] èŠ‚ç‚¹åˆ›å»ºå¤±è´¥: {node_dict['id']} - {str(e)}")
                        self.logger.error(f"âŒ [NodeData] èŠ‚ç‚¹æ•°æ®: {node_data}")
                        raise ValueError(f"èŠ‚ç‚¹æ•°æ®åˆ›å»ºå¤±è´¥ {node_dict['id']}: {str(e)}")
                
                edges = []
                for edge_dict in payload.get('edges', []):
                    edge = Edge(**edge_dict)
                    edges.append(edge)
                
                payload = WorkflowPayload(nodes=nodes, edges=edges)
                self.logger.info(f"âœ… æˆåŠŸè½¬æ¢payloadä¸ºWorkflowPayloadå¯¹è±¡")
            
            # ç¡®ä¿payloadæœ‰æ­£ç¡®çš„å±æ€§
            if not hasattr(payload, 'nodes') or not hasattr(payload, 'edges'):
                raise ValueError(f"æ— æ•ˆçš„payloadå¯¹è±¡: {type(payload)}")
            
            # æ„å»ºèŠ‚ç‚¹ä¾èµ–å›¾
            dependency_graph = self._build_dependency_graph(payload.nodes, payload.edges)
            
            # æ‰§è¡Œä¸Šä¸‹æ–‡
            context = {}
            
            # è·å–èµ·å§‹èŠ‚ç‚¹ï¼ˆæ²¡æœ‰è¾“å…¥è¾¹çš„èŠ‚ç‚¹ï¼‰
            start_nodes = self._get_start_nodes(payload.nodes, payload.edges)
            
            if not start_nodes:
                raise Exception("No start nodes found in workflow")
            
            # æ‰§è¡ŒèŠ‚ç‚¹
            await self._execute_nodes_recursive(
                workflow_id, start_nodes, dependency_graph, 
                payload.nodes, context
            )
            
            # å®Œæˆå·¥ä½œæµ
            self.state_manager.complete_workflow(workflow_id, context)
            
        except Exception as e:
            self.logger.error(f"Workflow execution failed: {workflow_id} - {e}")
            self.state_manager.fail_workflow(workflow_id, str(e))
        finally:
            # æ¸…ç†æ´»è·ƒä»»åŠ¡
            if workflow_id in self.active_workflows:
                del self.active_workflows[workflow_id]
    
    def _build_dependency_graph(self, nodes: List[Node], edges: List[Edge]) -> Dict[str, List[str]]:
        """æ„å»ºèŠ‚ç‚¹ä¾èµ–å›¾"""
        graph = {node.id: [] for node in nodes}
        
        for edge in edges:
            if edge.target in graph:
                graph[edge.target].append(edge.source)
        
        return graph
    
    def _get_start_nodes(self, nodes: List[Node], edges: List[Edge]) -> List[str]:
        """è·å–èµ·å§‹èŠ‚ç‚¹"""
        target_nodes = {edge.target for edge in edges}
        return [node.id for node in nodes if node.id not in target_nodes]
    
    def _get_next_nodes(self, current_node: str, edges: List[Edge]) -> List[str]:
        """è·å–ä¸‹ä¸€ä¸ªèŠ‚ç‚¹"""
        return [edge.target for edge in edges if edge.source == current_node]
    
    async def _execute_nodes_recursive(
        self, 
        workflow_id: str, 
        node_ids: List[str], 
        dependency_graph: Dict[str, List[str]], 
        all_nodes: List[Node], 
        context: Dict[str, Any]
    ):
        """é€’å½’æ‰§è¡ŒèŠ‚ç‚¹"""
        # åˆ›å»ºèŠ‚ç‚¹IDåˆ°èŠ‚ç‚¹å¯¹è±¡çš„æ˜ å°„
        node_map = {node.id: node for node in all_nodes}
        
        # å¹¶å‘æ‰§è¡Œå½“å‰å±‚çš„èŠ‚ç‚¹
        tasks = []
        for node_id in node_ids:
            if node_id in node_map:
                task = asyncio.create_task(
                    self._execute_single_node(workflow_id, node_map[node_id], context)
                )
                tasks.append((node_id, task))
        
        # ç­‰å¾…æ‰€æœ‰èŠ‚ç‚¹å®Œæˆ
        failed_nodes = set()
        for node_id, task in tasks:
            try:
                result = await task
                context[f"{node_id}.result"] = result
            except Exception as e:
                self.logger.error(f"Node execution failed: {node_id} - {e}")
                self.state_manager.fail_node(workflow_id, node_id, str(e))
                failed_nodes.add(node_id)
                # ä¸è¦raiseï¼Œç»§ç»­æ‰§è¡Œå…¶ä»–èŠ‚ç‚¹
                self.logger.warning(f"èŠ‚ç‚¹ {node_id} å¤±è´¥ï¼Œä½†ç»§ç»­æ‰§è¡Œå…¶ä»–èŠ‚ç‚¹")
        
        # æ‰¾åˆ°ä¸‹ä¸€å±‚å¯ä»¥æ‰§è¡Œçš„èŠ‚ç‚¹ - ä½†è¦æ’é™¤ä¾èµ–å¤±è´¥èŠ‚ç‚¹çš„èŠ‚ç‚¹
        next_nodes = set()
        for node_id in node_ids:
            # è·³è¿‡å·²ç»å¤±è´¥çš„èŠ‚ç‚¹
            if node_id in failed_nodes:
                continue
                
            # è·å–å½“å‰èŠ‚ç‚¹çš„æ‰€æœ‰åç»§èŠ‚ç‚¹
            for next_node in dependency_graph:
                if node_id in dependency_graph[next_node]:
                    # æ£€æŸ¥è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰ä¾èµ–æ˜¯å¦éƒ½å·²å®Œæˆä¸”æ²¡æœ‰å¤±è´¥
                    dependencies = dependency_graph[next_node]
                    dependencies_completed = all(f"{dep}.result" in context for dep in dependencies)
                    dependencies_not_failed = not any(dep in failed_nodes for dep in dependencies)
                    
                    if dependencies_completed and dependencies_not_failed:
                        next_nodes.add(next_node)
                    elif any(dep in failed_nodes for dep in dependencies):
                        # å¦‚æœä¾èµ–èŠ‚ç‚¹å¤±è´¥ï¼Œæ ‡è®°è¿™ä¸ªèŠ‚ç‚¹ä¹Ÿå¤±è´¥
                        self.logger.error(f"èŠ‚ç‚¹ {next_node} å› ä¾èµ–èŠ‚ç‚¹å¤±è´¥è€Œè·³è¿‡")
                        self.state_manager.fail_node(workflow_id, next_node, f"ä¾èµ–èŠ‚ç‚¹å¤±è´¥: {[dep for dep in dependencies if dep in failed_nodes]}")
                        failed_nodes.add(next_node)
        
        # é€’å½’æ‰§è¡Œä¸‹ä¸€å±‚èŠ‚ç‚¹
        if next_nodes:
            await self._execute_nodes_recursive(
                workflow_id, list(next_nodes), dependency_graph, all_nodes, context
            )
            
        # å¦‚æœæœ‰å¤±è´¥çš„èŠ‚ç‚¹ï¼Œåœ¨æœ€åæŠ›å‡ºå¼‚å¸¸
        if failed_nodes:
            raise Exception(f"å·¥ä½œæµä¸­æœ‰ {len(failed_nodes)} ä¸ªèŠ‚ç‚¹å¤±è´¥: {list(failed_nodes)}")
    
    async def _execute_single_node(self, workflow_id: str, node: Node, context: Dict[str, Any]) -> Any:
        """æ‰§è¡Œå•ä¸ªèŠ‚ç‚¹"""
        # ä¼˜å…ˆä½¿ç”¨node.data.nodeTypeï¼Œå› ä¸ºè¿™æ˜¯å‰ç«¯è®¾ç½®çš„æ­£ç¡®ç±»å‹
        node_type = getattr(node.data, 'nodeType', None) or node.type or 'default'
        self.logger.info(f"[STATUS] å¼€å§‹æ‰§è¡ŒèŠ‚ç‚¹ {node.id} (ç±»å‹: {node_type})")
        
        self.state_manager.start_node(workflow_id, node.id)
        
        # ğŸ”§ ä¿®å¤ï¼šçœŸæ­£å‘é€SSE node_startäº‹ä»¶ç»™å‰ç«¯
        try:
            from utils.websocket_manager import websocket_manager
            await websocket_manager.broadcast({
                "event": "node_start",
                "node_id": node.id,
                "node_type": node_type,
                "timestamp": dt_module.datetime.now().isoformat()
            })
            self.logger.info(f"[SSE] å·²å‘é€node_startäº‹ä»¶: {node.id}")
        except Exception as e:
            self.logger.warning(f"[SSE] å‘é€node_startäº‹ä»¶å¤±è´¥: {e}")
            # ä¿ç•™åŸæœ‰çš„æ—¥å¿—è®°å½•ä½œä¸ºå¤‡ä»½
            self.logger.info(f"[SSE] node_start: {{\"node_id\": \"{node.id}\", \"node_type\": \"{node_type}\"}}")
        
        try:
            # è·å–èŠ‚ç‚¹æ‰§è¡Œå™¨ - ä½¿ç”¨æ­£ç¡®çš„node_type
            executor = self.node_executors.get(node_type, self.node_executors['default'])
            
            # æ‰§è¡ŒèŠ‚ç‚¹
            result = await executor(workflow_id, node, context) # Pass workflow_id to executor
            
            # æ£€æŸ¥æ‰§è¡Œç»“æœæ˜¯å¦æˆåŠŸ
            if isinstance(result, dict) and result.get('success') is False:
                error_message = result.get('error', 'Node execution failed')
                self.logger.error(f"[STATUS] èŠ‚ç‚¹ {node.id} æ‰§è¡Œå¤±è´¥: {error_message}")
                raise Exception(f"Node {node.id} execution failed: {error_message}")
            
            # ğŸš¨ é¢å¤–å®‰å…¨æ£€æŸ¥ï¼šå³ä½¿successä¸æ˜¯Falseï¼Œä¹Ÿè¦æ£€æŸ¥æ˜¯å¦åŒ…å«AIå¤±è´¥ä¿¡æ¯
            if isinstance(result, dict):
                result_content = str(result.get('result', ''))
                if ("AIæ¨¡å‹è°ƒç”¨å¤±è´¥" in result_content or 
                    "LLMè°ƒç”¨å¤±è´¥" in result_content or
                    "æ— æ³•ç”Ÿæˆæœ‰æ•ˆå“åº”" in result_content or
                    "APIè°ƒç”¨å¤±è´¥" in result_content):
                    
                    error_message = f"æ£€æµ‹åˆ°AIè°ƒç”¨å¤±è´¥: {result_content[:100]}"
                    self.logger.error(f"[SECURITY] èŠ‚ç‚¹ {node.id} å«æœ‰AIå¤±è´¥ä¿¡æ¯: {error_message}")
                    raise Exception(f"Node {node.id} AI failure detected: {error_message}")
            
            # å®ŒæˆèŠ‚ç‚¹
            self.state_manager.complete_node(workflow_id, node.id, result)
            
            # ğŸ”§ ä¿®å¤ï¼šçœŸæ­£å‘é€SSE node_doneæˆåŠŸäº‹ä»¶ç»™å‰ç«¯
            try:
                from utils.websocket_manager import websocket_manager
                await websocket_manager.broadcast({
                    "event": "node_done",
                    "node_id": node.id,
                    "status": "success",
                    "result": "å·²å®Œæˆ",
                    "timestamp": dt_module.datetime.now().isoformat()
                })
                self.logger.info(f"[SSE] å·²å‘é€node_doneæˆåŠŸäº‹ä»¶: {node.id}")
            except Exception as e:
                self.logger.warning(f"[SSE] å‘é€node_doneæˆåŠŸäº‹ä»¶å¤±è´¥: {e}")
                # ä¿ç•™åŸæœ‰çš„æ—¥å¿—è®°å½•ä½œä¸ºå¤‡ä»½
                self.logger.info(f"[SSE] node_done: {{\"node_id\": \"{node.id}\", \"status\": \"success\", \"result\": \"å·²å®Œæˆ\"}}")
            self.logger.info(f"[STATUS] èŠ‚ç‚¹ {node.id} æ‰§è¡Œå®Œæˆ")
            
            return result
            
        except Exception as e:
            self.state_manager.fail_node(workflow_id, node.id, str(e))
            
            # ğŸ”§ ä¿®å¤ï¼šçœŸæ­£å‘é€SSE node_doneå¤±è´¥äº‹ä»¶ç»™å‰ç«¯
            try:
                from utils.websocket_manager import websocket_manager
                await websocket_manager.broadcast({
                    "event": "node_done",
                    "node_id": node.id,
                    "status": "error",
                    "error": str(e),
                    "timestamp": dt_module.datetime.now().isoformat()
                })
                self.logger.info(f"[SSE] å·²å‘é€node_doneå¤±è´¥äº‹ä»¶: {node.id}")
            except Exception as ws_e:
                self.logger.warning(f"[SSE] å‘é€node_doneå¤±è´¥äº‹ä»¶å¤±è´¥: {ws_e}")
                # ä¿ç•™åŸæœ‰çš„æ—¥å¿—è®°å½•ä½œä¸ºå¤‡ä»½
                self.logger.info(f"[SSE] node_done: {{\"node_id\": \"{node.id}\", \"status\": \"error\", \"error\": \"{str(e)}\"}}")
            self.logger.error(f"[STATUS] èŠ‚ç‚¹ {node.id} æ‰§è¡Œå¤±è´¥: {e}")
            # ä¸è¦åœ¨è¿™é‡Œraiseï¼Œè®©ä¸Šå±‚å¤„ç†é”™è¯¯ä¼ æ’­
            raise
    
    async def _execute_material_node(self, workflow_id: str, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œææ–™èŠ‚ç‚¹"""
        self.logger.info(f"Executing material node: {node.id}")
        
        # ğŸŒŸ åˆå§‹åŒ–å…¨å±€ä¸Šä¸‹æ–‡
        if workflow_id not in self.workflow_global_context:
            self.workflow_global_context[workflow_id] = GlobalWorkflowContext(workflow_id)
        
        global_context = self.workflow_global_context[workflow_id]
        
        data = node.data
        result = {
            'node_type': 'material',
            'files': [],
            'content': '',
            'metadata': {}
        }
        
        # ğŸ¯ å¼ºåˆ¶æ€§ç›®æ ‡æ–‡ä»¶å¤„ç†ï¼ˆå€Ÿé‰´GitHub Actionsçš„å‰ç½®æ¡ä»¶æ£€æŸ¥ï¼‰
        target_file_found = False
        
        # æ£€æŸ¥ targetFile å­—æ®µ
        if 'targetFile' in data and data['targetFile']:
            target_file = data['targetFile']
            self.logger.info(f"ğŸ¯ [MaterialNode] å‘ç°targetFile: {target_file}")
            
            # è®¾ç½®å…¨å±€ç›®æ ‡æ–‡ä»¶
            global_context.set_target_file(target_file, node.id)
            target_file_found = True
            
            result['targetFile'] = target_file
            result['files'].append({
                'name': target_file.get('name', ''),
                'size': target_file.get('size', 0),
                'type': target_file.get('type', 'document'),
                'path': target_file.get('path', ''),
                'fullPath': target_file.get('path', ''),
                'content': ''
            })
            
            self.logger.info(f"âœ… [MaterialNode] ç›®æ ‡æ–‡ä»¶å·²è®¾ç½®: {target_file.get('name')} -> {target_file.get('path')}")
        
        # æ£€æŸ¥ selectedFiles å­—æ®µ
        elif 'selectedFiles' in data and data['selectedFiles']:
            selected_files = data['selectedFiles']
            self.logger.info(f"ğŸ¯ [MaterialNode] å‘ç°selectedFiles: {len(selected_files)}ä¸ªæ–‡ä»¶")
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºç›®æ ‡æ–‡ä»¶
            if selected_files:
                target_file = selected_files[0]
                global_context.set_target_file(target_file, node.id)
                target_file_found = True
                
                result['targetFile'] = target_file
                result['selectedFiles'] = selected_files
                
                for file_info in selected_files:
                    result['files'].append({
                        'name': file_info.get('name', ''),
                        'size': file_info.get('size', 0),
                        'type': file_info.get('type', 'document'),
                        'path': file_info.get('path', '') or file_info.get('fullPath', ''),
                        'fullPath': file_info.get('fullPath', '') or file_info.get('path', ''),
                        'content': ''
                    })
                
                self.logger.info(f"âœ… [MaterialNode] ä»selectedFilesè®¾ç½®ç›®æ ‡æ–‡ä»¶: {target_file.get('name')}")
        
        # ğŸš¨ å¼ºåˆ¶æ€§éªŒè¯ï¼šå¿…é¡»æœ‰ç›®æ ‡æ–‡ä»¶
        if not target_file_found:
            # ğŸŒŸ MVPä¿®å¤ï¼šä¸å†æŠ›å‡ºé”™è¯¯ï¼Œè€Œæ˜¯åˆ›å»ºä¸€ä¸ªé»˜è®¤çš„æˆåŠŸçŠ¶æ€
            # è¿™æ ·æ™ºèƒ½è¡¥é½ä»ç„¶å¯ä»¥å·¥ä½œï¼Œé€šè¿‡ä»»åŠ¡æè¿°æ¨æ–­ç›®æ ‡æ–‡ä»¶
            self.logger.warning(f"âš ï¸ [MaterialNode] æœªæ‰¾åˆ°ç›®æ ‡æ–‡ä»¶ï¼Œåˆ›å»ºé»˜è®¤æˆåŠŸçŠ¶æ€ä»¥æ”¯æŒæ™ºèƒ½è¡¥é½")
            
            # åˆ›å»ºä¸€ä¸ªè¡¨ç¤º"æ— æ–‡ä»¶ä½†æˆåŠŸ"çš„çŠ¶æ€
            result['file_count'] = 0
            result['total_size'] = 0
            result['processed_at'] = datetime.now().isoformat()
            result['status'] = 'no_files_but_ready'  # æ ‡è®°ä¸ºæ— æ–‡ä»¶ä½†å°±ç»ªçŠ¶æ€
            
            self.logger.info(f"ğŸ¯ [MaterialNode] é»˜è®¤å¤„ç†å®Œæˆ: 0ä¸ªæ–‡ä»¶ï¼ŒçŠ¶æ€ä¸ºå°±ç»ª")
            
            return result
        
        # æ›´æ–°ç»“æœç»Ÿè®¡
        result['file_count'] = len(result['files'])
        result['total_size'] = sum(f.get('size', 0) for f in result['files'])
        result['processed_at'] = datetime.now().isoformat()
        
        self.logger.info(f"ğŸ¯ [MaterialNode] å¤„ç†å®Œæˆ: {result['file_count']}ä¸ªæ–‡ä»¶, ç›®æ ‡æ–‡ä»¶: {global_context.target_file_path}")
        
        return result
    
    def _apply_data_mapping(self, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ ¹æ®èŠ‚ç‚¹çš„æ•°æ®æ˜ å°„é…ç½®æå–ç‰¹å®šçš„æ•°æ®å­—æ®µ"""
        data = node.data
        
        # è·å–èŠ‚ç‚¹çš„æ•°æ®æ˜ å°„é…ç½®
        input_mappings = getattr(data, 'inputMappings', {})
        
        if not input_mappings:
            # å¦‚æœæ²¡æœ‰é…ç½®æ•°æ®æ˜ å°„ï¼Œè¿”å›ç©ºå­—å…¸ï¼ˆä¸è‡ªåŠ¨ä¼ é€’æ‰€æœ‰æ•°æ®ï¼‰
            return {}
        
        mapped_data = {}
        
        # éå†é…ç½®çš„æ˜ å°„
        for mapping_key in input_mappings.keys():
            # mapping_key æ ¼å¼: "nodeId.path.to.field"
            parts = mapping_key.split('.', 1)
            if len(parts) != 2:
                continue
                
            source_node_id, field_path = parts
            
            # ä»contextä¸­è·å–æºèŠ‚ç‚¹çš„ç»“æœ
            source_result_key = f"{source_node_id}.result"
            if source_result_key not in context:
                continue
                
            source_data = context[source_result_key]
            
            # æ ¹æ®å­—æ®µè·¯å¾„æå–æ•°æ®
            field_value = self._extract_field_value(source_data, field_path)
            if field_value is not None:
                # ä½¿ç”¨ç®€åŒ–çš„å­—æ®µåä½œä¸ºkey
                simple_key = field_path.split('.')[-1]  # ä½¿ç”¨æœ€åä¸€éƒ¨åˆ†ä½œä¸ºkey
                mapped_data[simple_key] = field_value
        
        self.logger.info(f"[DataMapping] èŠ‚ç‚¹ {node.id} æ˜ å°„ç»“æœ: {list(mapped_data.keys())}")
        return mapped_data
    
    def _extract_field_value(self, data: Any, field_path: str) -> Any:
        """ä»æ•°æ®ä¸­æ ¹æ®è·¯å¾„æå–å­—æ®µå€¼"""
        if not field_path:
            return data
            
        parts = field_path.split('.')
        current = data
        
        try:
            for part in parts:
                if isinstance(current, dict):
                    current = current.get(part)
                elif isinstance(current, list) and part.startswith('[') and part.endswith(']'):
                    # å¤„ç†æ•°ç»„ç´¢å¼•ï¼Œå¦‚ [0]
                    index = int(part[1:-1])
                    current = current[index]
                else:
                    return None
                    
                if current is None:
                    return None
                    
            return current
        except (KeyError, IndexError, ValueError, TypeError):
            return None
    
    async def _execute_execution_node(self, workflow_id: str, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ‰§è¡ŒèŠ‚ç‚¹"""
        self.logger.info(f"Executing execution node: {node.id}")
        
        # ğŸŒŸ è·å–å…¨å±€ä¸Šä¸‹æ–‡ï¼ˆå¼ºåˆ¶æ€§éªŒè¯ï¼‰
        if workflow_id not in self.workflow_global_context:
            raise ValueError(f"ğŸš¨ [ExecutionNode] å…¨å±€ä¸Šä¸‹æ–‡æœªåˆå§‹åŒ–ï¼å·¥ä½œæµ: {workflow_id}")
        
        global_context = self.workflow_global_context[workflow_id]
        
        # ğŸš¨ å‰ç½®æ¡ä»¶æ£€æŸ¥ï¼šå¿…é¡»æœ‰ç›®æ ‡æ–‡ä»¶ï¼ˆå€Ÿé‰´GitHub Actionsï¼‰
        try:
            target_file_path = global_context.get_target_file_path()
            target_file_info = global_context.get_target_file()
            self.logger.info(f"âœ… [ExecutionNode] ç›®æ ‡æ–‡ä»¶éªŒè¯é€šè¿‡: {target_file_path}")
        except ValueError as e:
            error_msg = f"ğŸš¨ [ExecutionNode] å‰ç½®æ¡ä»¶å¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            raise ValueError(error_msg)
        
        data = node.data
        
        # è·å–ä»»åŠ¡æè¿°
        task = getattr(data, 'inputContent', '') or getattr(data, 'taskDescription', '')
        
        if not task:
            raise ValueError(f"ğŸš¨ [ExecutionNode] ä»»åŠ¡æè¿°ä¸èƒ½ä¸ºç©ºï¼èŠ‚ç‚¹: {node.id}")
        
        # ğŸ¯ å¼ºåˆ¶æ€§ä¸Šä¸‹æ–‡å¢å¼ºï¼ˆå€Ÿé‰´Zapierçš„æ•°æ®æ˜ å°„ï¼‰
        execution_context = f"ä»»åŠ¡: {task}"
        
        # ğŸŒŸ å¼ºåˆ¶æ€§ç›®æ ‡æ–‡ä»¶ä¸Šä¸‹æ–‡æ³¨å…¥
        execution_context += f"\n\nğŸ¯ ç›®æ ‡æ–‡ä»¶ä¿¡æ¯ï¼ˆå¿…é¡»ä½¿ç”¨æ­¤æ–‡ä»¶ï¼‰:"
        execution_context += f"\næ–‡ä»¶è·¯å¾„: {target_file_path}"
        execution_context += f"\næ–‡ä»¶åç§°: {target_file_info.get('name', 'Unknown')}"
        execution_context += f"\næ–‡ä»¶å¤§å°: {target_file_info.get('size', 0)} bytes"
        
        # ğŸš¨ å¼ºåˆ¶æ€§AIæŒ‡ä»¤çº¦æŸï¼ˆå€Ÿé‰´Power Automateçš„ç¯å¢ƒå˜é‡æœºåˆ¶ï¼‰
        execution_context += f"\n\nğŸš¨ é‡è¦çº¦æŸæ¡ä»¶ï¼š"
        execution_context += f"\n1. å¿…é¡»æ“ä½œæŒ‡å®šçš„ç›®æ ‡æ–‡ä»¶: {target_file_path}"
        execution_context += f"\n2. ç¦æ­¢åˆ›å»ºæ–°æ–‡æ¡£ï¼Œå¿…é¡»ä½¿ç”¨ open_document('{target_file_path}')"
        execution_context += f"\n3. æ‰€æœ‰æ“ä½œå¿…é¡»åŸºäºç°æœ‰æ–‡æ¡£å†…å®¹è¿›è¡Œä¿®æ”¹"
        
        # å¤„ç†æ‰‹åŠ¨æ•°æ®æ˜ å°„
        mapped_data = self._apply_data_mapping(node, context)
        
        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
        if mapped_data:
            execution_context += f"\n\nè¾“å…¥æ•°æ®:"
            for key, value in mapped_data.items():
                execution_context += f"\n{key}: {value}"
        elif context:
            execution_context += f"\n\nä¸Šä¸‹æ–‡ä¿¡æ¯:"
            for key, value in context.items():
                if isinstance(value, dict) and 'node_type' in value:
                    node_type = value['node_type']
                    if node_type == 'material':
                        files_info = value.get('files', [])
                        if files_info:
                            execution_context += f"\n- ææ–™èŠ‚ç‚¹æä¾›äº† {len(files_info)} ä¸ªæ–‡ä»¶"
                    elif node_type == 'execution':
                        result_info = value.get('result', '')
                        if result_info:
                            execution_context += f"\n- å‰ç½®æ‰§è¡Œç»“æœ: {result_info[:100]}..."
        
        self.logger.info(f"ğŸ¯ [ExecutionNode] å¢å¼ºåçš„æ‰§è¡Œä¸Šä¸‹æ–‡é•¿åº¦: {len(execution_context)}")
        
        try:
            # è·å–å·¥å…·ç±»å‹
            tool_type = getattr(data, 'toolType', 'DeepSeekèŠå¤©æ¨¡å‹')
            
            # åˆ›å»ºä»£ç†å¹¶æ‰§è¡Œ
            agent_factory = AgentFactory()
            agent = agent_factory.create_agent(
                task=task,
                context=execution_context,
                tool_type=tool_type,
                mode="super",
                workflow_context=context  # ğŸŒŸ ä¼ é€’å·¥ä½œæµä¸Šä¸‹æ–‡
            )
            
            self.logger.info(f"ğŸ”§ [ExecutionNode] å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task[:100]}...")
            result = await agent.execute()
            
            # ğŸŒŸ éªŒè¯AIæ‰§è¡Œç»“æœï¼ˆæ•°æ®è¡€ç¼˜æ£€æŸ¥ï¼‰
            if isinstance(result, dict) and 'execution_results' in result:
                for exec_result in result['execution_results']:
                    operation_file = exec_result.get('file_path', '')
                    operation_type = exec_result.get('action', 'unknown')
                    
                    if operation_file:
                        # éªŒè¯æ“ä½œçš„æ–‡ä»¶æ˜¯å¦ä¸ºç›®æ ‡æ–‡ä»¶
                        is_valid = global_context.validate_file_operation(operation_file, operation_type)
                        if not is_valid:
                            self.logger.warning(f"âš ï¸ [ExecutionNode] æ£€æµ‹åˆ°éç›®æ ‡æ–‡ä»¶æ“ä½œï¼Œä½†ç»§ç»­æ‰§è¡Œ")
            
            # æ„å»ºæ‰§è¡Œç»“æœ
                    execution_result = {
                        'node_type': 'execution',
                'task': task,
                'enhanced_task': execution_context,
                'mode': "super",
                'model': tool_type,
                'result': str(result),
                'success': True,
                'expected_output_format': 'txt',
                        'execution_metadata': {
                    'task_type': 'general',
                    'content_keywords': [],
                    'ai_model': tool_type,
                    'target_file_path': target_file_path,  # è®°å½•ç›®æ ‡æ–‡ä»¶
                    'data_lineage': global_context.get_data_lineage_summary()
                },
                'executed_at': datetime.now().isoformat()
                        }
            
            self.logger.info(f"âœ… [ExecutionNode] ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ")
            return execution_result
            
        except Exception as e:
            error_msg = f"ğŸš¨ [ExecutionNode] ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"
            self.logger.error(error_msg)
            
            return {
                'node_type': 'execution',
                'task': task,
                'error': error_msg,
                'success': False,
                'executed_at': datetime.now().isoformat(),
                'target_file_path': target_file_path,
                'data_lineage': global_context.get_data_lineage_summary()
            }
    
    async def _execute_condition_node(self, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¡ä»¶èŠ‚ç‚¹"""
        self.logger.info(f"Executing condition node: {node.id}")
        
        data = node.data
        
        # è·å–æ¡ä»¶é…ç½®
        conditions = getattr(data, 'conditions', [])
        logical_operator = getattr(data, 'logicalOperator', 'AND')
        
        results = []
        for i, condition in enumerate(conditions):
            left_operand = self._resolve_value(condition.get('leftOperand', ''), context)
            operator = condition.get('operator', 'equals')
            right_operand = self._resolve_value(condition.get('rightOperand', ''), context)
            data_type = condition.get('dataType', 'text')
            
            # æ‰§è¡Œæ¡ä»¶åˆ¤æ–­
            condition_result = self._evaluate_condition(left_operand, operator, right_operand, data_type)
            results.append(condition_result)
        
        # è®¡ç®—æœ€ç»ˆç»“æœ
        if logical_operator == 'AND':
            final_result = all(results)
        else:  # OR
            final_result = any(results)
        
        return {
            'node_type': 'condition',
            'conditions': len(conditions),
            'logical_operator': logical_operator,
            'individual_results': results,
            'final_result': final_result,
            'evaluated_at': dt_module.datetime.now().isoformat()
        }
    
    async def _execute_result_node(self, workflow_id: str, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç»“æœèŠ‚ç‚¹"""
        self.logger.info(f"Executing result node: {node.id}")
        
        # ğŸŒŸ è·å–å…¨å±€ä¸Šä¸‹æ–‡
        global_context = self.workflow_global_context.get(workflow_id)
        
        data = node.data
        
        # è·å–è¾“å‡ºé…ç½®
        output_type = getattr(data, 'outputType', 'file')
        params_dict = getattr(data, 'params', {}) or {}
        
        # ğŸ¯ å¼ºåˆ¶æ€§ç›®æ ‡æ–‡ä»¶éªŒè¯å’Œæ˜¾ç¤ºï¼ˆå€Ÿé‰´Zapierçš„æ•°æ®æ˜ å°„éªŒè¯ï¼‰
        result_content = "âœ… å·¥ä½œæµæ‰§è¡Œå®Œæˆï¼\n\n"
        
        if global_context:
            try:
                target_file_path = global_context.get_target_file_path()
                target_file_info = global_context.get_target_file()
                
                result_content += f"ğŸ¯ ç›®æ ‡æ–‡ä»¶æ“ä½œå®Œæˆ:\n"
                result_content += f"æ–‡ä»¶åç§°: {target_file_info.get('name', 'Unknown')}\n"
                result_content += f"æ–‡ä»¶è·¯å¾„: {target_file_path}\n"
                result_content += f"æ–‡ä»¶å¤§å°: {target_file_info.get('size', 0)} bytes\n\n"
                
                # ğŸŒŸ æ•°æ®è¡€ç¼˜æ‘˜è¦
                lineage_summary = global_context.get_data_lineage_summary()
                result_content += f"ğŸ“‹ æ“ä½œå†å²:\n{lineage_summary}\n\n"
                
                # è®¾ç½®ç»“æœæ–‡ä»¶ä¿¡æ¯
                result_file_name = target_file_info.get('name', 'processed_document.docx')
                result_file_path = target_file_path
                
                self.logger.info(f"âœ… [ResultNode] ç›®æ ‡æ–‡ä»¶å¤„ç†å®Œæˆ: {result_file_name}")
                
            except ValueError as e:
                self.logger.warning(f"âš ï¸ [ResultNode] æ— æ³•è·å–ç›®æ ‡æ–‡ä»¶ä¿¡æ¯: {str(e)}")
                result_content += f"âš ï¸ è­¦å‘Š: {str(e)}\n\n"
                result_file_name = "workflow_result.txt"
                result_file_path = None
            else:
                self.logger.warning(f"âš ï¸ [ResultNode] å…¨å±€ä¸Šä¸‹æ–‡ä¸å­˜åœ¨")
                result_content += "âš ï¸ è­¦å‘Š: å…¨å±€ä¸Šä¸‹æ–‡ä¸å­˜åœ¨\n\n"
                result_file_name = "workflow_result.txt"
                result_file_path = None
        
        # å¤„ç†æ‰‹åŠ¨æ•°æ®æ˜ å°„
        mapped_data = self._apply_data_mapping(node, context)
        
        # æ·»åŠ æ˜ å°„çš„æ•°æ®æˆ–ä¸Šä¸‹æ–‡ç»“æœæ‘˜è¦
        if mapped_data:
            result_content += "ğŸ“Š è¾“å…¥æ•°æ®æ‘˜è¦:\n"
            for key, value in mapped_data.items():
                result_content += f"- {key}: {str(value)[:200]}{'...' if len(str(value)) > 200 else ''}\n"
        elif context:
            result_content += "ğŸ“Š èŠ‚ç‚¹æ‰§è¡Œæ‘˜è¦:\n"
            for key, value in context.items():
                if isinstance(value, dict) and 'node_type' in value:
                    node_type = value['node_type']
                    if node_type == 'material':
                        file_count = value.get('file_count', 0)
                        result_content += f"- ææ–™èŠ‚ç‚¹: å¤„ç†äº† {file_count} ä¸ªæ–‡ä»¶\n"
                    elif node_type == 'execution':
                        success = value.get('success', False)
                        result_content += f"- æ‰§è¡ŒèŠ‚ç‚¹: {'æˆåŠŸ' if success else 'å¤±è´¥'}\n"
                        if 'result' in value:
                            exec_result = str(value['result'])[:200]
                            result_content += f"  ç»“æœ: {exec_result}{'...' if len(str(value['result'])) > 200 else ''}\n"
        
        # æ„å»ºç»“æœå¯¹è±¡
        result = {
                'node_type': 'result',
                'output_type': output_type,
            'output_format': 'auto',
            'file_name': result_file_name,
            'content': result_content,
            'generated_at': datetime.now().isoformat(),
            'ready_to_export': True
        }
        
        # ğŸŒŸ å¦‚æœæœ‰ç›®æ ‡æ–‡ä»¶è·¯å¾„ï¼Œæ·»åŠ æ–‡ä»¶ä¿¡æ¯
        if result_file_path:
            try:
                import os
                if os.path.exists(result_file_path):
                    file_size = os.path.getsize(result_file_path)
                    result.update({
                        'size': file_size,
                        'storage_path': os.path.dirname(result_file_path),
                        'saved_file_path': result_file_path
                    })
                    self.logger.info(f"âœ… [ResultNode] ç›®æ ‡æ–‡ä»¶éªŒè¯æˆåŠŸ: {result_file_path} ({file_size} bytes)")
                else:
                    self.logger.warning(f"âš ï¸ [ResultNode] ç›®æ ‡æ–‡ä»¶ä¸å­˜åœ¨: {result_file_path}")
            except Exception as e:
                self.logger.warning(f"âš ï¸ [ResultNode] æ–‡ä»¶ä¿¡æ¯è·å–å¤±è´¥: {str(e)}")
        
        self.logger.info(f"ğŸ¯ [ResultNode] ç»“æœèŠ‚ç‚¹æ‰§è¡Œå®Œæˆï¼Œæ–‡ä»¶: {result_file_name}")
        return result
    


    async def _execute_default_node(self, workflow_id: str, node: Node, context: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œé»˜è®¤èŠ‚ç‚¹"""
        self.logger.info(f"Executing default node: {node.id}")

        return {
            'node_type': 'default',
            'node_id': node.id,
            'label': getattr(node.data, 'label', ''),
            'executed_at': dt_module.datetime.now().isoformat()
        }
    
    def _infer_output_format_from_task(self, task_description: str, result_content: str) -> str:
        """æ ¹æ®ä»»åŠ¡æè¿°å’Œç»“æœå†…å®¹æ™ºèƒ½æ¨æ–­è¾“å‡ºæ ¼å¼"""
        task_lower = task_description.lower()
        content_lower = result_content.lower() if result_content else ""
        
        # æ¼”ç¤ºæ–‡ç¨¿ç±»ä»»åŠ¡ï¼ˆä¼˜å…ˆæ£€æŸ¥ï¼Œé¿å…è¢«æ–‡æ¡£ç±»åŒ¹é…ï¼‰
        if any(keyword in task_lower for keyword in ['æ¼”ç¤º', 'ppt', 'presentation', 'å¹»ç¯ç‰‡', 'powerpoint']):
            return 'pptx'
        
        # æ–‡æ¡£ç±»ä»»åŠ¡
        elif any(keyword in task_lower for keyword in ['å†™', 'æ–‡æ¡£', 'æŠ¥å‘Š', 'è¯´æ˜', 'æ‰‹å†Œ', 'document', 'report']):
            if any(keyword in task_lower for keyword in ['è¡¨æ ¼', 'excel', 'sheet']):
                return 'xlsx'
            else:
                return 'docx'
        
        # æ•°æ®ç±»ä»»åŠ¡
        elif any(keyword in task_lower for keyword in ['æ•°æ®', 'data', 'ç»Ÿè®¡']):
            if any(keyword in task_lower for keyword in ['è¡¨æ ¼', 'excel', 'csv']):
                return 'xlsx'
            elif 'json' in task_lower or 'api' in task_lower:
                return 'json'
            else:
                return 'csv'
        
        # ä»£ç ç±»ä»»åŠ¡
        elif any(keyword in task_lower for keyword in ['ä»£ç ', 'code', 'script', 'è„šæœ¬', 'ç¨‹åº', 'python', 'javascript']):
            if 'python' in task_lower or 'py' in task_lower or 'python' in task_lower:
                return 'py'
            elif 'javascript' in task_lower or 'js' in task_lower:
                return 'js'
            elif 'html' in task_lower:
                return 'html'
            else:
                return 'py'  # é»˜è®¤ä»£ç æ ¼å¼æ”¹ä¸ºpy
        
        # ç½‘é¡µå¼€å‘ç±»ä»»åŠ¡
        elif any(keyword in task_lower for keyword in ['ç½‘é¡µ', 'html', 'web', 'é¡µé¢', 'å¼€å‘']):
            return 'html'
        
        # é»˜è®¤è¿”å›æ–‡æœ¬æ ¼å¼
        return 'txt'
    
    def _classify_task_type(self, task_description: str) -> str:
        """åˆ†ç±»ä»»åŠ¡ç±»å‹ï¼Œç”¨äºæ™ºèƒ½å‘½å"""
        task_lower = task_description.lower()
        
        if any(keyword in task_lower for keyword in ['å†™', 'åˆ›å»º', 'ç”Ÿæˆ']):
            if any(keyword in task_lower for keyword in ['æŠ¥å‘Š', 'report']):
                return 'report'
            elif any(keyword in task_lower for keyword in ['æ–‡æ¡£', 'document']):
                return 'document'
            elif any(keyword in task_lower for keyword in ['è®¡åˆ’', 'plan']):
                return 'plan'
            elif any(keyword in task_lower for keyword in ['æ€»ç»“', 'summary']):
                return 'summary'
            else:
                return 'creation'
        elif any(keyword in task_lower for keyword in ['åˆ†æ', 'analysis', 'ç ”ç©¶']):
            return 'analysis'
        elif any(keyword in task_lower for keyword in ['ç¿»è¯‘', 'translate']):
            return 'translation'
        elif any(keyword in task_lower for keyword in ['ä¼˜åŒ–', 'optimize', 'æ”¹è¿›']):
            return 'optimization'
        else:
            return 'general'
    
    def _extract_task_keywords(self, task_description: str) -> list:
        """ä»ä»»åŠ¡æè¿°ä¸­æå–å…³é”®è¯"""
        # ç®€å•çš„å…³é”®è¯æå–
        keywords = []
        common_keywords = ['æŠ¥å‘Š', 'æ–‡æ¡£', 'åˆ†æ', 'è®¡åˆ’', 'æ€»ç»“', 'æ•°æ®', 'ä»£ç ', 'ç½‘é¡µ']
        
        for keyword in common_keywords:
            if keyword in task_description:
                keywords.append(keyword)
        
        return keywords[:3]  # æœ€å¤šè¿”å›3ä¸ªå…³é”®è¯
    
    def _resolve_value(self, value: str, context: Dict[str, Any]) -> Any:
        """è§£æå€¼ä¸­çš„ä¸Šä¸‹æ–‡å¼•ç”¨"""
        if isinstance(value, str) and value.startswith('@'):
            ref_key = value[1:]
            return context.get(ref_key, value)
        return value
    
    def _evaluate_condition(self, left: Any, operator: str, right: Any, data_type: str) -> bool:
        """è¯„ä¼°æ¡ä»¶"""
        try:
            if data_type == 'number':
                left = float(left) if left is not None else 0
                right = float(right) if right is not None else 0
            elif data_type == 'text':
                left = str(left) if left is not None else ''
                right = str(right) if right is not None else ''
            
            if operator == 'equals':
                return left == right
            elif operator == 'notEquals':
                return left != right
            elif operator == 'greater':
                return left > right
            elif operator == 'less':
                return left < right
            elif operator == 'greaterEqual':
                return left >= right
            elif operator == 'lessEqual':
                return left <= right
            elif operator == 'contains':
                return str(right) in str(left)
            elif operator == 'notContains':
                return str(right) not in str(left)
            else:
                return False
                
        except Exception as e:
            self.logger.error(f"Condition evaluation error: {e}")
            return False
    
    def _generate_output_content(self, context: Dict[str, Any], output_type: str, output_format: str) -> str:
        """ç”Ÿæˆè¾“å‡ºå†…å®¹
        è¿”å›å­—ç¬¦ä¸²å…§å®¹ï¼›è‹¥ç‚º docxï¼Œè©²å‡½æ•¸å°‡å¯«å…¥è‡¨æ™‚æ–‡æœ¬ä¸¦ç”±ä¸Šå±¤ä¿å­˜ç‚º .docx ç”±å°ˆç”¨é‚è¼¯è™•ç†
        """
        # å„ªå…ˆå¾æœ€è¿‘çš„åŸ·è¡Œç¯€é»çµæœæ‹¿æ–‡æœ¬
        latest_exec_text = ''
        try:
            # å–æœ€å¾Œä¸€å€‹ *.result ä¸­çš„ 'result' æˆ– 'content'
            exec_keys = [k for k in context.keys() if k.endswith('.result')]
            for key in sorted(exec_keys, reverse=True):
                val = context.get(key) or {}
                if isinstance(val, dict):
                    latest_exec_text = val.get('result') or val.get('content') or ''
                    if latest_exec_text:
                        break
                else:
                    latest_exec_text = str(val)
                    break
        except Exception:
            pass

        if output_format == 'docx':
            # è¿”å›ç´”æ–‡æœ¬ï¼Œå¯«æª”æµç¨‹åœ¨ _execute_result_node å…§è™•ç† docx
            if latest_exec_text:
                return latest_exec_text
            # å¾Œå‚™ï¼šè‹¥æ²’æœ‰åŸ·è¡Œçµæœå°±åºåˆ—åŒ–ä¸Šä¸‹æ–‡
            return json.dumps(context, ensure_ascii=False, indent=2)

        if output_format == 'json':
            return json.dumps(context, indent=2, ensure_ascii=False)
        elif output_format == 'txt':
            if latest_exec_text:
                return latest_exec_text
            lines = []
            lines.append("å·¥ä½œæµæ‰§è¡Œç»“æœ")
            lines.append("=" * 50)
            lines.append(f"ç”Ÿæˆæ—¶é—´: {dt_module.datetime.now().isoformat()}")
            lines.append("")
            for key, value in context.items():
                lines.append(f"{key}:")
                lines.append(f"  {value}")
                lines.append("")
            return "\n".join(lines)
        else:
            return str(context)
    
    async def pause_workflow(self, workflow_id: str) -> bool:
        """æš‚åœå·¥ä½œæµ"""
        if workflow_id in self.active_workflows:
            task = self.active_workflows[workflow_id]
            task.cancel()
            self.state_manager.pause_workflow(workflow_id, "User requested pause")
            return True
        return False
    
    async def resume_workflow(self, workflow_id: str, payload: WorkflowPayload) -> bool:
        """æ¢å¤å·¥ä½œæµ"""
        execution = self.state_manager.get_workflow_status(workflow_id)
        if execution and execution.status == WorkflowStatus.PAUSED:
            self.state_manager.resume_workflow(workflow_id)

            # é‡æ–°å¯åŠ¨æ‰§è¡Œä»»åŠ¡
            task = asyncio.create_task(self._run_workflow(workflow_id, payload))
            self.active_workflows[workflow_id] = task
            return True
        return False

    async def reset_and_restart_workflow(self, workflow_id: str, payload: WorkflowPayload) -> str:
        """é‡ç½®å¹¶é‡æ–°å¯åŠ¨å·¥ä½œæµ"""
        try:
            # åœæ­¢å½“å‰æ‰§è¡Œï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
            if workflow_id in self.active_workflows:
                task = self.active_workflows[workflow_id]
                task.cancel()
                del self.active_workflows[workflow_id]

            # é‡ç½®å·¥ä½œæµçŠ¶æ€
            execution = self.state_manager.reset_workflow(workflow_id, payload)

            # é‡æ–°å¯åŠ¨å·¥ä½œæµ
            task = asyncio.create_task(self._run_workflow(workflow_id, payload))
            self.active_workflows[workflow_id] = task

            self.logger.info(f"Reset and restarted workflow: {workflow_id}")
            return workflow_id

        except Exception as e:
            self.logger.error(f"Failed to reset and restart workflow: {e}")
            self.state_manager.fail_workflow(workflow_id, str(e))
            raise
    
    def get_workflow_status(self, workflow_id: str) -> Optional[Dict[str, Any]]:
        """è·å–å·¥ä½œæµçŠ¶æ€"""
        try:
            # å°è¯•ä½¿ç”¨get_workflow_statusæ–¹æ³•
            execution = self.state_manager.get_workflow_status(workflow_id)
        except AttributeError:
            # å¦‚æœæ–¹æ³•ä¸å­˜åœ¨ï¼Œä½¿ç”¨workflowså­—å…¸ç›´æ¥è·å–
            execution = self.state_manager.workflows.get(workflow_id)
        if execution:
            # è·å–åŸºæœ¬å·¥ä½œæµçŠ¶æ€
            status_dict = execution.to_dict()
            
            # æ·»åŠ èŠ‚ç‚¹æ‰§è¡ŒçŠ¶æ€ - è¿™å¯¹å‰ç«¯æµç¨‹çº¿äº¤äº’è‡³å…³é‡è¦
            if workflow_id in self.state_manager.node_executions:
                status_dict['node_executions'] = {}
                for node_id, node_exec in self.state_manager.node_executions[workflow_id].items():
                    status_dict['node_executions'][node_id] = node_exec.to_dict()
            
            return status_dict
        return None
    
    def get_node_status(self, workflow_id: str, node_id: str) -> Optional[Dict[str, Any]]:
        """è·å–èŠ‚ç‚¹çŠ¶æ€"""
        node_execution = self.state_manager.get_node_status(workflow_id, node_id)
        if node_execution:
            return node_execution.to_dict()
        return None

    def _generate_output_content(self, context: Dict[str, Any], output_type: str, output_format: str) -> str:
        """ç”Ÿæˆè¾“å‡ºå†…å®¹"""
        if not context:
            return "# å·¥ä½œæµæ‰§è¡Œç»“æœ\n\næ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ‰§è¡Œå†…å®¹ã€‚"
        
        # æ”¶é›†æ‰€æœ‰èŠ‚ç‚¹çš„æ‰§è¡Œç»“æœ
        content_parts = []
        content_parts.append(f"# å·¥ä½œæµæ‰§è¡Œç»“æœ")
        content_parts.append(f"ç”Ÿæˆæ—¶é—´: {dt_module.datetime.now().isoformat()}")
        content_parts.append("")
        
        for node_id, node_result in context.items():
            if isinstance(node_result, dict):
                node_type = node_result.get('node_type', 'unknown')
                
                if node_type == 'execution':
                    # æ‰§è¡ŒèŠ‚ç‚¹çš„ç»“æœ
                    task = node_result.get('task', 'æœªçŸ¥ä»»åŠ¡')
                    result = node_result.get('result', '')
                    
                    content_parts.append(f"## æ‰§è¡Œä»»åŠ¡: {task}")
                    content_parts.append("")
                    if result:
                        content_parts.append(result)
                    else:
                        content_parts.append("*æ­¤ä»»åŠ¡æ²¡æœ‰ç”Ÿæˆè¾“å‡ºå†…å®¹*")
                    content_parts.append("")
                    
                elif node_type == 'material':
                    # ææ–™èŠ‚ç‚¹çš„ç»“æœ
                    content_parts.append(f"## ææ–™èŠ‚ç‚¹")
                    content_parts.append("")
                    content_parts.append(str(node_result.get('content', node_result)))
                    content_parts.append("")
                    
                elif node_type == 'condition':
                    # æ¡ä»¶èŠ‚ç‚¹çš„ç»“æœ
                    condition_result = node_result.get('result', False)
                    content_parts.append(f"## æ¡ä»¶åˆ¤æ–­: {'é€šè¿‡' if condition_result else 'ä¸é€šè¿‡'}")
                    content_parts.append("")
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹ï¼Œæ·»åŠ ä¸€ä¸ªé»˜è®¤è¯´æ˜
        if len(content_parts) <= 3:  # åªæœ‰æ ‡é¢˜å’Œæ—¶é—´
            content_parts.append("## æ‰§è¡Œè¯´æ˜")
            content_parts.append("")
            content_parts.append("å·¥ä½œæµå·²æˆåŠŸæ‰§è¡Œï¼Œä½†å„ä¸ªèŠ‚ç‚¹æ²¡æœ‰ç”Ÿæˆå¯æ˜¾ç¤ºçš„å†…å®¹ã€‚")
            content_parts.append("è¿™å¯èƒ½æ˜¯å› ä¸ºï¼š")
            content_parts.append("- æ‰§è¡ŒèŠ‚ç‚¹ä½¿ç”¨äº†æ¨¡æ‹Ÿæ¨¡å¼")
            content_parts.append("- APIé…ç½®éœ€è¦æ£€æŸ¥")
            content_parts.append("- ä»»åŠ¡ç±»å‹ä¸éœ€è¦ç”Ÿæˆæ–‡æœ¬è¾“å‡º")
        
        return "\n".join(content_parts)


# å…¨å±€æ‰§è¡Œå¼•æ“å®ä¾‹
mindmap_execution_engine = MindmapExecutionEngine()
