"""
AIæ¨¡å‹é›†æˆæ¨¡å—
é›†æˆå¤šç§AIæ¨¡å‹APIï¼Œæä¾›çœŸæ­£çš„æ™ºèƒ½å¯¹è¯å’Œåˆ†æèƒ½åŠ›
æ”¯æŒæ™ºèƒ½æ¨¡å¼æ£€æµ‹å’Œè‡ªé€‚åº”å¤„ç†ç­–ç•¥
"""

import logging
import asyncio
import os
import json
from typing import Dict, Any, List, Optional
from dataclasses import dataclass
from enum import Enum
import aiohttp
from datetime import datetime

# å¯¼å…¥ç°æœ‰ç³»ç»Ÿç»„ä»¶
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from agent_system.agent_factory import AgentFactory

# å¯¼å…¥æ™ºèƒ½æ¨¡å¼æ£€æµ‹å™¨
from .mode_detection.integration import get_mode_detection_service, detect_processing_mode


class AIProvider(Enum):
    """AIæœåŠ¡æä¾›å•†"""
    DEEPSEEK = "deepseek"
    QWEN = "qwen"
    ZHIPU = "zhipu"
    BAIDU = "baidu"
    MOONSHOT = "moonshot"
    DOUBAO = "doubao"
    OPENAI = "openai"
    CLAUDE = "claude"


@dataclass
class AIResponse:
    """AIå“åº”æ•°æ®ç»“æ„"""
    content: str
    confidence: float
    provider: str
    model: str
    tokens_used: int
    response_time: float
    metadata: Dict[str, Any]


class IntelligentAIIntegration:
    """
    æ™ºèƒ½AIé›†æˆç³»ç»Ÿ
    
    æä¾›å¤šç§AIæ¨¡å‹çš„ç»Ÿä¸€æ¥å£ï¼Œå®ç°ï¼š
    1. å¤šæ¨¡å‹æ”¯æŒå’Œè‡ªåŠ¨åˆ‡æ¢
    2. æ™ºèƒ½æç¤ºè¯å·¥ç¨‹
    3. å“åº”è´¨é‡è¯„ä¼°
    4. é”™è¯¯å¤„ç†å’Œé‡è¯•
    5. æˆæœ¬ä¼˜åŒ–
    """
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
        self.providers = {}
        self.current_provider = AIProvider.DEEPSEEK
        self.is_initialized = False
        
        # é…ç½®ä¿¡æ¯
        self.api_keys = {
            'deepseek': os.getenv('DEEPSEEK_API_KEY'),
            'qwen': os.getenv('QWEN_API_KEY'),
            'zhipu': os.getenv('ZHIPU_API_KEY'),
            'baidu': os.getenv('BAIDU_API_KEY'),
            'moonshot': os.getenv('MOONSHOT_API_KEY'),
            'doubao': os.getenv('DOUBAO_API_KEY'),
            'openai': os.getenv('OPENAI_API_KEY'),
            'claude': os.getenv('CLAUDE_API_KEY')
        }
        
        # æ¨¡å‹é…ç½®
        self.model_configs = {
            AIProvider.DEEPSEEK: {
                'model': 'deepseek-chat',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.001
            },
            AIProvider.QWEN: {
                'model': 'qwen-turbo',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.0005
            },
            AIProvider.ZHIPU: {
                'model': 'glm-4',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.001
            },
            AIProvider.BAIDU: {
                'model': 'ernie-4.0-8k',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.0008
            },
            AIProvider.MOONSHOT: {
                'model': 'moonshot-v1-8k',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.0012
            },
            AIProvider.DOUBAO: {
                'model': 'doubao-lite-4k',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.0005
            },
            AIProvider.OPENAI: {
                'model': 'gpt-3.5-turbo',
                'max_tokens': 4000,
                'temperature': 0.7,
                'cost_per_1k': 0.002
            }
        }

        # æ™ºèƒ½æ¨¡å¼æ£€æµ‹æœåŠ¡
        self.mode_detection_service = None
        self.mode_detection_enabled = True
    
    async def initialize(self):
        """åˆå§‹åŒ–AIé›†æˆç³»ç»Ÿ"""
        if self.is_initialized:
            return

        self.logger.info("ğŸ¤– [AIIntegration] åˆå§‹åŒ–AIé›†æˆç³»ç»Ÿ...")

        try:
            # æ£€æŸ¥å¯ç”¨çš„APIå¯†é’¥
            available_providers = []
            for provider, api_key in self.api_keys.items():
                if api_key:
                    available_providers.append(provider)
                    self.logger.info(f"âœ… [AIIntegration] {provider} APIå¯†é’¥å·²é…ç½®")
                else:
                    self.logger.warning(f"âš ï¸ [AIIntegration] {provider} APIå¯†é’¥æœªé…ç½®")

            if not available_providers:
                raise RuntimeError("âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥ï¼šæœªé…ç½®æœ‰æ•ˆçš„AI APIå¯†é’¥ï¼è¯·æ·»åŠ  DEEPSEEK_API_KEY æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            else:
                # é€‰æ‹©æœ€ä¼˜çš„æä¾›å•†
                self.current_provider = self._select_best_provider(available_providers)
                self.logger.info(f"ğŸ¯ [AIIntegration] é€‰æ‹©AIæä¾›å•†: {self.current_provider.value}")

            # åˆå§‹åŒ–æ™ºèƒ½æ¨¡å¼æ£€æµ‹æœåŠ¡
            if self.mode_detection_enabled:
                try:
                    self.mode_detection_service = get_mode_detection_service(self)
                    await self.mode_detection_service.initialize()
                    self.logger.info("ğŸ§  [AIIntegration] æ™ºèƒ½æ¨¡å¼æ£€æµ‹æœåŠ¡å·²å¯ç”¨")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [AIIntegration] æ¨¡å¼æ£€æµ‹æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
                    self.mode_detection_enabled = False

            self.is_initialized = True
            self.logger.info("âœ… [AIIntegration] AIé›†æˆç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            self.logger.error(f"âŒ [AIIntegration] åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def generate_intelligent_response(self,
                                          user_input: str,
                                          context: Dict[str, Any] = None,
                                          response_type: str = "conversational",
                                          user_preferred_mode: str = None) -> AIResponse:
        """
        ç”Ÿæˆæ™ºèƒ½å“åº”ï¼ˆæ”¯æŒæ™ºèƒ½æ¨¡å¼æ£€æµ‹ï¼‰

        Args:
            user_input: ç”¨æˆ·è¾“å…¥
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            response_type: å“åº”ç±»å‹ (conversational, analytical, creative, technical)
            user_preferred_mode: ç”¨æˆ·æŒ‡å®šçš„å¤„ç†æ¨¡å¼ (daily/professional)
        """
        if not self.is_initialized:
            await self.initialize()

        self.logger.info(f"ğŸ§  [AIIntegration] ç”Ÿæˆæ™ºèƒ½å“åº”: {user_input[:50]}...")

        try:
            # æ™ºèƒ½æ¨¡å¼æ£€æµ‹
            processing_mode = "daily"  # é»˜è®¤æ¨¡å¼
            mode_confidence = 0.5
            mode_reasoning = "é»˜è®¤æ—¥å¸¸æ¨¡å¼"
            detection_id = None

            if user_preferred_mode:
                # ç”¨æˆ·æŒ‡å®šæ¨¡å¼
                processing_mode = user_preferred_mode
                mode_confidence = 1.0
                mode_reasoning = "ç”¨æˆ·æŒ‡å®šæ¨¡å¼"
                self.logger.info(f"ğŸ‘¤ [AIIntegration] ç”¨æˆ·æŒ‡å®šæ¨¡å¼: {processing_mode}")
            elif self.mode_detection_enabled and self.mode_detection_service:
                # è‡ªåŠ¨æ£€æµ‹æ¨¡å¼ï¼ˆé»˜è®¤ä½¿ç”¨autoæ¨¡å¼è¿›è¡Œæ™ºèƒ½æ£€æµ‹ï¼‰
                try:
                    mode_result = await self.mode_detection_service.detect_user_mode(
                        user_input, context, user_mode_preference="auto"
                    )
                    if mode_result["success"]:
                        processing_mode = mode_result["mode"]
                        mode_confidence = mode_result["confidence"]
                        mode_reasoning = mode_result["reasoning"]
                        detection_id = mode_result["detection_id"]
                        mode_source = mode_result.get("mode_source", "auto_detected")
                        self.logger.info(f"ğŸ¯ [AIIntegration] æ£€æµ‹åˆ°æ¨¡å¼: {processing_mode} (ç½®ä¿¡åº¦: {mode_confidence:.2f}, æ¥æº: {mode_source})")
                    else:
                        self.logger.warning(f"âš ï¸ [AIIntegration] æ¨¡å¼æ£€æµ‹å¤±è´¥: {mode_result.get('error', 'unknown')}")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ [AIIntegration] æ¨¡å¼æ£€æµ‹å¼‚å¸¸: {e}")

            # æ ¹æ®æ£€æµ‹æ¨¡å¼è°ƒæ•´ä¸Šä¸‹æ–‡
            enhanced_context = context.copy() if context else {}
            enhanced_context.update({
                "processing_mode": processing_mode,
                "mode_confidence": mode_confidence,
                "mode_reasoning": mode_reasoning,
                "detection_id": detection_id
            })

            # æ„å»ºæ™ºèƒ½æç¤ºè¯
            prompt = await self._build_intelligent_prompt(user_input, enhanced_context, response_type)

            # è°ƒç”¨AIæ¨¡å‹
            start_time = datetime.now()

            response_content = await self._call_ai_api(prompt)

            response_time = (datetime.now() - start_time).total_seconds()

            # è¯„ä¼°å“åº”è´¨é‡
            confidence = await self._evaluate_response_quality(response_content, user_input)

            return AIResponse(
                content=response_content,
                confidence=confidence,
                provider=self.current_provider.value,
                model=self.model_configs.get(self.current_provider.value, {}).get('model', 'unknown'),
                tokens_used=len(response_content.split()) * 1.3 if response_content else 0,  # ä¼°ç®—
                response_time=response_time,
                metadata={
                    'response_type': response_type,
                    'context_used': bool(context),
                    'prompt_length': len(prompt),
                    'processing_mode': processing_mode,
                    'mode_confidence': mode_confidence,
                    'mode_reasoning': mode_reasoning,
                    'detection_id': detection_id
                }
            )

        except Exception as e:
            self.logger.error(f"âŒ [AIIntegration] ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            # è¿”å›å¤‡ç”¨å“åº”
            return await self._generate_fallback_response(user_input, context)
    
    async def _build_intelligent_prompt(self,
                                      user_input: str,
                                      context: Dict[str, Any],
                                      response_type: str) -> str:
        """æ„å»ºæ™ºèƒ½æç¤ºè¯ï¼ˆæ”¯æŒæ¨¡å¼æ„ŸçŸ¥ï¼‰"""

        # è·å–å¤„ç†æ¨¡å¼
        processing_mode = context.get("processing_mode", "daily")
        mode_confidence = context.get("mode_confidence", 0.5)

        # æ ¹æ®å¤„ç†æ¨¡å¼æ„å»ºä¸åŒçš„ç³»ç»Ÿæç¤º
        if processing_mode == "professional":
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçº§AIé¡¾é—®ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹è´¨ï¼š
1. ä¸¥è°¨ä¸“ä¸šçš„åˆ†æèƒ½åŠ›å’Œæƒå¨æ€§
2. ç³»ç»Ÿæ€§æ€è€ƒå’Œå…¨é¢è€ƒè™‘é—®é¢˜
3. ä½¿ç”¨å‡†ç¡®çš„ä¸“ä¸šæœ¯è¯­å’Œæ ‡å‡†åŒ–è¡¨è¾¾
4. æä¾›ä¼ä¸šçº§æ ‡å‡†çš„è§£å†³æ–¹æ¡ˆ
5. ç¡®ä¿å†…å®¹çš„å‡†ç¡®æ€§å’Œå¯é æ€§

å¤„ç†åŸåˆ™ï¼š
- ä¼ä¸šæ ‡å‡†ï¼šç¬¦åˆæ­£å¼å•†åŠ¡è§„èŒƒå’Œä¼ä¸šçº§è¦æ±‚
- ç³»ç»Ÿå®Œæ•´ï¼šå…¨é¢åˆ†æå„ä¸ªæ–¹é¢ï¼Œä¸é—æ¼è¦ç‚¹
- ä¸“ä¸šæƒå¨ï¼šä½¿ç”¨ä¸“ä¸šæœ¯è¯­ï¼Œæä¾›æƒå¨å¯ä¿¡çš„å»ºè®®
- è´¨é‡ä¿è¯ï¼šç¡®ä¿å†…å®¹çš„å‡†ç¡®æ€§å’Œä¸“ä¸šæ€§
- åˆè§„å®‰å…¨ï¼šéµå¾ªç›¸å…³æ³•è§„å’Œæ ‡å‡†

è¯·æä¾›é«˜è´¨é‡çš„ä¸“ä¸šå›ç­”ã€‚"""
        else:
            system_prompt = """ä½ æ˜¯ä¸€ä¸ªå‹å¥½å®ç”¨çš„æ—¥å¸¸å·¥ä½œåŠ©æ‰‹ï¼Œå…·å¤‡ä»¥ä¸‹ç‰¹è´¨ï¼š
1. äº²åˆ‡å‹å¥½çš„äº¤æµæ–¹å¼å’Œå®ç”¨å¯¼å‘
2. å¿«é€Ÿé«˜æ•ˆåœ°è§£å†³å®é™…é—®é¢˜
3. ç®€æ´æ˜äº†çš„è¡¨è¾¾å’Œæ˜“äºç†è§£
4. é‡ç‚¹çªå‡ºï¼Œé¿å…è¿‡åº¦å¤æ‚åŒ–
5. æä¾›å¯ç«‹å³æ‰§è¡Œçš„å»ºè®®

å¤„ç†åŸåˆ™ï¼š
- å¤Ÿç”¨å°±è¡Œï¼šåŒ…å«å¿…è¦ä¿¡æ¯ï¼Œä¸è¿‡åº¦å¤æ‚
- å¿«é€Ÿé«˜æ•ˆï¼šç›´æ¥æ˜äº†ï¼Œä¾¿äºç†è§£å’Œæ‰§è¡Œ
- å®ç”¨å¯¼å‘ï¼šé‡ç‚¹çªå‡ºï¼Œè§£å†³å®é™…é—®é¢˜
- å‹å¥½ä¸“ä¸šï¼šè¯­è°ƒé€‚ä¸­ï¼Œä¸è¿‡åˆ†æ­£å¼
- äº’åŠ¨æ€§å¼ºï¼šé¼“åŠ±è¿›ä¸€æ­¥äº¤æµå’Œåé¦ˆ

è¯·æä¾›å®ç”¨é«˜æ•ˆçš„å›ç­”ã€‚"""

        # æ ¹æ®å“åº”ç±»å‹è°ƒæ•´æç¤ºè¯
        if processing_mode == "professional":
            type_prompts = {
                "conversational": "è¯·ä»¥æ­£å¼ã€ä¸“ä¸šçš„æ–¹å¼å›åº”ï¼Œç¡®ä¿å†…å®¹çš„æƒå¨æ€§ã€‚",
                "analytical": "è¯·æä¾›æ·±å…¥çš„ä¸“ä¸šåˆ†æï¼ŒåŒ…å«ç†è®ºæ”¯æ’‘å’Œç³»ç»Ÿæ€§è§è§£ã€‚",
                "creative": "è¯·æä¾›åˆ›æ–°ä½†ä¸“ä¸šçš„è§£å†³æ–¹æ¡ˆï¼Œç¡®ä¿å¯è¡Œæ€§å’Œä¸“ä¸šæ€§ã€‚",
                "technical": "è¯·æä¾›ä¼ä¸šçº§çš„æŠ€æœ¯è§£ç­”ï¼ŒåŒ…å«è¯¦ç»†çš„å®æ–½æ–¹æ¡ˆå’Œæ ‡å‡†ã€‚"
            }
        else:
            type_prompts = {
                "conversational": "è¯·ä»¥è½»æ¾ã€å‹å¥½çš„å¯¹è¯æ–¹å¼å›åº”ã€‚",
                "analytical": "è¯·æä¾›å®ç”¨çš„åˆ†æå’Œè§è§£ï¼Œé‡ç‚¹çªå‡ºå…³é”®è¦ç‚¹ã€‚",
                "creative": "è¯·å‘æŒ¥åˆ›é€ åŠ›ï¼Œæä¾›ç®€å•æ˜“è¡Œçš„åˆ›æ–°æƒ³æ³•ã€‚",
                "technical": "è¯·æä¾›å®ç”¨çš„æŠ€æœ¯è§£ç­”ï¼ŒåŒ…å«å…·ä½“çš„æ“ä½œæ­¥éª¤ã€‚"
            }

        prompt = f"{system_prompt}\n\n{type_prompts.get(response_type, '')}\n\n"

        # æ·»åŠ æ¨¡å¼ä¿¡æ¯
        if mode_confidence > 0.8:
            prompt += f"æ£€æµ‹åˆ°çš„å¤„ç†æ¨¡å¼ï¼š{processing_mode}ï¼ˆé«˜ç½®ä¿¡åº¦ï¼‰\n"
        elif mode_confidence > 0.6:
            prompt += f"æ£€æµ‹åˆ°çš„å¤„ç†æ¨¡å¼ï¼š{processing_mode}ï¼ˆä¸­ç­‰ç½®ä¿¡åº¦ï¼‰\n"
        else:
            prompt += f"ä½¿ç”¨é»˜è®¤å¤„ç†æ¨¡å¼ï¼š{processing_mode}\n"

        # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆæ’é™¤æ¨¡å¼æ£€æµ‹ç›¸å…³ä¿¡æ¯ï¼‰
        if context:
            filtered_context = {k: v for k, v in context.items()
                              if k not in ["processing_mode", "mode_confidence", "mode_reasoning", "detection_id"]}
            if filtered_context:
                prompt += f"ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š{json.dumps(filtered_context, ensure_ascii=False)}\n\n"

        prompt += f"ç”¨æˆ·è¾“å…¥ï¼š{user_input}\n\nè¯·æä¾›å›ç­”ï¼š"

        return prompt

    async def record_mode_feedback(self, detection_id: str, user_choice: str, satisfaction: int = None, comments: str = None) -> bool:
        """è®°å½•ç”¨æˆ·å¯¹æ¨¡å¼é€‰æ‹©çš„åé¦ˆ"""

        if not self.mode_detection_enabled or not self.mode_detection_service:
            return False

        try:
            return await self.mode_detection_service.record_user_choice(
                detection_id, user_choice, satisfaction, comments
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ [AIIntegration] è®°å½•æ¨¡å¼åé¦ˆå¤±è´¥: {e}")
            return False

    async def get_mode_detection_stats(self) -> Dict[str, Any]:
        """è·å–æ¨¡å¼æ£€æµ‹ç»Ÿè®¡ä¿¡æ¯"""

        if not self.mode_detection_enabled or not self.mode_detection_service:
            return {"enabled": False}

        try:
            stats = await self.mode_detection_service.get_detection_stats()
            stats["enabled"] = True
            return stats
        except Exception as e:
            self.logger.warning(f"âš ï¸ [AIIntegration] è·å–æ¨¡å¼æ£€æµ‹ç»Ÿè®¡å¤±è´¥: {e}")
            return {"enabled": True, "error": str(e)}

    def toggle_mode_detection(self, enabled: bool) -> None:
        """å¯ç”¨æˆ–ç¦ç”¨æ¨¡å¼æ£€æµ‹"""

        self.mode_detection_enabled = enabled
        self.logger.info(f"ğŸ¯ [AIIntegration] æ¨¡å¼æ£€æµ‹å·²{'å¯ç”¨' if enabled else 'ç¦ç”¨'}")

    async def get_supported_modes(self) -> Dict[str, Any]:
        """è·å–æ”¯æŒçš„æ¨¡å¼åˆ—è¡¨"""

        if not self.mode_detection_enabled or not self.mode_detection_service:
            return {"enabled": False}

        try:
            return await self.mode_detection_service.get_supported_modes()
        except Exception as e:
            self.logger.warning(f"âš ï¸ [AIIntegration] è·å–æ”¯æŒæ¨¡å¼å¤±è´¥: {e}")
            return {"error": str(e)}

    async def get_mode_info(self, mode: str) -> Dict[str, Any]:
        """è·å–ç‰¹å®šæ¨¡å¼çš„ä¿¡æ¯"""

        if not self.mode_detection_enabled or not self.mode_detection_service:
            return {"enabled": False}

        try:
            return await self.mode_detection_service.get_mode_info(mode)
        except Exception as e:
            self.logger.warning(f"âš ï¸ [AIIntegration] è·å–æ¨¡å¼ä¿¡æ¯å¤±è´¥: {e}")
            return {"error": str(e)}

    async def is_mode_enabled(self, mode: str) -> bool:
        """æ£€æŸ¥æ¨¡å¼æ˜¯å¦å¯ç”¨"""

        if not self.mode_detection_enabled or not self.mode_detection_service:
            return False

        try:
            return await self.mode_detection_service.is_mode_enabled(mode)
        except Exception as e:
            self.logger.warning(f"âš ï¸ [AIIntegration] æ£€æŸ¥æ¨¡å¼çŠ¶æ€å¤±è´¥: {e}")
            return False

    async def _call_ai_api(self, prompt: str) -> str:
        """è°ƒç”¨AI API"""
        try:
            # ä½¿ç”¨ç°æœ‰çš„AgentFactory
            response = await AgentFactory.ask_llm(prompt)
            
            # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
            if "[å ä½ç¬¦å›åº”]" in response or "[é”™è¯¯]" in response:
                raise Exception("AI APIä¸å¯ç”¨")
            
            return response
            
        except Exception as e:
            self.logger.error(f"âŒ [AIIntegration] AI APIè°ƒç”¨å¤±è´¥: {e}")
            raise
    

    
    async def _evaluate_response_quality(self, response: str, user_input: str) -> float:
        """è¯„ä¼°å“åº”è´¨é‡"""
        try:
            # åŸºç¡€è´¨é‡æŒ‡æ ‡
            quality_score = 0.5
            
            # é•¿åº¦åˆç†æ€§
            if 20 <= len(response) <= 1000:
                quality_score += 0.2
            
            # ç›¸å…³æ€§æ£€æŸ¥ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰
            user_keywords = set(user_input.lower().split())
            response_keywords = set(response.lower().split())
            relevance = len(user_keywords & response_keywords) / max(len(user_keywords), 1)
            quality_score += relevance * 0.2
            
            # å®Œæ•´æ€§æ£€æŸ¥
            if not any(phrase in response for phrase in ['[å ä½ç¬¦', '[é”™è¯¯', 'å¤±è´¥', 'æ— æ³•']):
                quality_score += 0.1
            
            return min(1.0, quality_score)
            
        except Exception:
            return 0.5
    
    async def _generate_fallback_response(self, user_input: str, context: Dict[str, Any]) -> AIResponse:
        """ç”Ÿæˆå¤‡ç”¨å“åº”"""
        raise RuntimeError("âŒ AI APIè°ƒç”¨å¤±è´¥ä¸”æ— å¤‡ç”¨æ–¹æ¡ˆï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIå¯†é’¥é…ç½®")
        
        return AIResponse(
            content=fallback_content,
            confidence=0.3,
            provider="fallback",
            model="local",
            tokens_used=len(fallback_content.split()) if fallback_content else 0,
            response_time=0.1,
            metadata={'is_fallback': True}
        )
    
    def _select_best_provider(self, available_providers: List[str]) -> AIProvider:
        """é€‰æ‹©æœ€ä½³çš„AIæä¾›å•†"""
        # ä¼˜å…ˆçº§æ’åº - å›½å†…AIæœåŠ¡å•†ä¼˜å…ˆ
        priority = ['deepseek', 'qwen', 'zhipu', 'moonshot', 'doubao', 'baidu', 'openai', 'claude']

        for provider in priority:
            if provider in available_providers:
                return AIProvider(provider)

        return AIProvider.LOCAL
