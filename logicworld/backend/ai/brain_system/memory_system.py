"""
æ™ºèƒ½è®°å¿†ä¸çŸ¥è¯†å›¾è°±ç³»ç»Ÿ
å‡çº§PromptXè®°å¿†ç³»ç»Ÿï¼Œæ„å»ºåŠ¨æ€çŸ¥è¯†å›¾è°±å’Œé•¿æœŸè®°å¿†æ•´åˆæœºåˆ¶
"""

import logging
import asyncio
import json
import sqlite3
from typing import Dict, Any, List, Optional, Tuple, Set
from dataclasses import dataclass, asdict
from enum import Enum
from datetime import datetime, timedelta
import hashlib
import os

# å¯¼å…¥ç°æœ‰ç³»ç»Ÿç»„ä»¶
import sys
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
from agent_system.agent_factory import AgentFactory


class MemoryType(Enum):
    """è®°å¿†ç±»å‹"""
    WORKING = "working"         # å·¥ä½œè®°å¿†ï¼ˆçŸ­æœŸï¼‰
    SEMANTIC = "semantic"       # è¯­ä¹‰è®°å¿†ï¼ˆé•¿æœŸï¼‰
    EPISODIC = "episodic"       # æƒ…æ™¯è®°å¿†ï¼ˆäº‹ä»¶ï¼‰
    PROCEDURAL = "procedural"   # ç¨‹åºæ€§è®°å¿†ï¼ˆæŠ€èƒ½ï¼‰
    DECLARATIVE = "declarative" # é™ˆè¿°æ€§è®°å¿†ï¼ˆäº‹å®ï¼‰


class MemoryImportance(Enum):
    """è®°å¿†é‡è¦æ€§"""
    LOW = 1
    MEDIUM = 2
    HIGH = 3
    CRITICAL = 4


@dataclass
class MemoryNode:
    """è®°å¿†èŠ‚ç‚¹"""
    id: str
    content: str
    memory_type: MemoryType
    importance: MemoryImportance
    timestamp: str
    access_count: int = 0
    last_accessed: str = ""
    tags: List[str] = None
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.tags is None:
            self.tags = []
        if self.metadata is None:
            self.metadata = {}


@dataclass
class KnowledgeEdge:
    """çŸ¥è¯†è¾¹ï¼ˆå…³ç³»ï¼‰"""
    source_id: str
    target_id: str
    relation_type: str
    strength: float
    timestamp: str
    metadata: Dict[str, Any] = None
    
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}


@dataclass
class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°±"""
    nodes: Dict[str, MemoryNode]
    edges: List[KnowledgeEdge]
    
    def __post_init__(self):
        if not hasattr(self, 'nodes'):
            self.nodes = {}
        if not hasattr(self, 'edges'):
            self.edges = []


class IntelligentMemorySystem:
    """
    æ™ºèƒ½è®°å¿†ä¸çŸ¥è¯†å›¾è°±ç³»ç»Ÿ
    
    åŠŸèƒ½ï¼š
    1. å¤šç±»å‹è®°å¿†ç®¡ç†ï¼ˆå·¥ä½œè®°å¿†ã€è¯­ä¹‰è®°å¿†ã€æƒ…æ™¯è®°å¿†ç­‰ï¼‰
    2. åŠ¨æ€çŸ¥è¯†å›¾è°±æ„å»ºå’Œç»´æŠ¤
    3. æ™ºèƒ½è®°å¿†æ£€ç´¢å’Œå…³è”
    4. è®°å¿†é‡è¦æ€§è¯„ä¼°å’Œé—å¿˜æœºåˆ¶
    5. è·¨ä¼šè¯è®°å¿†æŒä¹…åŒ–
    """
    
    def __init__(self, db_path: str = "brain_memory.db"):
        self.logger = logging.getLogger(__name__)
        self.db_path = db_path
        self.is_initialized = False
        
        # å†…å­˜ä¸­çš„çŸ¥è¯†å›¾è°±
        self.knowledge_graph = KnowledgeGraph({}, [])
        
        # å·¥ä½œè®°å¿†ï¼ˆä¸´æ—¶å­˜å‚¨ï¼‰
        self.working_memory = {}

        # ç®€åŒ–çš„ä¼šè¯è®°å¿†ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        self.session_memory: Dict[str, List[Dict[str, Any]]] = {}

        # è®°å¿†è®¿é—®ç»Ÿè®¡
        self.access_stats = {}
        
    async def initialize(self):
        """åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ"""
        if self.is_initialized:
            return
            
        self.logger.info("ğŸ§  [IntelligentMemory] åˆå§‹åŒ–æ™ºèƒ½è®°å¿†ç³»ç»Ÿ...")
        
        try:
            # åˆå§‹åŒ–æ•°æ®åº“
            await self._initialize_database()
            
            # åŠ è½½ç°æœ‰è®°å¿†åˆ°å†…å­˜
            await self._load_memory_to_cache()
            
            # åˆå§‹åŒ–çŸ¥è¯†å›¾è°±
            await self._initialize_knowledge_graph()
            
            self.is_initialized = True
            self.logger.info("âœ… [IntelligentMemory] æ™ºèƒ½è®°å¿†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
            
        except Exception as e:
            self.logger.error(f"âŒ [IntelligentMemory] åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def process(self, request, analysis: Dict[str, Any], previous_results: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è®°å¿†è¯·æ±‚"""
        if not self.is_initialized:
            await self.initialize()
            
        self.logger.info("ğŸ§  [IntelligentMemory] å¤„ç†è®°å¿†è¯·æ±‚...")
        
        try:
            # 1. å­˜å‚¨å½“å‰äº¤äº’åˆ°è®°å¿†
            memory_node = await self._store_interaction(request, analysis, previous_results)

            # 2. æ£€ç´¢ç›¸å…³è®°å¿†
            relevant_memories = await self._retrieve_relevant_memories(request.input_text, analysis)

            # 3. æ›´æ–°çŸ¥è¯†å›¾è°±
            await self._update_knowledge_graph(memory_node, relevant_memories, analysis)

            # 4. ç”Ÿæˆè®°å¿†å¢å¼ºçš„å“åº”
            enhanced_response = await self._generate_memory_enhanced_response(
                request, relevant_memories, analysis
            )
            
            return {
                'output': enhanced_response,
                'confidence': 0.8,
                'relevant_memories': relevant_memories,
                'new_memory_id': memory_node.id,
                'updates': [{'type': 'memory_stored', 'id': memory_node.id}],
                'reasoning_trace': [
                    {
                        'module': 'intelligent_memory',
                        'action': 'process_memory',
                        'result': {
                            'stored_memory': memory_node.id,
                            'retrieved_count': len(relevant_memories)
                        },
                        'timestamp': datetime.now().isoformat()
                    }
                ],
                'metadata': {
                    'memory_type': memory_node.memory_type.value,
                    'importance': memory_node.importance.value,
                    'graph_nodes_count': len(self.knowledge_graph.nodes),
                    'graph_edges_count': len(self.knowledge_graph.edges)
                }
            }
            
        except Exception as e:
            self.logger.error(f"âŒ [IntelligentMemory] å¤„ç†å¤±è´¥: {e}")
            return await self._create_fallback_memory_response(request, str(e))

    async def _create_fallback_memory_response(self, request, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨è®°å¿†å“åº”"""
        self.logger.info("ğŸ”§ [IntelligentMemory] ä½¿ç”¨ç®€åŒ–è®°å¿†æ¨¡å¼...")

        try:
            # ç®€åŒ–çš„è®°å¿†å¤„ç†
            user_id = getattr(request, 'user_id', 'default')
            session_id = getattr(request, 'session_id', 'default')

            # ç®€å•çš„ä¼šè¯è®°å¿†
            session_key = f"{user_id}_{session_id}"
            if session_key not in self.session_memory:
                self.session_memory[session_key] = []

            # å­˜å‚¨å½“å‰äº¤äº’
            interaction = {
                'input': request.input_text,
                'timestamp': datetime.now().isoformat(),
                'id': f"mem_{len(self.session_memory[session_key])}"
            }
            self.session_memory[session_key].append(interaction)

            # ä¿æŒæœ€è¿‘çš„10æ¡è®°å¿†
            if len(self.session_memory[session_key]) > 10:
                self.session_memory[session_key] = self.session_memory[session_key][-10:]

            # æ£€ç´¢ç›¸å…³è®°å¿†ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰
            relevant_memories = []
            input_words = set(request.input_text.lower().split())

            for memory in self.session_memory[session_key][:-1]:  # æ’é™¤å½“å‰è®°å¿†
                memory_words = set(memory['input'].lower().split())
                if input_words & memory_words:  # æœ‰å…±åŒè¯æ±‡
                    relevant_memories.append(memory)

            # ç”Ÿæˆè®°å¿†å¢å¼ºå“åº”
            if relevant_memories:
                memory_context = f"åŸºäºæˆ‘ä»¬ä¹‹å‰çš„å¯¹è¯ï¼Œæˆ‘è®°å¾—æ‚¨æ›¾ç»è¯¢é—®è¿‡ç›¸å…³çš„å†…å®¹ã€‚"
            else:
                memory_context = "è¿™æ˜¯æˆ‘ä»¬åœ¨è¿™ä¸ªè¯é¢˜ä¸Šçš„æ–°å¯¹è¯ã€‚"

            return {
                'output': memory_context,
                'confidence': 0.5,
                'relevant_memories': relevant_memories[-3:] if relevant_memories else [],  # æœ€è¿‘3æ¡ç›¸å…³è®°å¿†
                'new_memory_id': interaction['id'],
                'updates': [{'type': 'simple_memory_stored', 'id': interaction['id']}],
                'reasoning_trace': [
                    {
                        'module': 'intelligent_memory',
                        'action': 'fallback_memory_processing',
                        'result': f"å­˜å‚¨è®°å¿†ï¼Œæ‰¾åˆ°{len(relevant_memories)}æ¡ç›¸å…³è®°å¿†",
                        'timestamp': datetime.now().isoformat()
                    }
                ],
                'metadata': {
                    'is_fallback': True,
                    'session_memories_count': len(self.session_memory[session_key]),
                    'relevant_count': len(relevant_memories),
                    'error': error_msg
                }
            }

        except Exception as e:
            self.logger.error(f"âŒ [IntelligentMemory] å¤‡ç”¨è®°å¿†å¤„ç†ä¹Ÿå¤±è´¥: {e}")
            return {
                'output': "æˆ‘æ­£åœ¨è®°å½•æˆ‘ä»¬çš„å¯¹è¯ï¼Œä»¥ä¾¿ä¸ºæ‚¨æä¾›æ›´å¥½çš„æœåŠ¡ã€‚",
                'confidence': 0.3,
                'relevant_memories': [],
                'new_memory_id': 'fallback',
                'updates': [],
                'reasoning_trace': [],
                'metadata': {'error': str(e)}
            }
    
    async def _store_interaction(self, request, analysis: Dict[str, Any], previous_results: Dict[str, Any]) -> MemoryNode:
        """å­˜å‚¨äº¤äº’åˆ°è®°å¿†"""
        self.logger.debug("ğŸ’¾ [IntelligentMemory] å­˜å‚¨äº¤äº’è®°å¿†...")
        
        # ç”Ÿæˆè®°å¿†ID
        memory_id = self._generate_memory_id(request.input_text)
        
        # ç¡®å®šè®°å¿†ç±»å‹
        memory_type = await self._determine_memory_type(request, analysis)
        
        # è¯„ä¼°é‡è¦æ€§
        importance = await self._assess_memory_importance(request, analysis, previous_results)
        
        # æå–æ ‡ç­¾
        tags = await self._extract_memory_tags(request, analysis)
        
        # åˆ›å»ºè®°å¿†èŠ‚ç‚¹
        memory_node = MemoryNode(
            id=memory_id,
            content=request.input_text,
            memory_type=memory_type,
            importance=importance,
            timestamp=datetime.now().isoformat(),
            tags=tags,
            metadata={
                'user_id': getattr(request, 'user_id', 'default'),
                'session_id': getattr(request, 'session_id', 'default'),
                'analysis': self._serialize_analysis(analysis),  # åºåˆ—åŒ–analysis
                'previous_results': self._serialize_results(previous_results)  # åºåˆ—åŒ–results
            }
        )
        
        # å­˜å‚¨åˆ°æ•°æ®åº“
        await self._save_memory_to_db(memory_node)
        
        # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
        self.knowledge_graph.nodes[memory_id] = memory_node
        
        return memory_node

    def _serialize_analysis(self, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """åºåˆ—åŒ–åˆ†æç»“æœï¼Œå¤„ç†ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡"""
        if not analysis:
            return {}

        serialized = {}
        for key, value in analysis.items():
            try:
                # å°è¯•JSONåºåˆ—åŒ–æµ‹è¯•
                json.dumps(value)
                serialized[key] = value
            except (TypeError, ValueError):
                # å¦‚æœæ— æ³•åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²æˆ–åŸºæœ¬ç±»å‹
                if hasattr(value, 'value'):  # æšä¸¾ç±»å‹
                    serialized[key] = value.value
                elif hasattr(value, 'name'):  # æšä¸¾ç±»å‹
                    serialized[f"{key}_name"] = value.name
                    if hasattr(value, 'value'):
                        serialized[key] = value.value
                else:
                    serialized[key] = str(value)

        return serialized

    def _serialize_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """åºåˆ—åŒ–ç»“æœæ•°æ®ï¼Œå¤„ç†ä¸å¯JSONåºåˆ—åŒ–çš„å¯¹è±¡"""
        if not results:
            return {}

        serialized = {}
        for key, value in results.items():
            try:
                # å°è¯•JSONåºåˆ—åŒ–æµ‹è¯•
                json.dumps(value)
                serialized[key] = value
            except (TypeError, ValueError):
                # å¦‚æœæ— æ³•åºåˆ—åŒ–ï¼Œè½¬æ¢ä¸ºåŸºæœ¬ç±»å‹
                if isinstance(value, dict):
                    serialized[key] = self._serialize_results(value)  # é€’å½’å¤„ç†
                elif hasattr(value, 'value'):  # æšä¸¾ç±»å‹
                    serialized[key] = value.value
                else:
                    serialized[key] = str(value)

        return serialized

    async def _retrieve_relevant_memories(self, query: str, analysis: Dict[str, Any]) -> List[MemoryNode]:
        """æ£€ç´¢ç›¸å…³è®°å¿†"""
        self.logger.debug("ğŸ” [IntelligentMemory] æ£€ç´¢ç›¸å…³è®°å¿†...")
        
        relevant_memories = []
        
        # 1. åŸºäºå…³é”®è¯çš„æ£€ç´¢
        keyword_matches = await self._keyword_based_retrieval(query, analysis)
        relevant_memories.extend(keyword_matches)
        
        # 2. åŸºäºè¯­ä¹‰çš„æ£€ç´¢
        semantic_matches = await self._semantic_based_retrieval(query, analysis)
        relevant_memories.extend(semantic_matches)
        
        # 3. åŸºäºå›¾ç»“æ„çš„æ£€ç´¢
        graph_matches = await self._graph_based_retrieval(query, analysis)
        relevant_memories.extend(graph_matches)
        
        # å»é‡å’Œæ’åº
        unique_memories = self._deduplicate_memories(relevant_memories)
        sorted_memories = await self._rank_memories_by_relevance(unique_memories, query, analysis)
        
        # æ›´æ–°è®¿é—®ç»Ÿè®¡
        for memory in sorted_memories[:5]:  # åªæ›´æ–°å‰5ä¸ªæœ€ç›¸å…³çš„
            await self._update_access_stats(memory.id)
        
        return sorted_memories[:10]  # è¿”å›å‰10ä¸ªæœ€ç›¸å…³çš„è®°å¿†
    
    async def _update_knowledge_graph(self, new_memory: MemoryNode, relevant_memories: List[MemoryNode], analysis: Dict[str, Any]):
        """æ›´æ–°çŸ¥è¯†å›¾è°±"""
        self.logger.debug("ğŸ”— [IntelligentMemory] æ›´æ–°çŸ¥è¯†å›¾è°±...")
        
        # ä¸ºæ–°è®°å¿†åˆ›å»ºä¸ç›¸å…³è®°å¿†çš„è¿æ¥
        for relevant_memory in relevant_memories[:3]:  # åªè¿æ¥å‰3ä¸ªæœ€ç›¸å…³çš„
            # è®¡ç®—å…³ç³»å¼ºåº¦
            strength = await self._calculate_relation_strength(new_memory, relevant_memory, analysis)
            
            if strength > 0.3:  # åªåˆ›å»ºå¼ºåº¦è¶³å¤Ÿçš„è¿æ¥
                # ç¡®å®šå…³ç³»ç±»å‹
                relation_type = await self._determine_relation_type(new_memory, relevant_memory, analysis)
                
                # åˆ›å»ºçŸ¥è¯†è¾¹
                edge = KnowledgeEdge(
                    source_id=new_memory.id,
                    target_id=relevant_memory.id,
                    relation_type=relation_type,
                    strength=strength,
                    timestamp=datetime.now().isoformat(),
                    metadata={'analysis': analysis}
                )
                
                self.knowledge_graph.edges.append(edge)
                
                # ä¿å­˜åˆ°æ•°æ®åº“
                await self._save_edge_to_db(edge)
    
    async def _generate_memory_enhanced_response(self, request, relevant_memories: List[MemoryNode], analysis: Dict[str, Any]) -> str:
        """ç”Ÿæˆè®°å¿†å¢å¼ºçš„å“åº”"""
        if not relevant_memories:
            return "æˆ‘ç†è§£äº†æ‚¨çš„è¯·æ±‚ï¼Œå¹¶å°†å…¶è®°å½•åœ¨è®°å¿†ä¸­ã€‚"
        
        # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = []
        for memory in relevant_memories[:3]:
            memory_context.append(f"ç›¸å…³è®°å¿†ï¼š{memory.content}")
        
        context_str = "\n".join(memory_context)
        
        # ç”Ÿæˆå¢å¼ºå“åº”
        prompt = f"""
åŸºäºä»¥ä¸‹ç›¸å…³è®°å¿†ï¼Œä¸ºç”¨æˆ·è¯·æ±‚ç”Ÿæˆå“åº”ï¼š

ç”¨æˆ·è¯·æ±‚ï¼š{request.input_text}

ç›¸å…³è®°å¿†ï¼š
{context_str}

è¯·ç”Ÿæˆä¸€ä¸ªæ•´åˆäº†è®°å¿†ä¿¡æ¯çš„æ™ºèƒ½å“åº”ã€‚
"""
        
        try:
            response = await AgentFactory.ask_llm(prompt)
            return response.strip()
        except Exception as e:
            self.logger.warning(f"ç”Ÿæˆè®°å¿†å¢å¼ºå“åº”å¤±è´¥: {e}")
            return f"åŸºäºæˆ‘çš„è®°å¿†ï¼Œæˆ‘è®°å¾—æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡ç›¸å…³è¯é¢˜ã€‚è®©æˆ‘ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚"
    
    # æ•°æ®åº“ç›¸å…³æ–¹æ³•
    async def _initialize_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        self.logger.debug("ğŸ—„ï¸ [IntelligentMemory] åˆå§‹åŒ–æ•°æ®åº“...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åˆ›å»ºè®°å¿†èŠ‚ç‚¹è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS memory_nodes (
                id TEXT PRIMARY KEY,
                content TEXT NOT NULL,
                memory_type TEXT NOT NULL,
                importance INTEGER NOT NULL,
                timestamp TEXT NOT NULL,
                access_count INTEGER DEFAULT 0,
                last_accessed TEXT,
                tags TEXT,
                metadata TEXT
            )
        ''')
        
        # åˆ›å»ºçŸ¥è¯†è¾¹è¡¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS knowledge_edges (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_id TEXT NOT NULL,
                target_id TEXT NOT NULL,
                relation_type TEXT NOT NULL,
                strength REAL NOT NULL,
                timestamp TEXT NOT NULL,
                metadata TEXT,
                FOREIGN KEY (source_id) REFERENCES memory_nodes (id),
                FOREIGN KEY (target_id) REFERENCES memory_nodes (id)
            )
        ''')
        
        # åˆ›å»ºç´¢å¼•
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_memory_type ON memory_nodes (memory_type)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_importance ON memory_nodes (importance)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON memory_nodes (timestamp)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_edges_source ON knowledge_edges (source_id)')
        cursor.execute('CREATE INDEX IF NOT EXISTS idx_edges_target ON knowledge_edges (target_id)')
        
        conn.commit()
        conn.close()
    
    async def _load_memory_to_cache(self):
        """åŠ è½½è®°å¿†åˆ°ç¼“å­˜"""
        self.logger.debug("ğŸ“¥ [IntelligentMemory] åŠ è½½è®°å¿†åˆ°ç¼“å­˜...")
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # åŠ è½½æœ€è¿‘çš„è®°å¿†èŠ‚ç‚¹ï¼ˆé™åˆ¶æ•°é‡é¿å…å†…å­˜è¿‡å¤§ï¼‰
        cursor.execute('''
            SELECT * FROM memory_nodes 
            ORDER BY timestamp DESC 
            LIMIT 1000
        ''')
        
        rows = cursor.fetchall()
        for row in rows:
            memory_node = self._row_to_memory_node(row)
            self.knowledge_graph.nodes[memory_node.id] = memory_node
        
        # åŠ è½½å¯¹åº”çš„è¾¹
        if self.knowledge_graph.nodes:
            node_ids = list(self.knowledge_graph.nodes.keys())
            placeholders = ','.join(['?' for _ in node_ids])
            
            cursor.execute(f'''
                SELECT * FROM knowledge_edges 
                WHERE source_id IN ({placeholders}) 
                OR target_id IN ({placeholders})
            ''', node_ids + node_ids)
            
            edge_rows = cursor.fetchall()
            for row in edge_rows:
                edge = self._row_to_knowledge_edge(row)
                self.knowledge_graph.edges.append(edge)
        
        conn.close()
        
        self.logger.info(f"ğŸ“¥ [IntelligentMemory] åŠ è½½äº† {len(self.knowledge_graph.nodes)} ä¸ªè®°å¿†èŠ‚ç‚¹å’Œ {len(self.knowledge_graph.edges)} æ¡è¾¹")
    
    async def _initialize_knowledge_graph(self):
        """åˆå§‹åŒ–çŸ¥è¯†å›¾è°±"""
        self.logger.debug("ğŸ•¸ï¸ [IntelligentMemory] åˆå§‹åŒ–çŸ¥è¯†å›¾è°±...")
        
        # å¦‚æœæ²¡æœ‰ç°æœ‰çš„å›¾è°±ï¼Œåˆ›å»ºåŸºç¡€ç»“æ„
        if not self.knowledge_graph.nodes:
            self.knowledge_graph = KnowledgeGraph({}, [])
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ å›¾è°±åˆ†æå’Œä¼˜åŒ–é€»è¾‘
        await self._analyze_graph_structure()
    
    async def _analyze_graph_structure(self):
        """åˆ†æå›¾è°±ç»“æ„"""
        node_count = len(self.knowledge_graph.nodes)
        edge_count = len(self.knowledge_graph.edges)
        
        self.logger.info(f"ğŸ•¸ï¸ [IntelligentMemory] çŸ¥è¯†å›¾è°±ç»“æ„: {node_count} èŠ‚ç‚¹, {edge_count} è¾¹")
        
        # è®¡ç®—å›¾è°±å¯†åº¦
        if node_count > 1:
            max_edges = node_count * (node_count - 1) / 2
            density = edge_count / max_edges if max_edges > 0 else 0
            self.logger.info(f"ğŸ•¸ï¸ [IntelligentMemory] å›¾è°±å¯†åº¦: {density:.3f}")
    
    # è¾…åŠ©æ–¹æ³•
    def _generate_memory_id(self, content: str) -> str:
        """ç”Ÿæˆè®°å¿†ID"""
        timestamp = datetime.now().isoformat()
        content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
        return f"mem_{timestamp}_{content_hash}"
    
    def _row_to_memory_node(self, row) -> MemoryNode:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºè®°å¿†èŠ‚ç‚¹"""
        return MemoryNode(
            id=row[0],
            content=row[1],
            memory_type=MemoryType(row[2]),
            importance=MemoryImportance(row[3]),
            timestamp=row[4],
            access_count=row[5] or 0,
            last_accessed=row[6] or "",
            tags=json.loads(row[7]) if row[7] else [],
            metadata=json.loads(row[8]) if row[8] else {}
        )
    
    def _row_to_knowledge_edge(self, row) -> KnowledgeEdge:
        """å°†æ•°æ®åº“è¡Œè½¬æ¢ä¸ºçŸ¥è¯†è¾¹"""
        return KnowledgeEdge(
            source_id=row[1],
            target_id=row[2],
            relation_type=row[3],
            strength=row[4],
            timestamp=row[5],
            metadata=json.loads(row[6]) if row[6] else {}
        )

    # è®°å¿†åˆ†æå’Œå¤„ç†æ–¹æ³•
    async def _determine_memory_type(self, request, analysis: Dict[str, Any]) -> MemoryType:
        """ç¡®å®šè®°å¿†ç±»å‹"""
        intent = analysis.get('semantic_analysis', {}).get('intent_analysis', {}).get('primary_intent', 'unknown')

        if intent == 'question':
            return MemoryType.EPISODIC  # é—®é¢˜é€šå¸¸æ˜¯æƒ…æ™¯è®°å¿†
        elif intent in ['request', 'command']:
            return MemoryType.PROCEDURAL  # è¯·æ±‚å’Œå‘½ä»¤æ˜¯ç¨‹åºæ€§è®°å¿†
        elif any(keyword in request.input_text for keyword in ['äº‹å®', 'çŸ¥è¯†', 'å®šä¹‰', 'æ¦‚å¿µ']):
            return MemoryType.DECLARATIVE  # äº‹å®æ€§å†…å®¹
        else:
            return MemoryType.SEMANTIC  # é»˜è®¤ä¸ºè¯­ä¹‰è®°å¿†

    async def _assess_memory_importance(self, request, analysis: Dict[str, Any], previous_results: Dict[str, Any]) -> MemoryImportance:
        """è¯„ä¼°è®°å¿†é‡è¦æ€§"""
        importance_score = 0

        # åŸºäºå¤æ‚åº¦
        complexity = analysis.get('semantic_analysis', {}).get('complexity_score', 0)
        importance_score += complexity * 2

        # åŸºäºæƒ…æ„Ÿå¼ºåº¦
        emotion_intensity = analysis.get('semantic_analysis', {}).get('emotion_analysis', {}).get('emotion_intensity', 0)
        importance_score += emotion_intensity

        # åŸºäºç”¨æˆ·æ„å›¾å¼ºåº¦
        intent_strength = analysis.get('semantic_analysis', {}).get('intent_analysis', {}).get('intent_strength', 0)
        importance_score += intent_strength

        # åŸºäºæ–‡æœ¬é•¿åº¦ï¼ˆæ›´é•¿çš„æ–‡æœ¬å¯èƒ½æ›´é‡è¦ï¼‰
        text_length_factor = min(1.0, len(request.input_text) / 200)
        importance_score += text_length_factor * 0.5

        # æ˜ å°„åˆ°é‡è¦æ€§çº§åˆ«
        if importance_score >= 3.0:
            return MemoryImportance.CRITICAL
        elif importance_score >= 2.0:
            return MemoryImportance.HIGH
        elif importance_score >= 1.0:
            return MemoryImportance.MEDIUM
        else:
            return MemoryImportance.LOW

    async def _extract_memory_tags(self, request, analysis: Dict[str, Any]) -> List[str]:
        """æå–è®°å¿†æ ‡ç­¾"""
        tags = []

        # ä»è¯­ä¹‰åˆ†æä¸­æå–å…³é”®è¯ä½œä¸ºæ ‡ç­¾
        keywords = analysis.get('semantic_analysis', {}).get('surface_analysis', {}).get('keywords', [])
        tags.extend(keywords[:5])  # æœ€å¤š5ä¸ªå…³é”®è¯æ ‡ç­¾

        # æ·»åŠ æ„å›¾æ ‡ç­¾
        intent = analysis.get('semantic_analysis', {}).get('intent_analysis', {}).get('primary_intent', '')
        if intent:
            tags.append(f"intent:{intent}")

        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾
        emotion = analysis.get('semantic_analysis', {}).get('emotion_analysis', {}).get('emotion_type', '')
        if emotion:
            tags.append(f"emotion:{emotion}")

        # æ·»åŠ å®ä½“æ ‡ç­¾
        entities = analysis.get('semantic_analysis', {}).get('surface_analysis', {}).get('entities', [])
        for entity in entities[:3]:  # æœ€å¤š3ä¸ªå®ä½“æ ‡ç­¾
            if isinstance(entity, dict) and 'type' in entity:
                tags.append(f"entity:{entity['type']}")

        return list(set(tags))  # å»é‡

    async def _save_memory_to_db(self, memory_node: MemoryNode):
        """ä¿å­˜è®°å¿†åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT OR REPLACE INTO memory_nodes
            (id, content, memory_type, importance, timestamp, access_count, last_accessed, tags, metadata)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
        ''', (
            memory_node.id,
            memory_node.content,
            memory_node.memory_type.value,
            memory_node.importance.value,
            memory_node.timestamp,
            memory_node.access_count,
            memory_node.last_accessed,
            json.dumps(memory_node.tags, ensure_ascii=False),
            json.dumps(memory_node.metadata, ensure_ascii=False)
        ))

        conn.commit()
        conn.close()

    async def _save_edge_to_db(self, edge: KnowledgeEdge):
        """ä¿å­˜çŸ¥è¯†è¾¹åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute('''
            INSERT INTO knowledge_edges
            (source_id, target_id, relation_type, strength, timestamp, metadata)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (
            edge.source_id,
            edge.target_id,
            edge.relation_type,
            edge.strength,
            edge.timestamp,
            json.dumps(edge.metadata, ensure_ascii=False)
        ))

        conn.commit()
        conn.close()

    # è®°å¿†æ£€ç´¢æ–¹æ³•
    async def _keyword_based_retrieval(self, query: str, analysis: Dict[str, Any]) -> List[MemoryNode]:
        """åŸºäºå…³é”®è¯çš„è®°å¿†æ£€ç´¢"""
        query_keywords = set(query.lower().split())
        matches = []

        for memory_node in self.knowledge_graph.nodes.values():
            # æ£€æŸ¥å†…å®¹åŒ¹é…
            content_keywords = set(memory_node.content.lower().split())
            overlap = len(query_keywords.intersection(content_keywords))

            if overlap > 0:
                # è®¡ç®—åŒ¹é…åˆ†æ•°
                score = overlap / len(query_keywords.union(content_keywords))
                matches.append((memory_node, score))

        # æŒ‰åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return [match[0] for match in matches[:20]]

    async def _semantic_based_retrieval(self, query: str, analysis: Dict[str, Any]) -> List[MemoryNode]:
        """åŸºäºè¯­ä¹‰çš„è®°å¿†æ£€ç´¢"""
        matches = []

        # æå–æŸ¥è¯¢çš„è¯­ä¹‰ç‰¹å¾
        query_intent = analysis.get('semantic_analysis', {}).get('intent_analysis', {}).get('primary_intent', '')
        query_emotion = analysis.get('semantic_analysis', {}).get('emotion_analysis', {}).get('emotion_type', '')
        query_keywords = analysis.get('semantic_analysis', {}).get('surface_analysis', {}).get('keywords', [])

        for memory_node in self.knowledge_graph.nodes.values():
            semantic_score = 0

            # æ„å›¾åŒ¹é…
            if f"intent:{query_intent}" in memory_node.tags:
                semantic_score += 0.3

            # æƒ…æ„ŸåŒ¹é…
            if f"emotion:{query_emotion}" in memory_node.tags:
                semantic_score += 0.2

            # å…³é”®è¯åŒ¹é…
            keyword_overlap = len(set(query_keywords).intersection(set(memory_node.tags)))
            semantic_score += keyword_overlap * 0.1

            if semantic_score > 0.1:
                matches.append((memory_node, semantic_score))

        # æŒ‰è¯­ä¹‰åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return [match[0] for match in matches[:15]]

    async def _graph_based_retrieval(self, query: str, analysis: Dict[str, Any]) -> List[MemoryNode]:
        """åŸºäºå›¾ç»“æ„çš„è®°å¿†æ£€ç´¢"""
        matches = []

        # é¦–å…ˆæ‰¾åˆ°ä¸æŸ¥è¯¢æœ€ç›¸å…³çš„èŠ‚ç‚¹
        initial_matches = await self._keyword_based_retrieval(query, analysis)

        if not initial_matches:
            return []

        # é€šè¿‡å›¾ç»“æ„æ‰©å±•æœç´¢
        visited = set()
        for initial_node in initial_matches[:3]:  # åªä»å‰3ä¸ªæœ€ç›¸å…³çš„èŠ‚ç‚¹å¼€å§‹
            connected_nodes = await self._get_connected_nodes(initial_node.id, max_depth=2)
            for node_id, distance in connected_nodes:
                if node_id not in visited and node_id in self.knowledge_graph.nodes:
                    visited.add(node_id)
                    # è·ç¦»è¶Šè¿‘ï¼Œåˆ†æ•°è¶Šé«˜
                    score = 1.0 / (distance + 1)
                    matches.append((self.knowledge_graph.nodes[node_id], score))

        # æŒ‰åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        return [match[0] for match in matches[:10]]

    async def _get_connected_nodes(self, node_id: str, max_depth: int = 2) -> List[Tuple[str, int]]:
        """è·å–è¿æ¥çš„èŠ‚ç‚¹"""
        connected = []
        visited = {node_id}
        queue = [(node_id, 0)]

        while queue and len(connected) < 20:  # é™åˆ¶ç»“æœæ•°é‡
            current_id, depth = queue.pop(0)

            if depth >= max_depth:
                continue

            # æŸ¥æ‰¾æ‰€æœ‰è¿æ¥çš„è¾¹
            for edge in self.knowledge_graph.edges:
                next_id = None
                if edge.source_id == current_id and edge.target_id not in visited:
                    next_id = edge.target_id
                elif edge.target_id == current_id and edge.source_id not in visited:
                    next_id = edge.source_id

                if next_id:
                    visited.add(next_id)
                    connected.append((next_id, depth + 1))
                    queue.append((next_id, depth + 1))

        return connected

    def _deduplicate_memories(self, memories: List[MemoryNode]) -> List[MemoryNode]:
        """å»é‡è®°å¿†"""
        seen_ids = set()
        unique_memories = []

        for memory in memories:
            if memory.id not in seen_ids:
                seen_ids.add(memory.id)
                unique_memories.append(memory)

        return unique_memories

    async def _rank_memories_by_relevance(self, memories: List[MemoryNode], query: str, analysis: Dict[str, Any]) -> List[MemoryNode]:
        """æŒ‰ç›¸å…³æ€§æ’åºè®°å¿†"""
        scored_memories = []

        for memory in memories:
            relevance_score = await self._calculate_relevance_score(memory, query, analysis)
            scored_memories.append((memory, relevance_score))

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        scored_memories.sort(key=lambda x: x[1], reverse=True)
        return [memory for memory, score in scored_memories]

    async def _calculate_relevance_score(self, memory: MemoryNode, query: str, analysis: Dict[str, Any]) -> float:
        """è®¡ç®—ç›¸å…³æ€§åˆ†æ•°"""
        score = 0.0

        # åŸºç¡€æ–‡æœ¬ç›¸ä¼¼åº¦
        query_words = set(query.lower().split())
        memory_words = set(memory.content.lower().split())
        text_similarity = len(query_words.intersection(memory_words)) / len(query_words.union(memory_words))
        score += text_similarity * 0.4

        # é‡è¦æ€§æƒé‡
        importance_weight = memory.importance.value / 4.0  # å½’ä¸€åŒ–åˆ°0-1
        score += importance_weight * 0.2

        # è®¿é—®é¢‘ç‡æƒé‡
        access_weight = min(1.0, memory.access_count / 10.0)  # å½’ä¸€åŒ–
        score += access_weight * 0.1

        # æ—¶é—´è¡°å‡ï¼ˆæœ€è¿‘çš„è®°å¿†æ›´ç›¸å…³ï¼‰
        try:
            memory_time = datetime.fromisoformat(memory.timestamp.replace('Z', '+00:00'))
            time_diff = datetime.now() - memory_time.replace(tzinfo=None)
            time_decay = max(0.1, 1.0 - (time_diff.days / 365.0))  # ä¸€å¹´åè¡°å‡åˆ°0.1
            score += time_decay * 0.3
        except:
            score += 0.1  # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œç»™ä¸ªåŸºç¡€åˆ†æ•°

        return score

    async def _update_access_stats(self, memory_id: str):
        """æ›´æ–°è®¿é—®ç»Ÿè®¡"""
        if memory_id in self.knowledge_graph.nodes:
            memory_node = self.knowledge_graph.nodes[memory_id]
            memory_node.access_count += 1
            memory_node.last_accessed = datetime.now().isoformat()

            # æ›´æ–°æ•°æ®åº“
            await self._save_memory_to_db(memory_node)

    # çŸ¥è¯†å›¾è°±ç›¸å…³æ–¹æ³•
    async def _calculate_relation_strength(self, memory1: MemoryNode, memory2: MemoryNode, analysis: Dict[str, Any]) -> float:
        """è®¡ç®—å…³ç³»å¼ºåº¦"""
        strength = 0.0

        # æ ‡ç­¾é‡å 
        tag_overlap = len(set(memory1.tags).intersection(set(memory2.tags)))
        strength += tag_overlap * 0.2

        # å†…å®¹ç›¸ä¼¼åº¦
        words1 = set(memory1.content.lower().split())
        words2 = set(memory2.content.lower().split())
        content_similarity = len(words1.intersection(words2)) / len(words1.union(words2))
        strength += content_similarity * 0.5

        # æ—¶é—´æ¥è¿‘åº¦
        try:
            time1 = datetime.fromisoformat(memory1.timestamp.replace('Z', '+00:00'))
            time2 = datetime.fromisoformat(memory2.timestamp.replace('Z', '+00:00'))
            time_diff = abs((time1 - time2).total_seconds())
            time_proximity = max(0, 1.0 - (time_diff / (24 * 3600)))  # ä¸€å¤©å†…çš„æ¥è¿‘åº¦
            strength += time_proximity * 0.3
        except:
            pass

        return min(1.0, strength)

    async def _determine_relation_type(self, memory1: MemoryNode, memory2: MemoryNode, analysis: Dict[str, Any]) -> str:
        """ç¡®å®šå…³ç³»ç±»å‹"""
        # åŸºäºæ ‡ç­¾ç¡®å®šå…³ç³»ç±»å‹
        common_tags = set(memory1.tags).intersection(set(memory2.tags))

        if any('intent:' in tag for tag in common_tags):
            return 'similar_intent'
        elif any('emotion:' in tag for tag in common_tags):
            return 'similar_emotion'
        elif any('entity:' in tag for tag in common_tags):
            return 'shared_entity'
        else:
            return 'semantic_similarity'

    async def _create_fallback_memory_response(self, request, error_msg: str) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨è®°å¿†å“åº”"""
        return {
            'output': f"è®°å¿†å¤„ç†é‡åˆ°é—®é¢˜ï¼š{error_msg}ã€‚æˆ‘ä»ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚",
            'confidence': 0.3,
            'relevant_memories': [],
            'new_memory_id': None,
            'updates': [],
            'reasoning_trace': [
                {
                    'module': 'intelligent_memory',
                    'action': 'fallback',
                    'error': error_msg,
                    'timestamp': datetime.now().isoformat()
                }
            ],
            'metadata': {'error': True, 'fallback': True}
        }
