"""
æ™ºèƒ½æ¨¡å¼æ£€æµ‹å™¨ä½¿ç”¨ç¤ºä¾‹
"""

import asyncio
import json
from detector import IntelligentModeDetector

class MockLLMClient:
    """æ¨¡æ‹ŸLLMå®¢æˆ·ç«¯"""
    
    async def generate(self, prompt: str) -> str:
        """æ¨¡æ‹ŸAIç”Ÿæˆå“åº”"""
        
        # ç®€å•çš„æ¨¡æ‹Ÿé€»è¾‘
        if "å¿«é€Ÿ" in prompt or "ç®€å•" in prompt or "å¸®æˆ‘" in prompt:
            return json.dumps({
                "recommended_mode": "daily",
                "confidence": 0.8,
                "reasoning": "æ£€æµ‹åˆ°å¿«é€Ÿä»»åŠ¡éœ€æ±‚ï¼Œæ¨èä½¿ç”¨æ—¥å¸¸æ¨¡å¼",
                "key_factors": ["å¿«é€Ÿéœ€æ±‚", "ç®€å•ä»»åŠ¡"],
                "alternative_mode": "professional",
                "risk_assessment": "ä½é£é™©é€‰æ‹©"
            })
        elif "åˆ¶å®š" in prompt or "å»ºç«‹" in prompt or "åˆ†æ" in prompt:
            return json.dumps({
                "recommended_mode": "professional",
                "confidence": 0.85,
                "reasoning": "æ£€æµ‹åˆ°ç³»ç»Ÿæ€§ä»»åŠ¡éœ€æ±‚ï¼Œæ¨èä½¿ç”¨ä¸“ä¸šæ¨¡å¼",
                "key_factors": ["ç³»ç»Ÿæ€§ä»»åŠ¡", "ä¸“ä¸šéœ€æ±‚"],
                "alternative_mode": "daily",
                "risk_assessment": "æ ‡å‡†ä¸“ä¸šä»»åŠ¡"
            })
        else:
            return json.dumps({
                "recommended_mode": "daily",
                "confidence": 0.6,
                "reasoning": "é»˜è®¤æ¨èæ—¥å¸¸æ¨¡å¼",
                "key_factors": ["é»˜è®¤é€‰æ‹©"],
                "alternative_mode": "professional",
                "risk_assessment": "ä¸­ç­‰é£é™©"
            })

async def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    mock_llm = MockLLMClient()
    detector = IntelligentModeDetector(llm_client=mock_llm)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "input": "å¸®æˆ‘æŠŠè¿™ä¸ªExcelè¡¨æ ¼æ•´ç†æˆWordæŠ¥å‘Š",
            "expected": "daily",
            "description": "å…¸å‹çš„æ—¥å¸¸åŠå…¬ä»»åŠ¡"
        },
        {
            "input": "åˆ¶å®šå…¬å¸çš„æ•°æ®ç®¡ç†è§„èŒƒå’Œæ ‡å‡†æµç¨‹",
            "expected": "professional", 
            "description": "ä¼ä¸šçº§è§„èŒƒåˆ¶å®šä»»åŠ¡"
        },
        {
            "input": "å¿«é€Ÿå†™ä¸ªä¼šè®®çºªè¦",
            "expected": "daily",
            "description": "å¿«é€Ÿç®€å•ä»»åŠ¡"
        },
        {
            "input": "æ·±å…¥åˆ†æå¸‚åœºç«äº‰æ ¼å±€ï¼Œåˆ¶å®šæˆ˜ç•¥åº”å¯¹æ–¹æ¡ˆ",
            "expected": "professional",
            "description": "æˆ˜ç•¥åˆ†æä»»åŠ¡"
        },
        {
            "input": "ç»™å®¢æˆ·å†™ä¸€ä»½æ­£å¼çš„é¡¹ç›®ææ¡ˆ",
            "expected": "professional",
            "description": "å¯¹å¤–æ­£å¼æ–‡æ¡£"
        },
        {
            "input": "æ•´ç†ä¸€ä¸‹ä»Šå¤©çš„å·¥ä½œæ¸…å•",
            "expected": "daily",
            "description": "ä¸ªäººå·¥ä½œæ•´ç†"
        }
    ]
    
    print("ğŸ§  æ™ºèƒ½æ¨¡å¼æ£€æµ‹å™¨æµ‹è¯•")
    print("=" * 50)
    
    correct_predictions = 0
    total_predictions = len(test_cases)
    
    for i, case in enumerate(test_cases, 1):
        print(f"\nğŸ“ æµ‹è¯•æ¡ˆä¾‹ {i}: {case['description']}")
        print(f"è¾“å…¥: {case['input']}")
        print(f"æœŸæœ›æ¨¡å¼: {case['expected']}")
        
        # æ‰§è¡Œæ£€æµ‹
        result = await detector.detect_mode(case['input'])
        
        predicted_mode = result['recommended_mode']
        confidence = result['confidence']
        reasoning = result['reasoning']
        
        print(f"æ£€æµ‹ç»“æœ: {predicted_mode}")
        print(f"ç½®ä¿¡åº¦: {confidence:.2f}")
        print(f"æ¨ç†è¿‡ç¨‹: {reasoning}")
        
        # æ£€æŸ¥å‡†ç¡®æ€§
        is_correct = predicted_mode == case['expected']
        if is_correct:
            correct_predictions += 1
            print("âœ… é¢„æµ‹æ­£ç¡®")
        else:
            print("âŒ é¢„æµ‹é”™è¯¯")
        
        # æ˜¾ç¤ºè¯¦ç»†åˆ†æï¼ˆå¯é€‰ï¼‰
        if result.get('detailed_analysis'):
            print("\nğŸ“Š è¯¦ç»†åˆ†æ:")
            detailed = result['detailed_analysis']
            
            # å…³é”®è¯åˆ†æ
            keyword_analysis = detailed.get('keyword_analysis', {})
            print(f"  å…³é”®è¯åˆ†æ: {keyword_analysis.get('dominant_mode', 'N/A')} (ç½®ä¿¡åº¦: {keyword_analysis.get('confidence', 0):.2f})")
            
            # è¯­è¨€æ¨¡å¼åˆ†æ
            pattern_analysis = detailed.get('pattern_analysis', {})
            print(f"  è¯­è¨€æ¨¡å¼: {pattern_analysis.get('dominant_mode', 'N/A')} (ç½®ä¿¡åº¦: {pattern_analysis.get('confidence', 0):.2f})")
            
            # å¤æ‚åº¦åˆ†æ
            complexity_analysis = detailed.get('complexity_analysis', {})
            print(f"  å¤æ‚åº¦: {complexity_analysis.get('primary_complexity', 'N/A')} â†’ {complexity_analysis.get('suggested_mode', 'N/A')}")
            
            # èåˆç»“æœ
            fusion_result = detailed.get('fusion_result', {})
            print(f"  èåˆç½®ä¿¡åº¦: {fusion_result.get('confidence', 0):.2f}")
            print(f"  ä¸€è‡´æ€§: {fusion_result.get('consensus_rate', 0):.2f}")
        
        print("-" * 40)
    
    # æ€»ç»“
    accuracy = correct_predictions / total_predictions
    print(f"\nğŸ“ˆ æµ‹è¯•æ€»ç»“:")
    print(f"æ€»æµ‹è¯•æ¡ˆä¾‹: {total_predictions}")
    print(f"æ­£ç¡®é¢„æµ‹: {correct_predictions}")
    print(f"å‡†ç¡®ç‡: {accuracy:.1%}")
    
    if accuracy >= 0.8:
        print("ğŸ‰ æ£€æµ‹å™¨è¡¨ç°ä¼˜ç§€ï¼")
    elif accuracy >= 0.6:
        print("ğŸ‘ æ£€æµ‹å™¨è¡¨ç°è‰¯å¥½")
    else:
        print("âš ï¸ æ£€æµ‹å™¨éœ€è¦æ”¹è¿›")

async def test_batch_detection():
    """æµ‹è¯•æ‰¹é‡æ£€æµ‹"""
    
    print("\nğŸ”„ æ‰¹é‡æ£€æµ‹æµ‹è¯•")
    print("=" * 30)
    
    mock_llm = MockLLMClient()
    detector = IntelligentModeDetector(llm_client=mock_llm)
    
    batch_inputs = [
        "å¸®æˆ‘å†™ä¸ªé‚®ä»¶",
        "åˆ¶å®šè¥é”€ç­–ç•¥",
        "æ•´ç†å®¢æˆ·èµ„æ–™",
        "åˆ†æè´¢åŠ¡æ•°æ®",
        "å¿«é€Ÿåšä¸ªè¡¨æ ¼"
    ]
    
    results = await detector.batch_detect_mode(batch_inputs)
    
    for i, (input_text, result) in enumerate(zip(batch_inputs, results), 1):
        print(f"{i}. {input_text}")
        print(f"   â†’ {result.get('recommended_mode', 'error')} (ç½®ä¿¡åº¦: {result.get('confidence', 0):.2f})")

async def test_feedback_learning():
    """æµ‹è¯•åé¦ˆå­¦ä¹ """
    
    print("\nğŸ“š åé¦ˆå­¦ä¹ æµ‹è¯•")
    print("=" * 30)
    
    mock_llm = MockLLMClient()
    detector = IntelligentModeDetector(llm_client=mock_llm)
    
    # æ¨¡æ‹Ÿä¸€ä¸ªæ£€æµ‹ç»“æœ
    test_input = "å¸®æˆ‘åˆ¶å®šé¡¹ç›®è®¡åˆ’"
    result = await detector.detect_mode(test_input)
    
    print(f"åŸå§‹æ£€æµ‹: {result['recommended_mode']} (ç½®ä¿¡åº¦: {result['confidence']:.2f})")
    
    # æ¨¡æ‹Ÿç”¨æˆ·åé¦ˆï¼ˆå‡è®¾ç”¨æˆ·è®¤ä¸ºåº”è¯¥æ˜¯ä¸“ä¸šæ¨¡å¼ï¼‰
    user_feedback = {
        "actual_mode": "professional",
        "satisfaction": 3,  # 1-5åˆ†
        "comments": "è¿™ä¸ªä»»åŠ¡éœ€è¦æ›´ä¸“ä¸šçš„å¤„ç†æ–¹å¼"
    }
    
    # è®°å½•åé¦ˆ
    success = await detector.record_user_feedback(result, user_feedback)
    print(f"åé¦ˆè®°å½•: {'æˆåŠŸ' if success else 'å¤±è´¥'}")
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = await detector.get_performance_stats()
    print(f"æ£€æµ‹ç»Ÿè®¡: {stats}")

async def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    
    print("\nğŸ“Š æ€§èƒ½ç›‘æ§æµ‹è¯•")
    print("=" * 30)
    
    mock_llm = MockLLMClient()
    detector = IntelligentModeDetector(llm_client=mock_llm)
    
    # è·å–åˆ†æå™¨ä¿¡æ¯
    analyzer_info = detector.get_analyzer_info()
    print("åˆ†æå™¨ä¿¡æ¯:")
    for analyzer, info in analyzer_info.items():
        print(f"  {analyzer}: {info}")
    
    # è·å–æ€§èƒ½ç»Ÿè®¡
    stats = await detector.get_performance_stats()
    print(f"\næ€§èƒ½ç»Ÿè®¡:")
    print(f"  æ£€æµ‹ç»Ÿè®¡: {stats.get('detection_stats', {})}")
    print(f"  å‡†ç¡®ç‡ç»Ÿè®¡: {stats.get('accuracy_stats', {})}")
    print(f"  èåˆæƒé‡: {stats.get('fusion_weights', {})}")

if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    asyncio.run(main())
    asyncio.run(test_batch_detection())
    asyncio.run(test_feedback_learning())
    asyncio.run(test_performance_monitoring())
