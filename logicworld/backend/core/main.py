# ==============================================================================
#  Initialization: Load .env FIRST, then import other modules
# ==============================================================================
import os
from dotenv import load_dotenv

# This is now the very first thing that runs.
# æŸ¥æ‰¾.envæ–‡ä»¶ï¼Œå…ˆæ£€æŸ¥å½“å‰ç›®å½•ï¼Œç„¶åæ£€æŸ¥ä¸Šçº§ç›®å½•ï¼Œæœ€åæ£€æŸ¥project-filesç›®å½•
env_path = None
if os.path.exists('.env'):
    env_path = '.env'
elif os.path.exists('.env_key'):
    env_path = '.env_key'
elif os.path.exists('../.env'):
    env_path = '../.env'
elif os.path.exists('../../project-files/.env'):
    env_path = '../../project-files/.env'

if env_path:
    load_dotenv(env_path)
    print(f"[SUCCESS] Environment file loaded: {env_path}")
else:
    print("[WARN] No .env file found, using default configuration")

# Now, import other modules that might depend on environment variables
from fastapi import FastAPI, HTTPException, Request, Depends, BackgroundTasks, Form, Body, WebSocket, WebSocketDisconnect, File, UploadFile
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
import json
import logging
from typing import List, Dict, Any, Optional, Union, Tuple, Callable, Awaitable

# è®¾ç½®logger
logger = logging.getLogger(__name__)
import asyncio
import re
import time
import random
import subprocess
# import spacy # <-- This was the missing import
from pathlib import Path
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).parent.parent))

from ai.prompt_lib import render_prompt, add_memory

from openai import AsyncOpenAI
from pydantic import BaseModel, Field
import uvicorn

# --- Local Imports ---
# These are now AFTER load_dotenv()
from agent_system.orchestrator import Orchestrator
from tools import tool_registry
# å¯¼å…¥å†…éƒ¨å·¥å…·æ¨¡å—
from tools.internal_tools import HTTPRequestTool, DataAnalysisTool, http_tool, data_tool
# å¯¼å…¥ç»Ÿä¸€å·¥å…·ç³»ç»Ÿ - æ›¿æ¢MCPå·¥å…·ç³»ç»Ÿ
from tools.unified_tool_system import unified_tool_system, execute_tool_call, get_available_tools
# å¯¼å…¥å·¥å…·è·¯ç”±å™¨
from tools.tool_router import tool_router
# MCPå·¥å…·å·²å®Œå…¨ç§»é™¤
# from modao_proxy import router as modao_router  # æ¨¡å—ä¸å­˜åœ¨ï¼Œæš‚æ—¶æ³¨é‡Š
from utils.schemas import Node, Edge, WorkflowPayload, NodeData, Position

# åœ¨å¯åŠ¨æ—¶æ³¨å†Œå†…éƒ¨ä¸åŸå­å·¥å…·ï¼Œç¡®ä¿å·¥å…·å¯ç”¨
try:
    from tools.internal_tools import register_internal_tools
    register_internal_tools()
except Exception as e:
    print(f"[WARN] å†…éƒ¨å·¥å…·æ³¨å†Œå¤±è´¥: {e}")

try:
    from agent_system.atomic_tools import register_atomic_tools
    register_atomic_tools()
except Exception as e:
    print(f"[WARN] åŸå­å·¥å…·æ³¨å†Œå¤±è´¥: {e}")

# å·¥å…·åŠ è½½å™¨å·²åˆ é™¤ - ç°åœ¨å®Œå…¨ä½¿ç”¨AIå·¥ä½œæµæ„å»ºå™¨

# å¯¼å…¥é…ç½®å’Œç›‘æ§æ¨¡å—
from utils.config_manager import get_config, get_cors_config
from utils.monitoring import metrics_collector, alert_manager, health_checker

# å¯¼å…¥é€»è¾‘æ™ºæ…§ç³»ç»Ÿ
try:
    from ai.brain_system.brain_api import brain_router
    LOGIC_WISDOM_SYSTEM_AVAILABLE = True
except ImportError as e:
    print(f"[WARN] é€»è¾‘æ™ºæ…§ç³»ç»Ÿå¯¼å…¥å¤±è´¥: {e}")
    LOGIC_WISDOM_SYSTEM_AVAILABLE = False
    brain_router = None

# Reverse proxy for Modao community services
# (å·²åœ¨ä¸Šé¢å¯¼å…¥)

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Global Variables & Clients ---
# Initialize the FastAPI app
app = FastAPI(
    title="æ€ç»´å¯¼å›¾ AI åç«¯",
    version="1.0.0",
    swagger_ui_parameters={"lang": "zh-CN"}
)

# ==== æœ¬åœ° Swagger-UI é™æ€èµ„æº ====
from fastapi.staticfiles import StaticFiles
from fastapi.openapi.docs import get_swagger_ui_html
# è·å– swagger_ui_bundle çš„é™æ€æ–‡ä»¶ç›®å½•ï¼ˆå…¼å®¹æ—  dist_path å±æ€§çš„ç‰ˆæœ¬ï¼‰
import swagger_ui_bundle

# å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ç›®å½•ç»“æ„
swagger_path = None

# 1) æ–°ç‰ˆæœ¬æä¾› dist_path
if hasattr(swagger_ui_bundle, "dist_path"):
    swagger_path = swagger_ui_bundle.dist_path
else:
    import pkg_resources, pathlib, os
    vendor_base = pathlib.Path(pkg_resources.resource_filename("swagger_ui_bundle", "vendor"))
    # å– vendor ä¸‹ç¬¬ä¸€ä¸ªä»¥ swagger-ui- å¼€å¤´çš„ç›®å½•
    for p in vendor_base.iterdir():
        if p.is_dir() and p.name.startswith("swagger-ui-"):
            swagger_path = p
            break

if not swagger_path or not swagger_path.exists():
    raise RuntimeError("æ— æ³•æ‰¾åˆ° swagger-ui é™æ€èµ„æºç›®å½•ï¼Œè¯·æ£€æŸ¥ swagger_ui_bundle å®‰è£…")

# æŒ‚è½½ Modao åå‘ä»£ç†è·¯ç”±ï¼ˆæ­¤æ—¶ app å·²å®šä¹‰ï¼‰
# app.include_router(modao_router)  # æ¨¡å—ä¸å­˜åœ¨ï¼Œæš‚æ—¶æ³¨é‡Š
# MCPå¹³å°ç®¡ç†è·¯ç”±å·²ç§»é™¤

# æŒ‚è½½å‰ç«¯é…ç½®ç®¡ç†è·¯ç”±
from utils.frontend_config import router as frontend_config_router
app.include_router(frontend_config_router)

# æŒ‚è½½å‰ç«¯é…ç½®ç®¡ç†è·¯ç”±åˆ° /api/tools å‰ç¼€ï¼ˆä¸ºäº†å…¼å®¹å‰ç«¯è°ƒç”¨ï¼‰
app.include_router(frontend_config_router, prefix="/api/tools")

# æ·»åŠ æµ‹è¯•ç«¯ç‚¹
@app.get("/api/tools/test")
async def test_api_tools():
    """æµ‹è¯•APIå·¥å…·è·¯ç”±æ˜¯å¦å·¥ä½œ"""
    return {"status": "success", "message": "API tools route is working"}

@app.post("/api/tools/test-post")
async def test_api_tools_post(data: dict):
    """æµ‹è¯•POSTè¯·æ±‚"""
    return {"status": "success", "message": "POST request received", "data": data}

# æ·»åŠ æ›´æ–°å·¥å…·çŠ¶æ€çš„ç«¯ç‚¹
@app.post("/api/tools/update-status/{tool_id}")
async def update_tool_status(tool_id: str, status_data: dict):
    """æ›´æ–°å·¥å…·æµ‹è¯•å’Œæ‰¹å‡†çŠ¶æ€"""
    try:
        tested = status_data.get("tested")
        approved = status_data.get("approved")

        await _update_tool_test_status(tool_id, tested, approved)

        return {
            "success": True,
            "message": "å·¥å…·çŠ¶æ€æ›´æ–°æˆåŠŸ",
            "tool_id": tool_id,
            "tested": tested,
            "approved": approved
        }
    except Exception as e:
        return {
            "success": False,
            "message": f"æ›´æ–°å¤±è´¥: {str(e)}"
        }

# æµ‹è¯•è·¯ç”±å·²ç§»é™¤ - å·¥å…·å¯ä»¥ç›´æ¥ä½¿ç”¨ï¼Œæ— éœ€æµ‹è¯•

# æ–°å¢ï¼šå®‰è£… uv åŒ…ç®¡ç†å™¨çš„ç«¯ç‚¹
@app.post("/api/tools/install-uv")
async def install_uv_package_manager():
    """å®‰è£… uv åŒ…ç®¡ç†å™¨"""
    try:
        import subprocess
        import sys

        print("[WRENCH2] å¼€å§‹å®‰è£… uv åŒ…ç®¡ç†å™¨...")

        # ä½¿ç”¨ pip å®‰è£… uv
        result = subprocess.run([
            sys.executable, "-m", "pip", "install", "uv"
        ], capture_output=True, text=True, timeout=300)

        if result.returncode == 0:
            print("[OK] uv å®‰è£…æˆåŠŸ!")
            print(f"è¾“å‡º: {result.stdout}")

            # éªŒè¯å®‰è£…
            verify_result = subprocess.run([
                "uv", "--version"
            ], capture_output=True, text=True, timeout=30)

            if verify_result.returncode == 0:
                return {
                    "success": True,
                    "message": f"uv å®‰è£…æˆåŠŸ! ç‰ˆæœ¬: {verify_result.stdout.strip()}",
                    "details": {
                        "install_output": result.stdout,
                        "version": verify_result.stdout.strip()
                    }
                }
            else:
                return {
                    "success": True,
                    "message": "uv å®‰è£…æˆåŠŸï¼Œä½†ç‰ˆæœ¬éªŒè¯å¤±è´¥",
                    "details": {
                        "install_output": result.stdout,
                        "verify_error": verify_result.stderr
                    }
                }
        else:
            print(f"[ERROR] uv å®‰è£…å¤±è´¥: {result.stderr}")
            return {
                "success": False,
                "message": f"uv å®‰è£…å¤±è´¥: {result.stderr}",
                "details": {
                    "stdout": result.stdout,
                    "stderr": result.stderr
                }
            }

    except subprocess.TimeoutExpired:
        return {
            "success": False,
            "message": "å®‰è£…è¶…æ—¶ï¼ˆ5åˆ†é’Ÿï¼‰",
            "details": None
        }
    except Exception as e:
        print(f"[ERROR] å®‰è£…è¿‡ç¨‹å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"å®‰è£…è¿‡ç¨‹å‡ºé”™: {str(e)}",
            "details": None
        }

# MCPå·¥å…·æµ‹è¯•APIç«¯ç‚¹å·²ç§»é™¤ - å·¥å…·å¯ä»¥ç›´æ¥ä½¿ç”¨

# æ–°å¢ï¼šMCPå·¥å…·åˆ†äº«åˆ°ç³»ç»ŸAPIç«¯ç‚¹
@app.post("/api/tools/share-to-system")
async def share_mcp_tool_to_system(request: dict):
    """å°†MCPå·¥å…·åˆ†äº«åˆ°ç³»ç»Ÿå·¥å…·åº“"""
    try:
        tool_id = request.get("toolId")
        share = request.get("share", True)

        print(f"ğŸŒ åˆ†äº«å·¥å…·åˆ°ç³»ç»Ÿè¯·æ±‚: {tool_id}, åˆ†äº«çŠ¶æ€: {share}")

        # æ›´æ–°å·¥å…·åˆ†äº«çŠ¶æ€
        await _update_tool_test_status(tool_id, tested=None, approved=share)

        if share:
            # åŒæ­¥åˆ°ç³»ç»Ÿå·¥å…·åº“
            await _sync_tool_to_system(tool_id)

        return {
            "success": True,
            "message": "å·¥å…·å·²åˆ†äº«åˆ°ç³»ç»Ÿå·¥å…·åº“" if share else "å·¥å…·åˆ†äº«å·²å–æ¶ˆ",
            "shared": share
        }

    except Exception as e:
        print(f"[ERROR] å·¥å…·åˆ†äº«å‡ºé”™: {str(e)}")
        return {
            "success": False,
            "message": f"Share error: {str(e)}"
        }

# å¤æ‚çš„å·¥å…·åŠŸèƒ½éªŒè¯é€»è¾‘å·²ç§»é™¤ - å·¥å…·å¯ä»¥ç›´æ¥ä½¿ç”¨



# Excelæµ‹è¯•åŠŸèƒ½å·²ç§»é™¤ - å·¥å…·å¯ä»¥ç›´æ¥ä½¿ç”¨


async def test_generic_functionality(process, steps, tool_name):
    """é€šç”¨å·¥å…·åŠŸèƒ½æµ‹è¯•"""
    try:
        print(f"[WRENCH2] å¼€å§‹æµ‹è¯•é€šç”¨å·¥å…·åŠŸèƒ½: {tool_name}")

        # è·å–å·¥å…·åˆ—è¡¨
        tools_request = {
            "jsonrpc": "2.0",
            "id": 2,
            "method": "tools/list"
        }

        process.stdin.write((json.dumps(tools_request) + "\n").encode())
        await process.stdin.drain()

        # è¯»å–å·¥å…·åˆ—è¡¨å“åº”
        tools_response_line = await asyncio.wait_for(process.stdout.readline(), timeout=10)
        tools_response = json.loads(tools_response_line.decode().strip())

        print(f"[WRENCH2] å·¥å…·åˆ—è¡¨å“åº”: {str(tools_response)[:200]}...")

        if tools_response.get("result") and "tools" in tools_response["result"]:
            tools = tools_response["result"]["tools"]
            print(f"[OK] å·¥å…·åˆ—è¡¨è·å–æˆåŠŸï¼Œå…± {len(tools)} ä¸ªå·¥å…·")

            if tools:
                # é€‰æ‹©ç¬¬ä¸€ä¸ªå·¥å…·è¿›è¡Œæµ‹è¯•
                test_tool = tools[0]
                tool_name_to_test = test_tool.get("name", "unknown")
                print(f"[DART] é€‰æ‹©æµ‹è¯•å·¥å…·: {tool_name_to_test}")

                # å°è¯•è°ƒç”¨å·¥å…·ï¼ˆä½¿ç”¨æœ€å°å‚æ•°ï¼‰
                call_request = {
                    "jsonrpc": "2.0",
                    "id": 3,
                    "method": "tools/call",
                    "params": {
                        "name": tool_name_to_test,
                        "arguments": {}  # ä½¿ç”¨ç©ºå‚æ•°è¿›è¡Œæµ‹è¯•
                    }
                }

                print(f"[TELEPHONE_RECEIVER] å‘é€å·¥å…·è°ƒç”¨è¯·æ±‚: {tool_name_to_test}")
                process.stdin.write((json.dumps(call_request) + "\n").encode())
                await process.stdin.drain()

                # è¯»å–è°ƒç”¨å“åº”
                call_response_line = await asyncio.wait_for(process.stdout.readline(), timeout=15)
                call_response = json.loads(call_response_line.decode().strip())

                print(f"[TELEPHONE_RECEIVER] è°ƒç”¨å“åº”: {str(call_response)[:200]}...")

                if "result" in call_response:
                    print(f"[OK] å·¥å…·è°ƒç”¨æˆåŠŸ: {tool_name_to_test}")
                    steps[2]["status"] = "success"
                    steps[2]["message"] = f"æˆåŠŸè°ƒç”¨å·¥å…· {tool_name_to_test}"
                    return True
                elif "error" in call_response:
                    error_msg = call_response.get("error", {}).get("message", "æœªçŸ¥é”™è¯¯")
                    # æŸäº›é”™è¯¯æ˜¯é¢„æœŸçš„ï¼ˆå¦‚å‚æ•°ä¸è¶³ï¼‰ï¼Œè¿™ä»ç„¶è¯æ˜å·¥å…·å¯ä»¥å“åº”
                    if "required" in error_msg.lower() or "missing" in error_msg.lower() or "argument" in error_msg.lower():
                        print(f"[OK] å·¥å…·å“åº”æ­£å¸¸ï¼ˆå‚æ•°é”™è¯¯æ˜¯é¢„æœŸçš„ï¼‰: {error_msg[:50]}")
                        steps[2]["status"] = "success"
                        steps[2]["message"] = f"å·¥å…· {tool_name_to_test} å“åº”æ­£å¸¸"
                        return True
                    else:
                        print(f"[ERROR] å·¥å…·è°ƒç”¨å¤±è´¥: {error_msg}")
                        steps[2]["status"] = "failed"
                        steps[2]["message"] = f"å·¥å…·è°ƒç”¨å¤±è´¥: {error_msg[:50]}"
                        return False
                else:
                    print("[ERROR] å·¥å…·è°ƒç”¨å“åº”æ ¼å¼å¼‚å¸¸")
                    steps[2]["status"] = "failed"
                    steps[2]["message"] = "å·¥å…·è°ƒç”¨å“åº”æ ¼å¼å¼‚å¸¸"
                    return False
            else:
                print("[ERROR] å·¥å…·åˆ—è¡¨ä¸ºç©º")
                steps[2]["status"] = "failed"
                steps[2]["message"] = "å·¥å…·åˆ—è¡¨ä¸ºç©º"
                return False
        else:
            print("[ERROR] å·¥å…·åˆ—è¡¨å“åº”æ ¼å¼é”™è¯¯")
            steps[2]["status"] = "failed"
            steps[2]["message"] = "å·¥å…·åˆ—è¡¨å“åº”æ ¼å¼é”™è¯¯"
            return False

    except Exception as e:
        print(f"[ERROR] é€šç”¨åŠŸèƒ½æµ‹è¯•å¼‚å¸¸: {str(e)}")
        steps[2]["status"] = "failed"
        steps[2]["message"] = f"æµ‹è¯•å¼‚å¸¸: {str(e)[:50]}"
        return False

# æŒ‚è½½æ€ç»´å¯¼å›¾ç®¡ç†è·¯ç”±
from mindmap_api import router as mindmap_router
app.include_router(mindmap_router)

# æŒ‚è½½å®‰å…¨æ¨¡å—è·¯ç”±
try:
    from security.api_routes import router as security_router
    app.include_router(security_router, prefix="/api/security")
    print("[OK] å®‰å…¨æ¨¡å—è·¯ç”±å·²æŒ‚è½½åˆ° /api/security")
except ImportError as e:
    print(f"[WARN] å®‰å…¨æ¨¡å—ä¸å¯ç”¨: {e}")

# æŒ‚è½½ç”¨æˆ·è®¾ç½®APIè·¯ç”±
try:
    from user_settings_api import router as user_settings_router
    app.include_router(user_settings_router, prefix="/api")
    print("[OK] ç”¨æˆ·è®¾ç½®APIè·¯ç”±å·²æŒ‚è½½åˆ° /api")
except ImportError as e:
    print(f"[WARN] ç”¨æˆ·è®¾ç½®APIä¸å¯ç”¨: {e}")

# æŒ‚è½½é€»è¾‘æ™ºæ…§ç³»ç»Ÿè·¯ç”±
if LOGIC_WISDOM_SYSTEM_AVAILABLE and brain_router:
    app.include_router(brain_router)
    print("[OK] é€»è¾‘æ™ºæ…§ç³»ç»Ÿè·¯ç”±å·²æŒ‚è½½åˆ° /brain")
else:
    print("[WARN] é€»è¾‘æ™ºæ…§ç³»ç»Ÿä¸å¯ç”¨ï¼Œè·³è¿‡è·¯ç”±æŒ‚è½½")

# æŒ‚è½½ LogicWorld Protocol é›†æˆè·¯ç”±
try:
    from logic.logicworld_integration.routes import router as logicworld_router
    app.include_router(logicworld_router)
    print("[OK] LogicWorld Protocol é›†æˆè·¯ç”±å·²æŒ‚è½½åˆ° /api/logicworld")
except ImportError as e:
    print(f"[WARN] LogicWorld Protocol é›†æˆå¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"[WARN] LogicWorld Protocol é›†æˆè·¯ç”±æŒ‚è½½å¤±è´¥: {e}")

# æŒ‚è½½AIæ™ºèƒ½å·¥ä½œæµè·¯ç”±
try:
    from ai.intelligent_workflow_fastapi import router as intelligent_workflow_router
    app.include_router(intelligent_workflow_router)
    print("[OK] AIæ™ºèƒ½å·¥ä½œæµAPIè·¯ç”±å·²æŒ‚è½½åˆ° /api/intelligent-workflow")
except ImportError as e:
    print(f"[WARN] AIæ™ºèƒ½å·¥ä½œæµAPIå¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"[WARN] AIæ™ºèƒ½å·¥ä½œæµAPIæŒ‚è½½å¤±è´¥: {e}")

# æŒ‚è½½æ™ºèƒ½è§„åˆ’APIè·¯ç”±
try:
    from api.intelligent_planning_api import router as intelligent_planning_router
    app.include_router(intelligent_planning_router)
    print("[OK] æ™ºèƒ½è§„åˆ’APIè·¯ç”±å·²æŒ‚è½½åˆ° /api/intelligent-planning")
except ImportError as e:
    print(f"[WARN] æ™ºèƒ½è§„åˆ’APIå¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"[WARN] æ™ºèƒ½è§„åˆ’APIæŒ‚è½½å¤±è´¥: {e}")

# æŒ‚è½½æœ¬åœ°å·¥å…·è·¯ç”±
try:
    import sys
    import os
    # æ·»åŠ backendç›®å½•åˆ°Pythonè·¯å¾„
    backend_path = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    if backend_path not in sys.path:
        sys.path.insert(0, backend_path)

    from utils.local_tools import router as local_tools_router
    app.include_router(local_tools_router, prefix="/api")
    print("[OK] æœ¬åœ°å·¥å…·è·¯ç”±å·²æŒ‚è½½åˆ° /api/local-tools")
except ImportError as e:
    print(f"[WARN] æœ¬åœ°å·¥å…·å¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"[WARN] æœ¬åœ°å·¥å…·è·¯ç”±æŒ‚è½½å¤±è´¥: {e}")

# æ–‡æ¡£é¢„è§ˆAPIè·¯ç”±
try:
    # æš‚æ—¶ç¦ç”¨æ–‡æ¡£é¢„è§ˆAPIé¿å…è¯­æ³•é”™è¯¯
    # from api.document_preview_api import router as document_preview_router
    # app.include_router(document_preview_router)
    print("[SKIP] æ–‡æ¡£é¢„è§ˆAPIè·¯ç”±å·²ç¦ç”¨")
except ImportError as e:
    print(f"[WARN] æ–‡æ¡£é¢„è§ˆAPIå¯¼å…¥å¤±è´¥: {e}")
except Exception as e:
    print(f"[WARN] æ–‡æ¡£é¢„è§ˆAPIè·¯ç”±æŒ‚è½½å¤±è´¥: {e}")

# åˆ«åç«¯ç‚¹ï¼šç¡®ä¿ /api/local-tools/file-manager/open å¯ç”¨ï¼ˆè½¬è°ƒ utils.local_tools.open_fileï¼‰
from fastapi import Body

@app.post("/api/local-tools/file-manager/open")
async def _alias_open_file(payload: dict = Body(...)):
    try:
        # å»¶è¿Ÿå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
        from utils.local_tools import open_file as _open_file
        return await _open_file(payload)
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

# å°†é™æ€ç›®å½•æŒ‚è½½åˆ° /swagger
app.mount("/swagger", StaticFiles(directory=str(swagger_path)), name="swagger")

# ä½¿ç”¨æœ¬åœ°èµ„æºæ¸²æŸ“ Swagger-UIï¼Œå¹¶è®¾ç½®ä¸­æ–‡ç•Œé¢
from fastapi.responses import HTMLResponse

# æ³¨å…¥å‰ç«¯è„šæœ¬ï¼Œå°† Swagger-UI æ®‹ä½™è‹±æ–‡æ ‡ç­¾åŠ¨æ€æ›¿æ¢ä¸ºä¸­æ–‡
_ZH_TRANSLATIONS = {
    "Try it out": "å°è¯•",
    " Try it out": "å°è¯•",
    "Try it out ": "å°è¯•",
    " Try it out ": "å°è¯•",
    "Execute": "æ‰§è¡Œ",
    "Cancel": "å–æ¶ˆ",
    "Schemas": "æ•°æ®ç»“æ„",
    "Schema": "æ•°æ®ç»“æ„",
    "Example Value": "ç¤ºä¾‹å€¼",
    "Responses": "å“åº”",
    "Response": "å“åº”",
    "Request body": "è¯·æ±‚ä½“",
    "Parameters": "å‚æ•°",
    "Parameter": "å‚æ•°",
    "Description": "æè¿°",
    "No parameters": "æ— å‚æ•°",
    "default": "é»˜è®¤",
    "Media type": "åª’ä½“ç±»å‹",
    "Code": "çŠ¶æ€ç ",
    "Status": "çŠ¶æ€",
    "Value": "å€¼",
    "Model": "æ¨¡å‹",
    "Download": "ä¸‹è½½",
    "Clear": "æ¸…é™¤",
    "Close": "å…³é—­",
    "Links": "é“¾æ¥",
    "No links": "æ— é“¾æ¥",
    "Available authorizations": "æˆæƒ",
    "Authorize": "æˆæƒ",
    "Headers": "è¯·æ±‚å¤´",
}


@app.get("/docs", include_in_schema=False)
async def custom_swagger_ui():
    base_html = get_swagger_ui_html(
        openapi_url=app.openapi_url,
        title=app.title + " - æ¥å£æ–‡æ¡£",
        swagger_js_url="/swagger/swagger-ui-bundle.js",
        swagger_css_url="/swagger/swagger-ui.css",
        swagger_ui_parameters={"lang": "zh-CN"},
    )

    # åŠ¨æ€æ³¨å…¥è„šæœ¬ï¼Œè¿è¡Œæ—¶æ›¿æ¢é¡µé¢æ–‡å­—
    import json as _json
    translate_js = """
    <script>
    (function() {
        const zhMap = %s;
        const lowerMap = {};
        Object.keys(zhMap).forEach(k => lowerMap[k.toLowerCase()] = zhMap[k]);

        function translate() {
            const elements = document.querySelectorAll('button, span, div, label, h4, td, th');
            elements.forEach(el => {
                const txt = (el.innerText || '').trim();
                const low = txt.toLowerCase();
                if (lowerMap[low]) {
                    el.innerText = lowerMap[low];
                }
            });
        }
        // åˆå§‹å°è¯•
        translate();
        // ç›‘å¬ DOM å˜åŒ–
        const observer = new MutationObserver(translate);
        observer.observe(document.body, { childList: true, subtree: true });
        // å®šæ—¶å…œåº•
        setInterval(translate, 1500);
    })();
    </script>
    """ % _json.dumps(_ZH_TRANSLATIONS, ensure_ascii=False)

    content = base_html.body.decode().replace("</body>", translate_js + "</body>")
    return HTMLResponse(content=content, status_code=base_html.status_code, headers=dict(base_html.headers))

DEEPSEEK_API_KEY = None
client: Optional[AsyncOpenAI] = None
WORKFLOW_RULES = []


@app.on_event("startup")
async def startup_event():
    """
    Initializes the application, loading necessary configurations and tools.
    """
    global DEEPSEEK_API_KEY, client
    # Load workflow rules on startup
    global WORKFLOW_RULES
    WORKFLOW_RULES = load_workflow_rules()
    
    DEEPSEEK_API_KEY = os.getenv("DEEPSEEK_API_KEY")
    if not DEEPSEEK_API_KEY:
        # Fallback to OpenAI key if DeepSeek key is not found
        DEEPSEEK_API_KEY = os.getenv("OPENAI_API_KEY")

    if not DEEPSEEK_API_KEY:
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
        print("!!! WARNING: DEEPSEEK_API_KEY or OPENAI_API_KEY not found.  !!!")
        print("!!! AI-based tools will fail. Please set it in your .env file.!!!")
        print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    
    # Initialize the client only if the key exists
    if DEEPSEEK_API_KEY:
        client = AsyncOpenAI(api_key=DEEPSEEK_API_KEY, base_url="https://api.deepseek.com/v1")

    # å·¥å…·æ³¨å†Œå·²ç¦ç”¨ - ç°åœ¨å®Œå…¨ä½¿ç”¨AIå·¥ä½œæµæ„å»ºå™¨
    try:
        logging.info("[OK] å·¥å…·æ³¨å†Œå·²è·³è¿‡ï¼Œä½¿ç”¨AIå·¥ä½œæµæ„å»ºå™¨")
        
        # åˆå§‹åŒ–å…¨å±€enhanced_tool_routerä»¥ç¡®ä¿MCPå·¥å…·æ­£å¸¸å·¥ä½œï¼ˆæ— å¤–éƒ¨MCPæœåŠ¡å™¨ï¼‰
        try:
            from tools.enhanced_tool_router import EnhancedToolRouter
            enhanced_tool_router = EnhancedToolRouter()
            await enhanced_tool_router.initialize()
            logging.info("âœ… å…¨å±€enhanced_tool_routeråˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logging.error(f"âŒ å…¨å±€enhanced_tool_routeråˆå§‹åŒ–å¤±è´¥: {e}")
            
    except Exception as e:
        logging.error(f"[ERROR] å·¥å…·æ³¨å†Œé…ç½®é”™è¯¯: {e}")
        # ç»§ç»­å¯åŠ¨ï¼Œä¸å› å·¥å…·æ³¨å†Œå¤±è´¥è€Œåœæ­¢

# CORS ç™½åå•ï¼ˆå‰ç«¯å¼€å‘ç«¯å£ & æœ¬åœ°æ–‡ä»¶ï¼‰
ALLOWED_ORIGINS = [
    "http://localhost:3000",
    "http://127.0.0.1:3000",
    "http://localhost:3001",
    "http://127.0.0.1:3001",
    "http://localhost:3002",
    "http://127.0.0.1:3002",
    "http://localhost:3003",
    "http://127.0.0.1:3003",
    "http://localhost",
    "http://127.0.0.1",
    "null",  # æ”¯æŒæœ¬åœ°æ–‡ä»¶è®¿é—®
    "*"      # ä¸´æ—¶å…è®¸æ‰€æœ‰æ¥æºï¼Œç”¨äºæµ‹è¯•
]

# ä»…æ·»åŠ ä¸€æ¬¡ CORS ä¸­é—´ä»¶ - ä½¿ç”¨æœ¬åœ°å®šä¹‰çš„ç™½åå•
app.add_middleware(
    CORSMiddleware,
    allow_origins=ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
)

# æ·»åŠ è¯·æ±‚éªŒè¯é”™è¯¯å¤„ç†å™¨
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    logging.error(f"[ERROR] è¯·æ±‚éªŒè¯é”™è¯¯: {exc}")
    print(f"[ERROR] è¯·æ±‚éªŒè¯é”™è¯¯: {exc}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": exc.body}
    )

# æµ‹è¯•ç«¯ç‚¹
@app.post("/test-chat")
async def test_chat(data: dict):
    """æµ‹è¯•èŠå¤©ç«¯ç‚¹"""
    logging.info(f"[WRENCH2] æµ‹è¯•ç«¯ç‚¹æ”¶åˆ°æ•°æ®: {data}")
    print(f"[WRENCH2] æµ‹è¯•ç«¯ç‚¹æ”¶åˆ°æ•°æ®: {data}")
    return {"status": "success", "received": data}

# --- Pro Orchestrator Request model must be defined before route decoration ---

from pydantic import BaseModel  # ensure BaseModel in scope for forward decl


class ValidationRequest(BaseModel):
    nodes: List[Dict[str, Any]]
import httpx
import re
from datetime import datetime

# Import tool registry AFTER environment variables are loaded so that tools can access API keys at import time.

# Import orchestrator AFTER dotenv as it requires API keys indirectly via AgentFactory

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- Environment & API Key ---
# ç§»é™¤è¿™é‡Œçš„APIå¯†é’¥æ£€æŸ¥ï¼Œå› ä¸ºåœ¨startupäº‹ä»¶ä¸­å·²ç»å¤„ç†äº†
# api_key = os.getenv("DEEPSEEK_API_KEY")
# if not api_key:
#     raise ValueError("DEEPSEEK_API_KEY environment variable not set.")
# client = AsyncOpenAI(api_key=api_key, base_url="https://api.deepseek.com")

# --- SpaCy Model for Keyword Extraction ---
# try:
#     nlp = spacy.load("en_core_web_sm")
# except OSError:
#     print("Downloading 'en_core_web_sm' model...")
#     from spacy.cli import download
#     download("en_core_web_sm")
#     nlp = spacy.load("en_core_web_sm")
nlp = None  # Temporarily disable spacy

# --- Rule Engine ---
WORKFLOW_RULES = []

def load_workflow_rules(rules_dir: str = "rules"):
    """Loads workflow validation rules from the specified directory.
    - rules_dir æ”¯æŒç›¸å¯¹è·¯å¾„ï¼›å°†è½¬æ¢ä¸ºç»å¯¹è·¯å¾„
    - ç›®å½•ä¸å­˜åœ¨æ—¶ä»…è®°å½•è­¦å‘Šï¼Œé¿å…å¯åŠ¨å¤±è´¥
    """
    import os as _os
    global WORKFLOW_RULES
    WORKFLOW_RULES = []
    # è§„èŒƒåŒ–ç›®å½•ä¸ºç»å¯¹è·¯å¾„
    abs_dir = _os.path.abspath(rules_dir)
    logging.info(f"Loading workflow rules from: {abs_dir}")
    if not _os.path.isdir(abs_dir):
        logging.warning(f"Rules directory not found: {abs_dir}, skip loading rules")
        return
    for filename in _os.listdir(abs_dir):
        if filename.endswith(".json"):
            filepath = _os.path.join(abs_dir, filename)
            try:
                with open(filepath, 'r', encoding='utf-8') as f:
                    rule = json.load(f)
                    WORKFLOW_RULES.append(rule)
                    logging.info(f"Successfully loaded rule: {rule.get('workflow_name')}")
            except Exception as e:
                logging.error(f"Failed to load rule from {filepath}: {e}")

# --- Memory/History Functions ---
def save_execution_to_history(payload: 'WorkflowPayload', final_result: Any):
    """Saves a successful workflow execution to the history log."""
    history_file = "backend/memory/execution_history.jsonl"
    log_entry = {
        "timestamp": datetime.utcnow().isoformat(),
        "workflow": payload.model_dump(), # Use model_dump for Pydantic v2
        "final_result": final_result
    }
    try:
        with open(history_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(log_entry) + '\n')
        logging.info("Successfully saved execution to history.")
    except Exception as e:
        logging.error(f"Failed to save execution to history: {e}")


# --- FastAPI App ---
# app å·²åœ¨æ–‡ä»¶é¡¶éƒ¨åˆå§‹åŒ–è¿‡ï¼Œæ­¤å¤„æ— éœ€é‡æ–°åˆ›å»ºï¼Œé¿å…è¦†ç›– swagger_ui_parameters è®¾ç½®
# app = FastAPI()

# ç§»é™¤é‡å¤çš„startupäº‹ä»¶å¤„ç†å™¨ï¼Œå·²åœ¨ä¸Šé¢å®šä¹‰è¿‡äº†
# @app.on_event("startup")
# async def startup_event():
#     """Load rules on application startup."""
#     load_workflow_rules()

# ç¬¬äºŒå¤„é‡å¤çš„ add_middleware å·²ç§»é™¤ï¼Œé¿å…å¤šæ¬¡è£…è½½

# ================================
#  Pro Orchestrator API
# ================================


class OrchestratorRequest(BaseModel):
    """è¯·æ±‚ä½“: æä¾›ä¸€ç³»åˆ—é«˜é˜¶ä»»åŠ¡ç»™ Pro Orchestrator"""
    tasks: List[str]
    # modeå­—æ®µå·²åˆ é™¤ï¼Œç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„æ‰§è¡Œé€»è¾‘


@app.post("/orchestrate")
async def orchestrate(request: OrchestratorRequest):
    """
    è¿è¡Œæ–°çš„ Pro ç¼–æ’å™¨ã€‚
    - 'normal' æ¨¡å¼: äº§ç”Ÿä¸€ä¸ªè½»é‡çº§çš„ã€çº¯æ–‡æœ¬çš„æ€è€ƒé“¾ã€‚
    - 'super' æ¨¡å¼: (æœªæ¥) å¯ç”¨å¤šä¸“å®¶ã€å·¥å…·æ‰§è¡Œçš„ä¸“ä¸šå·¥ä½œæµã€‚
    
    è¯·æ±‚ç¤ºä¾‹:
    {
        "tasks": [
            "æ˜ç¡®é¡¹ç›®éœ€æ±‚å¹¶æ’°å†™PRD",
            "è®¾è®¡ç³»ç»Ÿæ¶æ„",
            "å®ç°æ ¸å¿ƒåŠŸèƒ½ä»£ç ",
            "ç¼–å†™é›†æˆæµ‹è¯•"
        ],
        "mode": "normal"
    }
    """
    orchestrator = Orchestrator()
    result = await orchestrator.run(request.tasks)
    return result

def find_reference(args: Dict[str, Any]) -> Optional[Tuple[str, str]]:
    """
    Finds the first reference (a value starting with '@node-') in a dictionary.
    This was the missing function causing the NameError.
    """
    for key, value in args.items():
        if isinstance(value, str) and value.startswith('@node-'):
            return key, value
    return None, None

class ChatRequest(BaseModel):
    prompt: str
    model: str = 'deepseek-chat'
    history: list = []
    intent: str = 'chat'  # 'chat' or 'mindmap'
    multi_scene: bool = False  # æ˜¯å¦å¯ç”¨å¤šåœºæ™¯æ£€ç´¢

class ValidationRequest(BaseModel):
    nodes: List[Dict[str, Any]]


# --- Helper Functions ---
def generate_id():
    """Generates a unique ID for messages."""
    return f"{os.urandom(6).hex()}"

MINDMAP_KEYWORDS = {'generate', 'create', 'make', 'draw', 'mind map', 'æ€ç»´å¯¼å›¾', 'ç”Ÿæˆ', 'åˆ›å»º', 'ç”»'}

def detect_intent(prompt: str):
    """Detects if the user wants to generate a mind map."""
    return any(keyword in prompt.lower() for keyword in MINDMAP_KEYWORDS)

async def handle_step_generation_json(request: ChatRequest):
    """å¤„ç†æ­¥éª¤ç”Ÿæˆè¯·æ±‚ï¼Œè¿”å›ç»“æ„åŒ–JSONå“åº”"""
    try:
        logging.info("[WRENCH2] å¼€å§‹å¤„ç†æ­¥éª¤ç”Ÿæˆè¯·æ±‚")

        # æ£€æŸ¥APIå¯†é’¥
        api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY")
        if not api_key:
            logging.error("[ERROR] æœªé…ç½®APIå¯†é’¥")
            return StreamingResponse(
                iter([f"data: {json.dumps({'type': 'error', 'error': 'AIæœåŠ¡æœªé…ç½®'})}\n\n"]),
                media_type="text/event-stream"
            )

        # æ„å»ºAIæç¤ºè¯
        step_prompt = f"""è¯·ä¸ºä»¥ä¸‹éœ€æ±‚ç”Ÿæˆå¯æ‰§è¡Œçš„å·¥ä½œæµæ­¥éª¤ï¼š{request.prompt}

è¦æ±‚ï¼š
1. ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–æ–‡æœ¬æˆ–markdownæ ¼å¼ï¼š

{{
  "title": "[CLIPBOARD2] **æ‰§è¡Œæ­¥éª¤**",
  "stepCount": æ­¥éª¤æ•°é‡,
  "steps": ["æ­¥éª¤1æ ‡é¢˜", "æ­¥éª¤2æ ‡é¢˜", "æ­¥éª¤3æ ‡é¢˜"],
  "detailedSteps": [
    {{
      "title": "æ­¥éª¤1æ ‡é¢˜",
      "content": "æ­¥éª¤æè¿°",
      "tool": "å…·ä½“å·¥å…·åç§°",
      "toolName": "å…·ä½“å·¥å…·åç§°",
      "aiDescription": "è¯¦ç»†æ‰§è¡Œè¯´æ˜",
      "executionDescription": "è¯¦ç»†æ‰§è¡Œè¯´æ˜",
      "parameters": ""
    }}
  ]
}}

2. ç”Ÿæˆ5-10ä¸ªå…·ä½“å¯æ‰§è¡Œçš„æ­¥éª¤
3. å·¥å…·åç§°ä½¿ç”¨ï¼šDeepSeekèŠå¤©æ¨¡å‹ã€æ–‡ä»¶æ“ä½œå·¥å…·ã€ç½‘é¡µæµè§ˆå™¨ã€ä»£ç ç¼–è¾‘å™¨ã€ç»ˆç«¯å·¥å…·ã€APIè°ƒç”¨å™¨ã€æ•°æ®åº“å·¥å…·ã€æ–‡æœ¬å¤„ç†å™¨ã€å›¾åƒå¤„ç†å™¨ã€æ•°æ®åˆ†æå™¨ç­‰
4. æ‰§è¡Œè¯´æ˜è¦è¯¦ç»†å…·ä½“ï¼ŒåŒ…å«æ“ä½œæ–¹æ³•å’Œé¢„æœŸç»“æœ
5. åªè¿”å›JSONï¼Œä¸è¦ä»»ä½•å…¶ä»–æ ¼å¼æˆ–æ–‡æœ¬

è¯·ç›´æ¥è¿”å›JSONï¼š"""

        # åˆ›å»ºAIå®¢æˆ·ç«¯
        from openai import AsyncOpenAI
        base_url = "https://api.deepseek.com" if os.getenv("DEEPSEEK_API_KEY") else None
        client = AsyncOpenAI(api_key=api_key, base_url=base_url)

        # æµå¼å“åº”ç”Ÿæˆå™¨
        async def generate_stream():
            try:
                # è°ƒç”¨AI API
                response = await client.chat.completions.create(
                    model="deepseek-chat" if os.getenv("DEEPSEEK_API_KEY") else "gpt-3.5-turbo",
                    messages=[
                        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥ä½œæµè®¾è®¡å¸ˆï¼Œæ“…é•¿å°†å¤æ‚ä»»åŠ¡åˆ†è§£ä¸ºå¯æ‰§è¡Œçš„æ­¥éª¤ã€‚"},
                        {"role": "user", "content": step_prompt}
                    ],
                    stream=True,
                    max_tokens=2000,
                    temperature=0.7
                )

                accumulated_content = ""

                async for chunk in response:
                    if chunk.choices[0].delta.content:
                        content = chunk.choices[0].delta.content
                        accumulated_content += content

                        # å‘é€æµå¼æ•°æ®
                        yield f"data: {json.dumps({'type': 'content', 'content': content})}\n\n"

                # å°è¯•è§£ææœ€ç»ˆçš„JSON
                try:
                    # æ¸…ç†å†…å®¹ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ‡è®°
                    clean_content = accumulated_content.strip()
                    if clean_content.startswith("```json"):
                        clean_content = clean_content[7:]
                    if clean_content.endswith("```"):
                        clean_content = clean_content[:-3]
                    clean_content = clean_content.strip()

                    step_data = json.loads(clean_content)
                    yield f"data: {json.dumps({'type': 'complete', 'stepData': step_data})}\n\n"

                except json.JSONDecodeError:
                    logging.warning("JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨é™çº§å“åº”")
                    # é™çº§å“åº”
                    fallback_data = {
                        "title": "[CLIPBOARD2] **æ‰§è¡Œæ­¥éª¤**",
                        "stepCount": 3,
                        "steps": ["åˆ†æéœ€æ±‚", "å‡†å¤‡å·¥å…·", "æ‰§è¡Œä»»åŠ¡"],
                        "detailedSteps": [
                            {
                                "title": "åˆ†æéœ€æ±‚",
                                "content": "è¯¦ç»†åˆ†æç”¨æˆ·éœ€æ±‚å’Œç›®æ ‡",
                                "tool": "DeepSeekèŠå¤©æ¨¡å‹",
                                "toolName": "DeepSeekèŠå¤©æ¨¡å‹",
                                "aiDescription": "ä½¿ç”¨AIåˆ†æç”¨æˆ·è¾“å…¥çš„éœ€æ±‚",
                                "executionDescription": "ä½¿ç”¨AIåˆ†æç”¨æˆ·è¾“å…¥çš„éœ€æ±‚",
                                "parameters": ""
                            },
                            {
                                "title": "å‡†å¤‡å·¥å…·",
                                "content": "å‡†å¤‡æ‰§è¡Œä»»åŠ¡æ‰€éœ€çš„å·¥å…·å’Œèµ„æº",
                                "tool": "æ–‡ä»¶æ“ä½œå·¥å…·",
                                "toolName": "æ–‡ä»¶æ“ä½œå·¥å…·",
                                "aiDescription": "å‡†å¤‡å¿…è¦çš„å·¥å…·å’Œæ–‡ä»¶",
                                "executionDescription": "å‡†å¤‡å¿…è¦çš„å·¥å…·å’Œæ–‡ä»¶",
                                "parameters": ""
                            },
                            {
                                "title": "æ‰§è¡Œä»»åŠ¡",
                                "content": "æŒ‰ç…§è®¡åˆ’æ‰§è¡Œå…·ä½“ä»»åŠ¡",
                                "tool": "ä»£ç ç¼–è¾‘å™¨",
                                "toolName": "ä»£ç ç¼–è¾‘å™¨",
                                "aiDescription": "æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡æ“ä½œ",
                                "executionDescription": "æ‰§è¡Œå…·ä½“çš„ä»»åŠ¡æ“ä½œ",
                                "parameters": ""
                            }
                        ]
                    }
                    yield f"data: {json.dumps({'type': 'complete', 'stepData': fallback_data})}\n\n"

                # å‘é€ç»“æŸæ ‡è®°
                yield f"data: {json.dumps({'type': 'end'})}\n\n"

            except Exception as e:
                logging.error(f"æ­¥éª¤ç”Ÿæˆå¤±è´¥: {e}")
                yield f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"
                # å³ä½¿å‡ºé”™ä¹Ÿè¦å‘é€ç»“æŸæ ‡è®°
                yield f"data: {json.dumps({'type': 'end'})}\n\n"

        return StreamingResponse(generate_stream(), media_type="text/event-stream")

    except Exception as e:
        logging.error(f"æ­¥éª¤ç”Ÿæˆå¤„ç†å¤±è´¥: {e}")
        return StreamingResponse(
            iter([f"data: {json.dumps({'type': 'error', 'error': str(e)})}\n\n"]),
            media_type="text/event-stream"
        )


async def handle_simple_chat(request: ChatRequest):
    """Handle simple chat requests with intelligent role selection."""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯æ­¥éª¤ç”Ÿæˆè¯·æ±‚
        if "è¯·ä¸ºä»¥ä¸‹éœ€æ±‚ç”Ÿæˆå¯æ‰§è¡Œçš„å·¥ä½œæµæ­¥éª¤" in request.prompt or "ç”Ÿæˆæ­¥éª¤" in request.prompt:
            logging.info("[WRENCH2] æ£€æµ‹åˆ°æ­¥éª¤ç”Ÿæˆè¯·æ±‚ï¼Œä½¿ç”¨ç»“æ„åŒ–JSONå“åº”")
            return await handle_step_generation_json(request)

        # å¯¼å…¥æ™ºèƒ½è§’è‰²é€‰æ‹©å™¨
        from ai.intelligent_role_selector import role_selector

        # æ™ºèƒ½è§’è‰²é€‰æ‹©
        logging.info(f"[MASK] å¼€å§‹æ™ºèƒ½è§’è‰²é€‰æ‹©: {request.prompt}")
        role_match = role_selector.analyze_user_input(
            user_input=request.prompt,
            context=getattr(request, 'context', {})
        )

        logging.info(f"[DART] é€‰æ‹©è§’è‰²: {role_match.role_name} (ç½®ä¿¡åº¦: {role_match.confidence:.2f})")
        logging.info(f"[MEMO2] é€‰æ‹©ç†ç”±: {role_match.reasoning}")

        # æ„å»ºå¯¹è¯å†å²
        messages = []

        # ä½¿ç”¨æ™ºèƒ½é€‰æ‹©çš„è§’è‰²æ¨¡æ¿ä½œä¸ºç³»ç»Ÿæç¤º
        if role_match.template:
            try:
                # æ¸²æŸ“è§’è‰²æ¨¡æ¿
                system_prompt = role_match.template.format(**role_match.variables)
                logging.info(f"[MASK] ä½¿ç”¨è§’è‰²æ¨¡æ¿: {role_match.role_name}")
            except Exception as template_error:
                logging.warning(f"[WARN] æ¨¡æ¿æ¸²æŸ“å¤±è´¥: {template_error}")
                # é™çº§åˆ°é»˜è®¤æç¤º
                system_prompt = f"ä½ æ˜¯ä¸€ä¸ª{role_match.role_name}ï¼Œè¯·æ ¹æ®ä½ çš„ä¸“ä¸šè§’è‰²å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚"
        else:
            # é™çº§åˆ°é»˜è®¤æç¤º
            system_prompt = "ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„AIåŠ©æ‰‹ã€‚è¯·ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ï¼Œä¸éœ€è¦åˆ›å»ºå·¥ä½œæµæˆ–æ€ç»´å¯¼å›¾ã€‚"

        messages.append({
            "role": "system",
            "content": system_prompt
        })

        # æ·»åŠ å†å²å¯¹è¯
        for msg in request.history[-10:]:  # åªä¿ç•™æœ€è¿‘10æ¡æ¶ˆæ¯
            if msg.get('sender') == 'user':
                messages.append({"role": "user", "content": msg.get('text', '')})
            elif msg.get('sender') == 'ai':
                messages.append({"role": "assistant", "content": msg.get('text', '')})

        # æ·»åŠ å½“å‰ç”¨æˆ·æ¶ˆæ¯
        messages.append({"role": "user", "content": request.prompt})

        # ä½¿ç”¨æµå¼è¾“å‡ºè°ƒç”¨AIè·å–å›å¤
        from fastapi.responses import StreamingResponse
        import json

        async def generate_stream():
            try:
                logger.info("[LAUNCH] å¼€å§‹ç”Ÿæˆæµå¼å“åº”")

                # é¦–å…ˆå‘é€è§’è‰²ä¿¡æ¯
                role_info = {
                    "type": "role_info",
                    "role_info": {
                        "role_id": role_match.role_id,
                        "role_name": role_match.role_name,
                        "confidence": role_match.confidence,
                        "reasoning": role_match.reasoning
                    },
                    "timestamp": int(time.time() * 1000)
                }
                logger.info("[OUTBOX_TRAY] å‘é€è§’è‰²ä¿¡æ¯")
                yield f"data: {json.dumps(role_info, ensure_ascii=False)}\n\n"

                # åˆ›å»ºæµå¼è¯·æ±‚
                logger.info("[RELOAD] åˆ›å»ºæµå¼APIè¯·æ±‚")
                stream = await client.chat.completions.create(
                    model=request.model,
                    messages=messages,
                    temperature=0.7,
                    max_tokens=1000,
                    stream=True
                )
                logger.info("[OK] æµå¼APIè¯·æ±‚å·²åˆ›å»º")

                # å‘é€å¼€å§‹æ ‡è®°
                start_data = {
                    "type": "chat_start",
                    "timestamp": int(time.time() * 1000)
                }
                logger.info("[OUTBOX_TRAY] å‘é€å¼€å§‹æ ‡è®°")
                yield f"data: {json.dumps(start_data, ensure_ascii=False)}\n\n"

                # æµå¼è¾“å‡ºAIå›å¤
                logger.info("[RELOAD] å¼€å§‹å¤„ç†æµå¼å“åº”")
                chunk_count = 0
                async for chunk in stream:
                    chunk_count += 1
                    logger.info(f"[PACKAGE] æ”¶åˆ°ç¬¬{chunk_count}ä¸ªchunk: {chunk}")
                    if chunk.choices[0].delta.content is not None:
                        content = chunk.choices[0].delta.content
                        logger.info(f"[MEMO2] chunkå†…å®¹: '{content}'")
                        chunk_data = {
                            "type": "chat_chunk",
                            "content": content,
                            "timestamp": int(time.time() * 1000)
                        }
                        yield f"data: {json.dumps(chunk_data, ensure_ascii=False)}\n\n"

                        # æ·»åŠ å°å»¶è¿Ÿï¼Œç¡®ä¿å‰ç«¯èƒ½çœ‹åˆ°é€å­—æ•ˆæœ
                        import asyncio
                        await asyncio.sleep(0.05)  # 50æ¯«ç§’å»¶è¿Ÿï¼Œè®©ç”¨æˆ·èƒ½çœ‹åˆ°é€å­—æ•ˆæœ

                logger.info(f"[OK] æµå¼å¤„ç†å®Œæˆï¼Œå…±å¤„ç†{chunk_count}ä¸ªchunk")

                # å‘é€ç»“æŸæ ‡è®°
                end_data = {
                    "type": "chat_end",
                    "timestamp": int(time.time() * 1000)
                }
                yield f"data: {json.dumps(end_data, ensure_ascii=False)}\n\n"

            except Exception as e:
                error_data = {
                    "type": "error",
                    "message": f"æµå¼è¾“å‡ºé”™è¯¯: {str(e)}",
                    "timestamp": int(time.time() * 1000)
                }
                yield f"data: {json.dumps(error_data, ensure_ascii=False)}\n\n"

        # è¿”å›æµå¼å“åº”
        return StreamingResponse(
            generate_stream(),
            media_type="text/event-stream",
            headers={
                "Cache-Control": "no-cache",
                "Connection": "keep-alive",
                "Access-Control-Allow-Origin": "*",
                "Access-Control-Allow-Headers": "Content-Type"
            }
        )

    except Exception as e:
        logging.error(f"Error in simple chat: {e}")
        return JSONResponse(
            status_code=500,
            content={
                "type": "error",
                "message": f"èŠå¤©æœåŠ¡å‡ºç°é”™è¯¯: {str(e)}"
            }
        )

class IntentDetectionRequest(BaseModel):
    prompt: str

# --- System Tools Library Models ---
class SystemToolConfig(BaseModel):
    """ç³»ç»Ÿå·¥å…·åº“é…ç½®æ¨¡å‹"""
    name: str
    description: str
    transport: str = "stdio"
    command: str
    args: List[str] = []
    timeout: int = 60000
    env: Dict[str, str] = {}
    tested: bool = False
    approved: bool = False
    created_at: Optional[str] = None
    updated_at: Optional[str] = None
    # ç³»ç»Ÿå·¥å…·æ˜¾ç¤ºæ‰€éœ€çš„é¢å¤–å­—æ®µ
    id: Optional[str] = None
    category: str = "ç³»ç»Ÿå·¥å…·"
    functionalCategory: Union[str, List[str]] = "system_tools"
    icon: str = "[GEAR2]"
    source: str = "user_shared"
    tool_type: str = "mcp_tool"
    enabled: bool = True
    tags: List[str] = []
    url: Optional[str] = None

@app.post("/detect_intent")
async def detect_intent_endpoint(request: IntentDetectionRequest):
    """
    ä½¿ç”¨AIæ™ºèƒ½æ£€æµ‹ç”¨æˆ·æ„å›¾
    """
    try:
        prompt = request.prompt

        # ç®€åŒ–çš„å…³é”®è¯æ£€æµ‹
        mindmap_keywords = {
            'generate', 'create', 'make', 'draw', 'build', 'design', 'plan',
            'mind map', 'mindmap', 'workflow', 'process', 'task', 'project',
            'æ€ç»´å¯¼å›¾', 'ç”Ÿæˆ', 'åˆ›å»º', 'ç”»', 'åˆ¶ä½œ', 'è®¾è®¡', 'è§„åˆ’', 'å·¥ä½œæµ', 'æµç¨‹', 'ä»»åŠ¡', 'é¡¹ç›®'
        }

        prompt_lower = prompt.lower()
        has_mindmap_keyword = any(keyword in prompt_lower for keyword in mindmap_keywords)

        if has_mindmap_keyword:
            intent = 'mindmap'
            confidence = 'keyword_detected'
        else:
            intent = 'chat'
            confidence = 'keyword_detected'

        return {
            "intent": intent,
            "confidence": confidence
        }

    except Exception as e:
        logging.error(f"Error in intent detection: {e}")
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤æ„å›¾
        return {
            "intent": "chat",
            "confidence": "fallback"
        }

# ==============================================================================
# System Tools Library Management
# ==============================================================================

# ç³»ç»Ÿå·¥å…·åº“å­˜å‚¨æ–‡ä»¶è·¯å¾„
SYSTEM_TOOLS_FILE = "../data/configs/backend-config/system_tools.json"
USER_TOOLS_FILE = "user_tools.json"

def _load_system_tools() -> List[Dict[str, Any]]:
    """åŠ è½½ç³»ç»Ÿå·¥å…·åº“é…ç½®"""
    try:
        if os.path.exists(SYSTEM_TOOLS_FILE):
            with open(SYSTEM_TOOLS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    except Exception as e:
        logging.error(f"åŠ è½½ç³»ç»Ÿå·¥å…·åº“å¤±è´¥: {e}")
        return []

def _save_system_tools(tools: List[Dict[str, Any]]) -> bool:
    """ä¿å­˜ç³»ç»Ÿå·¥å…·åº“é…ç½®"""
    try:
        with open(SYSTEM_TOOLS_FILE, 'w', encoding='utf-8') as f:
            json.dump(tools, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜ç³»ç»Ÿå·¥å…·åº“å¤±è´¥: {e}")
        return False

def _load_user_tools() -> List[Dict[str, Any]]:
    """åŠ è½½ç”¨æˆ·å·¥å…·é…ç½®"""
    try:
        if os.path.exists(USER_TOOLS_FILE):
            with open(USER_TOOLS_FILE, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    except Exception as e:
        logging.error(f"åŠ è½½ç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        return []

def _save_user_tools(tools: List[Dict[str, Any]]) -> bool:
    """ä¿å­˜ç”¨æˆ·å·¥å…·é…ç½®"""
    try:
        with open(USER_TOOLS_FILE, 'w', encoding='utf-8') as f:
            json.dump(tools, f, ensure_ascii=False, indent=2)
        return True
    except Exception as e:
        logging.error(f"ä¿å­˜ç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        return False

@app.get("/system-tools")
async def list_system_tools():
    """è·å–ç³»ç»Ÿå·¥å…·åº“åˆ—è¡¨"""
    try:
        tools = _load_system_tools()
        return tools
    except Exception as e:
        logging.error(f"è·å–ç³»ç»Ÿå·¥å…·åº“å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç³»ç»Ÿå·¥å…·åº“å¤±è´¥: {str(e)}")

@app.post("/system-tools")
async def add_or_update_system_tool(config: SystemToolConfig):
    """æ·»åŠ æˆ–æ›´æ–°ç³»ç»Ÿå·¥å…·"""
    try:
        tools = _load_system_tools()

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        tool_dict = config.model_dump()
        tool_dict["created_at"] = tool_dict.get("created_at") or datetime.now().isoformat()
        tool_dict["updated_at"] = datetime.now().isoformat()

        # ç¡®ä¿æœ‰IDå­—æ®µ
        if not tool_dict.get("id"):
            tool_dict["id"] = f"user_shared_{config.name.replace(' ', '_').replace('-', '_').lower()}"

        # ç¡®ä¿æœ‰ç³»ç»Ÿå·¥å…·æ˜¾ç¤ºæ‰€éœ€çš„æ‰€æœ‰å­—æ®µ
        if not tool_dict.get("category"):
            tool_dict["category"] = "ç³»ç»Ÿå·¥å…·"
        if not tool_dict.get("functionalCategory"):
            tool_dict["functionalCategory"] = tool_dict.get("functionalCategory", ["system_tools"])
        if not tool_dict.get("icon"):
            tool_dict["icon"] = "[GEAR2]"
        if not tool_dict.get("source"):
            tool_dict["source"] = "user_shared"
        if not tool_dict.get("tool_type"):
            tool_dict["tool_type"] = "mcp_tool"
        if not tool_dict.get("tags"):
            tool_dict["tags"] = ["ç”¨æˆ·åˆ†äº«", "MCPå·¥å…·"]

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå·¥å…·
        existing_index = None
        for i, tool in enumerate(tools):
            if tool.get("name") == config.name:
                existing_index = i
                break

        if existing_index is not None:
            # æ›´æ–°ç°æœ‰å·¥å…·
            tools[existing_index] = tool_dict
            logging.info(f"æ›´æ–°ç³»ç»Ÿå·¥å…·: {config.name}")
        else:
            # æ·»åŠ æ–°å·¥å…·
            tools.append(tool_dict)
            logging.info(f"æ·»åŠ ç³»ç»Ÿå·¥å…·: {config.name}")

        # ä¿å­˜åˆ°æ–‡ä»¶
        if _save_system_tools(tools):
            return {"success": True, "message": f"å·¥å…· {config.name} å·²ä¿å­˜åˆ°ç³»ç»Ÿå·¥å…·åº“"}
        else:
            raise HTTPException(status_code=500, detail="ä¿å­˜å·¥å…·å¤±è´¥")

    except Exception as e:
        logging.error(f"æ·»åŠ /æ›´æ–°ç³»ç»Ÿå·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ“ä½œå¤±è´¥: {str(e)}")

# ==============================================================================
# User Tools Management
# ==============================================================================

class UserToolConfig(BaseModel):
    """ç”¨æˆ·å·¥å…·é…ç½®æ¨¡å‹"""
    name: str
    description: str
    type: str = "api"  # api, file, data, ai, search, utility
    config: Dict[str, Any] = {}
    enabled: bool = True
    category: str = "æˆ‘çš„å·¥å…·"
    icon: str = "[HAMMER_WRENCH2]"

@app.get("/api/tools/user-tools")
async def list_user_tools():
    """è·å–ç”¨æˆ·å·¥å…·åˆ—è¡¨"""
    try:
        tools = _load_user_tools()
        return {"success": True, "tools": tools}
    except Exception as e:
        logging.error(f"è·å–ç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç”¨æˆ·å·¥å…·å¤±è´¥: {str(e)}")

@app.post("/api/tools/user-tools")
async def create_user_tool(config: UserToolConfig):
    """åˆ›å»ºç”¨æˆ·å·¥å…·"""
    try:
        tools = _load_user_tools()

        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        tool_dict = config.dict()
        tool_dict["id"] = f"user_tool_{int(time.time())}"
        tool_dict["created_at"] = datetime.now().isoformat()
        tool_dict["updated_at"] = datetime.now().isoformat()
        tool_dict["tested"] = True  # æ–°å·¥å…·é»˜è®¤å·²æµ‹è¯•ï¼Œç›´æ¥å¯ä»¥æ‰¹å‡†
        tool_dict["source"] = "user"

        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨åŒåå·¥å…·
        if any(tool.get("name") == config.name for tool in tools):
            raise HTTPException(status_code=400, detail="å·¥å…·åç§°å·²å­˜åœ¨")

        tools.append(tool_dict)

        if _save_user_tools(tools):
            return {"success": True, "message": f"å·¥å…· {config.name} åˆ›å»ºæˆåŠŸ", "tool": tool_dict}
        else:
            raise HTTPException(status_code=500, detail="ä¿å­˜å·¥å…·å¤±è´¥")

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"åˆ›å»ºç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºç”¨æˆ·å·¥å…·å¤±è´¥: {str(e)}")

@app.put("/api/tools/user-tools/{tool_id}")
async def update_user_tool(tool_id: str, config: UserToolConfig):
    """æ›´æ–°ç”¨æˆ·å·¥å…·"""
    try:
        tools = _load_user_tools()

        # æŸ¥æ‰¾è¦æ›´æ–°çš„å·¥å…·
        tool_index = None
        for i, tool in enumerate(tools):
            if tool.get("id") == tool_id:
                tool_index = i
                break

        if tool_index is None:
            raise HTTPException(status_code=404, detail="å·¥å…·ä¸å­˜åœ¨")

        # æ›´æ–°å·¥å…·é…ç½®
        tool_dict = config.dict()
        tool_dict["id"] = tool_id
        tool_dict["updated_at"] = datetime.now().isoformat()
        tool_dict["created_at"] = tools[tool_index].get("created_at", datetime.now().isoformat())
        tool_dict["tested"] = tools[tool_index].get("tested", False)
        tool_dict["source"] = "user"

        tools[tool_index] = tool_dict

        if _save_user_tools(tools):
            return {"success": True, "message": f"å·¥å…· {config.name} æ›´æ–°æˆåŠŸ", "tool": tool_dict}
        else:
            raise HTTPException(status_code=500, detail="ä¿å­˜å·¥å…·å¤±è´¥")

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"æ›´æ–°ç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ›´æ–°ç”¨æˆ·å·¥å…·å¤±è´¥: {str(e)}")

@app.delete("/api/tools/user-tools/{tool_id}")
async def delete_user_tool(tool_id: str):
    """åˆ é™¤ç”¨æˆ·å·¥å…·"""
    try:
        tools = _load_user_tools()

        # æŸ¥æ‰¾è¦åˆ é™¤çš„å·¥å…·
        tool_index = None
        for i, tool in enumerate(tools):
            if tool.get("id") == tool_id:
                tool_index = i
                break

        if tool_index is None:
            raise HTTPException(status_code=404, detail="å·¥å…·ä¸å­˜åœ¨")

        deleted_tool = tools.pop(tool_index)

        if _save_user_tools(tools):
            return {"success": True, "message": f"å·¥å…· {deleted_tool.get('name', 'æœªçŸ¥')} åˆ é™¤æˆåŠŸ"}
        else:
            raise HTTPException(status_code=500, detail="ä¿å­˜å·¥å…·å¤±è´¥")

    except HTTPException:
        raise
    except Exception as e:
        logging.error(f"åˆ é™¤ç”¨æˆ·å·¥å…·å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"åˆ é™¤ç”¨æˆ·å·¥å…·å¤±è´¥: {str(e)}")

@app.post("/api/tools/test")
async def test_tool(tool_data: Dict[str, Any]):
    """æµ‹è¯•å·¥å…·è¿æ¥"""
    try:
        tool_type = tool_data.get("type", "api")
        tool_config = tool_data.get("config", {})

        if tool_type == "api":
            # æµ‹è¯•APIå·¥å…·
            base_url = tool_config.get("baseUrl", "")
            method = tool_config.get("method", "GET")
            auth_type = tool_config.get("authType", "none")
            auth_token = tool_config.get("authToken", "")

            if not base_url:
                return {"success": False, "error": "APIåœ°å€ä¸èƒ½ä¸ºç©º"}

            headers = {}
            if auth_type == "bearer" and auth_token:
                headers["Authorization"] = f"Bearer {auth_token}"
            elif auth_type == "apikey" and auth_token:
                headers["X-API-Key"] = auth_token

            import aiohttp
            async with aiohttp.ClientSession() as session:
                try:
                    async with session.request(method, base_url, headers=headers, timeout=10) as response:
                        if response.status < 400:
                            # æ›´æ–°å·¥å…·æµ‹è¯•çŠ¶æ€
                            if "id" in tool_data:
                                await _update_tool_test_status(tool_data["id"], True)
                            return {"success": True, "message": "APIè¿æ¥æµ‹è¯•æˆåŠŸ", "status_code": response.status}
                        else:
                            return {"success": False, "error": f"APIè¿”å›é”™è¯¯çŠ¶æ€ç : {response.status}"}
                except aiohttp.ClientError as e:
                    return {"success": False, "error": f"è¿æ¥å¤±è´¥: {str(e)}"}
        elif tool_type == "ai" or tool_data.get("provider") == "deepseek":
            # æµ‹è¯•AIå·¥å…·ï¼ˆå¦‚DeepSeekæ¨¡å‹ï¼‰
            try:
                # ä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„APIå¯†é’¥ï¼Œè€Œä¸æ˜¯é…ç½®æ–‡ä»¶ä¸­çš„
                api_key = os.getenv("DEEPSEEK_API_KEY") or os.getenv("OPENAI_API_KEY")
                if not api_key:
                    return {"success": False, "error": "æœªé…ç½®DEEPSEEK_API_KEYæˆ–OPENAI_API_KEYç¯å¢ƒå˜é‡"}

                # æµ‹è¯•DeepSeek APIè¿æ¥
                from openai import AsyncOpenAI
                test_client = AsyncOpenAI(
                    api_key=api_key,
                    base_url="https://api.deepseek.com/v1"
                )

                # å‘é€ä¸€ä¸ªç®€å•çš„æµ‹è¯•è¯·æ±‚
                response = await test_client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[{"role": "user", "content": "æµ‹è¯•è¿æ¥"}],
                    max_tokens=10
                )

                if response and response.choices:
                    # æ›´æ–°å·¥å…·æµ‹è¯•çŠ¶æ€
                    if "id" in tool_data:
                        await _update_tool_test_status(tool_data["id"], True)
                    return {"success": True, "message": "DeepSeekæ¨¡å‹è¿æ¥æµ‹è¯•æˆåŠŸ"}
                else:
                    return {"success": False, "error": "DeepSeek APIå“åº”å¼‚å¸¸"}

            except Exception as e:
                return {"success": False, "error": f"DeepSeekæ¨¡å‹æµ‹è¯•å¤±è´¥: {str(e)}"}
        else:
            # å…¶ä»–ç±»å‹å·¥å…·çš„æµ‹è¯•é€»è¾‘
            return {"success": True, "message": f"{tool_type}ç±»å‹å·¥å…·æµ‹è¯•é€šè¿‡"}

    except Exception as e:
        logging.error(f"å·¥å…·æµ‹è¯•å¤±è´¥: {e}")
        return {"success": False, "error": f"æµ‹è¯•å¤±è´¥: {str(e)}"}

async def _update_tool_test_status(tool_id: str, tested: bool = None, approved: bool = None):
    """æ›´æ–°å·¥å…·æµ‹è¯•çŠ¶æ€å’Œæ‰¹å‡†çŠ¶æ€"""
    try:
        # é¦–å…ˆå°è¯•æ›´æ–°ç”¨æˆ·å·¥å…·
        tools = _load_user_tools()
        tool_found = False
        for tool in tools:
            if tool.get("id") == tool_id:
                # åªæœ‰å½“testedä¸ä¸ºNoneæ—¶æ‰æ›´æ–°æµ‹è¯•çŠ¶æ€
                if tested is not None:
                    tool["tested"] = tested

                tool["updated_at"] = datetime.now().isoformat()

                # å¦‚æœæµ‹è¯•æˆåŠŸï¼Œè®¾ç½®ä¸ºå¯ç”¨çŠ¶æ€ï¼ˆä¸éœ€è¦å¾…æ‰¹å‡†ï¼‰
                if tested and approved is None:
                    tool["approval_status"] = "ready"  # æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥ä½¿ç”¨
                    tool["test_passed_at"] = datetime.now().isoformat()
                elif approved is not None:
                    tool["approval_status"] = "shared" if approved else "ready"  # æ‰¹å‡†=åˆ†äº«åˆ°ç³»ç»Ÿ
                    if approved:
                        tool["shared_at"] = datetime.now().isoformat()
                        tool["is_system_tool"] = True  # æ ‡è®°ä¸ºç³»ç»Ÿå·¥å…·

                tool_found = True
                break

        if tool_found:
            _save_user_tools(tools)
        else:
            # å¦‚æœåœ¨ç”¨æˆ·å·¥å…·ä¸­æ²¡æ‰¾åˆ°ï¼Œå°è¯•æ›´æ–°MCPå·¥å…·
            from utils.frontend_config import load_config, save_config, MCP_TOOLS_CONFIG
            mcp_tools = load_config(MCP_TOOLS_CONFIG)
            for tool in mcp_tools:
                if tool.get("id") == tool_id:
                    # åªæœ‰å½“testedä¸ä¸ºNoneæ—¶æ‰æ›´æ–°æµ‹è¯•çŠ¶æ€
                    if tested is not None:
                        tool["tested"] = tested

                    tool["updatedAt"] = datetime.now().isoformat()

                    # å¦‚æœæµ‹è¯•æˆåŠŸï¼Œè®¾ç½®ä¸ºå¯ç”¨çŠ¶æ€ï¼ˆä¸éœ€è¦å¾…æ‰¹å‡†ï¼‰
                    if tested and approved is None:
                        tool["approvalStatus"] = "ready"  # æµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥ä½¿ç”¨
                        tool["testPassedAt"] = datetime.now().isoformat()
                    elif approved is not None:
                        tool["approvalStatus"] = "shared" if approved else "ready"  # æ‰¹å‡†=åˆ†äº«åˆ°ç³»ç»Ÿ
                        if approved:
                            tool["sharedAt"] = datetime.now().isoformat()
                            tool["isSystemTool"] = True  # æ ‡è®°ä¸ºç³»ç»Ÿå·¥å…·

                    tool_found = True
                    break

            if tool_found:
                save_config(MCP_TOOLS_CONFIG, mcp_tools)
                print(f"[OK] MCPå·¥å…·çŠ¶æ€å·²æ›´æ–°: {tool_id}")

        if not tool_found:
            print(f"[WARN] æœªæ‰¾åˆ°å·¥å…·ID: {tool_id}")

    except Exception as e:
        logging.error(f"æ›´æ–°å·¥å…·æµ‹è¯•çŠ¶æ€å¤±è´¥: {e}")

async def _sync_tool_to_system(tool_id: str):
    """å°†å·¥å…·åŒæ­¥åˆ°ç³»ç»Ÿå·¥å…·åº“"""
    try:
        from utils.frontend_config import load_config, save_config, MCP_TOOLS_CONFIG

        # è·å–MCPå·¥å…·
        mcp_tools = load_config(MCP_TOOLS_CONFIG)
        tool_to_sync = None

        for tool in mcp_tools:
            if tool.get("id") == tool_id:
                tool_to_sync = tool
                break

        if not tool_to_sync:
            print(f"[WARN] æœªæ‰¾åˆ°è¦åŒæ­¥çš„å·¥å…·: {tool_id}")
            return False

        # åˆ›å»ºç³»ç»Ÿå·¥å…·ç‰ˆæœ¬ï¼ˆç§»é™¤æ•æ„Ÿä¿¡æ¯ï¼‰
        system_tool = {
            "id": f"system_{tool_id}",  # ç¡®ä¿æœ‰IDå­—æ®µ
            "name": tool_to_sync.get("name"),
            "description": tool_to_sync.get("description"),
            "transport": tool_to_sync.get("server_type", "stdio"),  # ä½¿ç”¨transportè€Œä¸æ˜¯server_type
            "command": tool_to_sync.get("command"),
            "args": tool_to_sync.get("args"),
            "url": tool_to_sync.get("url"),
            "timeout": 60000,
            "env": {},
            "enabled": True,
            "functionalCategory": tool_to_sync.get("functionalCategory", []),  # ä¿æŒåŠŸèƒ½åˆ†ç±»
            "version": tool_to_sync.get("version", "1.0.0"),
            "author": "ç³»ç»Ÿç”¨æˆ·åˆ†äº«",
            "created_at": datetime.now().isoformat(),
            "updated_at": datetime.now().isoformat(),
            "sourceType": "user_shared",
            "capabilities": tool_to_sync.get("capabilities", []),
            "tags": ["ç³»ç»Ÿå·¥å…·", "ç”¨æˆ·åˆ†äº«"],
            "tested": True,
            "approved": True,
            "originalToolId": tool_id,  # è®°å½•åŸå§‹å·¥å…·ID
            "isSystemTool": True
        }

        # æ·»åŠ åˆ°ç³»ç»Ÿå·¥å…·é…ç½®æ–‡ä»¶
        from utils.frontend_config import SYSTEM_TOOLS_CONFIG
        system_tools = load_config(SYSTEM_TOOLS_CONFIG)

        # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨ç›¸åŒçš„å·¥å…·ï¼ˆé¿å…é‡å¤æ·»åŠ ï¼‰
        existing_tool = None
        for existing in system_tools:
            if existing.get("originalToolId") == tool_id or existing.get("name") == tool_to_sync.get("name"):
                existing_tool = existing
                break

        if existing_tool:
            # æ›´æ–°ç°æœ‰å·¥å…·
            existing_tool.update(system_tool)
            print(f"[RELOAD] æ›´æ–°ç°æœ‰ç³»ç»Ÿå·¥å…·: {tool_to_sync.get('name')}")
        else:
            # æ·»åŠ æ–°å·¥å…·
            system_tools.append(system_tool)
            print(f"â• æ·»åŠ æ–°ç³»ç»Ÿå·¥å…·: {tool_to_sync.get('name')}")

        save_config(SYSTEM_TOOLS_CONFIG, system_tools)

        print(f"[OK] å·¥å…·å·²åŒæ­¥åˆ°ç³»ç»Ÿå·¥å…·åº“: {tool_to_sync.get('name')}")
        return True

    except Exception as e:
        print(f"[ERROR] åŒæ­¥å·¥å…·åˆ°ç³»ç»Ÿå¤±è´¥: {str(e)}")
        return False

# æ—§çš„å·¥å…·æ‰§è¡Œç«¯ç‚¹å·²åˆ é™¤ï¼Œä½¿ç”¨æ–°çš„ç»Ÿä¸€ç«¯ç‚¹


# ==============================================================================
# å†…éƒ¨å·¥å…·APIç«¯ç‚¹ - HTTPè¯·æ±‚å·¥å…·å’Œæ•°æ®åˆ†æå·¥å…·
# ==============================================================================

@app.post("/api/internal-tools/http-request")
async def internal_http_request(
    url: str = Body(...),
    method: str = Body("GET"),
    headers: Optional[Dict[str, str]] = Body(None),
    data: Optional[Dict[str, Any]] = Body(None),
    timeout: int = Body(30)
):
    """å†…éƒ¨HTTPè¯·æ±‚å·¥å…·APIç«¯ç‚¹"""
    try:
        result = await http_tool.make_request(
            url=url,
            method=method,
            headers=headers,
            data=data,
            timeout=timeout
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": f"å†…éƒ¨HTTPå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


@app.post("/api/internal-tools/http-request-sync")
async def internal_http_request_sync(
    url: str = Body(...),
    method: str = Body("GET"),
    headers: Optional[Dict[str, str]] = Body(None),
    data: Optional[Dict[str, Any]] = Body(None),
    timeout: int = Body(30)
):
    """å†…éƒ¨HTTPè¯·æ±‚å·¥å…·APIç«¯ç‚¹ï¼ˆåŒæ­¥ç‰ˆæœ¬ï¼‰"""
    try:
        result = http_tool.make_sync_request(
            url=url,
            method=method,
            headers=headers,
            data=data,
            timeout=timeout
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": f"å†…éƒ¨HTTPå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


@app.post("/api/internal-tools/data-analysis")
async def internal_data_analysis(
    data: List[Dict[str, Any]] = Body(...),
    analysis_type: str = Body("basic")
):
    """å†…éƒ¨æ•°æ®åˆ†æå·¥å…·APIç«¯ç‚¹"""
    try:
        result = data_tool.analyze_data(
            data=data,
            analysis_type=analysis_type
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": f"å†…éƒ¨æ•°æ®åˆ†æå·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


@app.post("/api/internal-tools/data-cleaning")
async def internal_data_cleaning(
    data: List[Dict[str, Any]] = Body(...),
    operations: Optional[List[str]] = Body(None)
):
    """å†…éƒ¨æ•°æ®æ¸…æ´—å·¥å…·APIç«¯ç‚¹"""
    try:
        result = data_tool.clean_data(
            data=data,
            operations=operations
        )
        return result
    except Exception as e:
        return {
            "success": False,
            "error": str(e),
            "message": f"å†…éƒ¨æ•°æ®æ¸…æ´—å·¥å…·æ‰§è¡Œå¤±è´¥: {str(e)}"
        }


# ==============================================================================
# AIå¤§è„‘å‡çº§ï¼šæ³¨å…¥äº†å…¨æ–°è§„åˆ’æ€æƒ³çš„ç³»ç»ŸæŒ‡ä»¤
# ==============================================================================
# NOTE: ä½¿ç”¨ .format() æ’å…¥ tool_descriptions æ—¶ï¼Œå¿…é¡»æŠŠ JSON ç¤ºä¾‹ä¸­çš„ { } è¿›è¡Œè½¬ä¹‰ä¸º {{ }}ï¼Œå¦åˆ™ä¼šè¢«å½“æˆå ä½ç¬¦ã€‚
SYSTEM_PROMPT = """
# è§’è‰² (Role)
ä½ æ˜¯ Workflow Plannerï¼Œä¸€ä¸ªä¸“é—¨æŠŠç”¨æˆ·ç›®æ ‡æ‹†è§£ä¸º**æœ‰åºå·¥å…·è°ƒç”¨è®¡åˆ’**çš„ AIã€‚

# ç›®æ ‡ (Goal)
äº§å‡º *ä¸¥æ ¼ç¬¦åˆ JSON ç»“æ„* çš„è¾“å‡ºï¼ŒåŒ…å«ä¸¤ä¸ªå­—æ®µï¼š
1. reasoning: ä½ é€æ­¥æ€è€ƒçš„ä¸­æ–‡è¿‡ç¨‹ï¼›
2. plan: å·¥å…·è°ƒç”¨åˆ—è¡¨ (æŒ‰é¡ºåºæ‰§è¡Œ)ã€‚

# å¯ç”¨å·¥å…· (Tools)
{tool_descriptions}
æ¯æ¡å·¥å…·è¯´æ˜åŒ…å«åç§°å’ŒåŠŸèƒ½æè¿°ï¼Œä½ å¿…é¡»ç¡®ä¿ tool_name å­—æ®µä¸ä¸Šè¡¨å®Œå…¨ä¸€è‡´ã€‚

# é“¾å¼æ€è€ƒ (Chain-of-Thought)
åœ¨å†³å®š plan ä¹‹å‰ï¼Œè¯·å…ˆç”Ÿæˆ reasoningï¼Œåˆ†ä¸‰æ­¥æ€è€ƒï¼š
â‘  åˆ†æç”¨æˆ·æ„å›¾ â‘¡ éœ€è¦å“ªäº›ä¿¡æ¯ â‘¢ é€‰æ‹©å¹¶æ’åºå·¥å…·ã€‚

# è¾“å‡ºæ ¼å¼ (Output Format)
ä»…è¾“å‡ºå¦‚ä¸‹ JSONï¼Œæ— å…¶ä»–å¤šä½™æ–‡å­—ï¼š
{{
  "reasoning": "...",
  "plan": [
    {{"tool_name": "<tool_name>", "arguments": {{"arg1": "..."}}}},
    ...
  ]
}}

# ç¤ºä¾‹ (Few-shot)
<ç”¨æˆ·> æŸ¥è¯¢"åä¸º AI æˆ˜ç•¥ ç™½çš®ä¹¦"å¹¶æ‘˜è¦ </ç”¨æˆ·>
<AI>
{{
  "reasoning": "ç”¨æˆ·æƒ³è·å–ç™½çš®ä¹¦å†…å®¹å¹¶æ¦‚è¦...",
  "plan": [
    {{"tool_name": "backend_tools_api_tools_web_search", "arguments": {{"query": "åä¸º AI æˆ˜ç•¥ ç™½çš®ä¹¦", "limit": 5}}}},
    {{"tool_name": "backend_tools_fetch_tools_safe_fetch_page", "arguments": {{"url": "@node-1.web_search_result[0].href"}}}},
    {{"tool_name": "backend_tools_ai_tools_summarize_text", "arguments": {{"text": "@node-2.safe_fetch_page_result", "style": "concise"}}}}
  ]
}}
</AI>

# çº¦æŸ (Rules)
1. å¿…é¡»ä½¿ç”¨ JSON å¯¹è±¡åŒ…è£¹ reasoning ä¸ plan ä¸¤ä¸ªé”®ã€‚
2. plan å†…æ­¥éª¤æŒ‰æ‰§è¡Œå…ˆåæ’åºï¼›è‹¥ä»»åŠ¡æ¶‰åŠå¤šä¸»ä½“ï¼Œå…ˆå¹¶è¡Œæœç´¢åå¹¶è¡ŒæŠ“å–ï¼›æœ€åæ±‡æ€»ã€‚
3. ä½¿ç”¨ @node-id.output_name è¯­æ³•æ­£ç¡®ä¸²è”æ•°æ®æµã€‚
4. è‹¥æ²¡æœ‰åˆé€‚å·¥å…·ï¼Œè¯šå®è¯´æ˜ï¼Œä½†ä»è¿”å›ç©ºåˆ—è¡¨ planã€‚ 
"""


# --- Tool Discovery and Registration ---
async def create_tool_based_plan_simple(prompt: str, model: str, tools: List[Dict[str, Any]]):
    """
    A fallback planner that uses the basic tool-calling feature.
    """
    messages = [
        {"role": "system", "content": "You are a helpful assistant that can plan tasks by calling available tools. Respond with a list of tool calls required to fulfill the user's request."},
        {"role": "user", "content": prompt}
    ]
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            tools=tools,
            tool_choice="auto",
        )
        response_message = response.choices[0].message
        tool_calls = response_message.tool_calls

        if not tool_calls:
            logging.info("Fallback AI did not choose to use any tools.")
            return None

        plan = []
        for tool_call in tool_calls:
            plan.append({
                "tool_name": tool_call.function.name,
                "arguments": json.loads(tool_call.function.arguments)
            })
        logging.info(f"Received tool-based plan from fallback AI: {plan}")
        return plan
    except Exception as e:
        logging.error(f"Error in fallback create_tool_based_plan_simple: {e}", exc_info=True)
        return None

async def create_fallback_plan(prompt: str, model: str):
    """
    å½“æ²¡æœ‰å·¥å…·å¯ç”¨æ—¶ï¼Œåˆ›å»ºåŸºç¡€çš„å›é€€è®¡åˆ’
    """
    logging.info("Creating fallback plan without registered tools.")

    # åˆ†æpromptç±»å‹ï¼Œç”Ÿæˆå¯¹åº”çš„åŸºç¡€è®¡åˆ’
    prompt_lower = prompt.lower()

    # é€šç”¨è®¡åˆ’
    return [
        {"tool_name": "task_processor", "arguments": {"task": prompt[:100]}},
        {"tool_name": "result_formatter", "arguments": {"result": "å¤„ç†ç»“æœ"}}
    ]

async def create_tool_based_plan(prompt: str, model: str):
    """
    Creates a sequential plan by asking the AI to generate a JSON structure.
    This is the primary, more intelligent planning method.
    """
    tools = tool_registry.get_tools()
    logging.info(f"Found {len(tools)} registered tools")

    if not tools:
        logging.warning("No tools are registered. Creating fallback plan with basic tools.")
        # åˆ›å»ºåŸºç¡€å·¥å…·çš„å›é€€è®¡åˆ’
        return await create_fallback_plan(prompt, model)

    # è®°å½•å¯ç”¨å·¥å…·
    tool_names = [tool["function"]["name"] for tool in tools]
    logging.info(f"Available tools: {tool_names}")

    tool_descriptions = ""
    for tool in tools:
        function_info = tool.get('function', {})
        func_name = function_info.get('name', 'unknown_function')
        func_desc = function_info.get('description', 'No description.')
        tool_descriptions += f"- `{func_name}`: {func_desc}\\n"

    # The assembler now needs to know about the plan to wire things correctly.
    # So we pass the plan to it.
    system_prompt_content = SYSTEM_PROMPT.format(tool_descriptions=tool_descriptions)


    messages = [
        {"role": "system", "content": system_prompt_content},
        {"role": "user", "content": prompt}
    ]

    logging.info("Requesting structured JSON plan from AI.")
    try:
        response = await client.chat.completions.create(
            model=model,
            messages=messages,
            response_format={"type": "json_object"},
        )
        response_content = response.choices[0].message.content
        plan_data = json.loads(response_content)

        if "reasoning" in plan_data and "plan" in plan_data and isinstance(plan_data["plan"], list):
            logging.info(f"Received structured plan from AI: {plan_data['plan']}")
            return plan_data["plan"]
        else:
            logging.warning(f"AI returned malformed JSON plan: {response_content}. Falling back.")
            return await create_tool_based_plan_simple(prompt, model, tools)

    except Exception as e:
        logging.error(f"Structured planning failed: {e}. Falling back to simple tool calling.", exc_info=True)
        return await create_tool_based_plan_simple(prompt, model, tools)


# --- REMOVED OLD PLANNER AND FORMATTER ---

def assembler(plan: list, multi_scene: bool = True):
    """
    New Assembler: Dynamically creates a graph from a linear plan.
    - It expands 'web_search' results into individual 'fetch_page' nodes.
    - It wires up dependencies correctly for a final summarization.
    """
    nodes, edges = [], []
    # ------- 0. Simple fallback: å¦‚æœè®¡åˆ’ä¸­æ²¡æœ‰ web_search ç›¸å…³å·¥å…·ï¼Œç›´æ¥çº¿æ€§ä¸²è” -------
    if not any("web_search" in step.get("tool_name", "") for step in plan):
        layout_y = 0
        prev_id = None
        # Start node
        nodes.append({
            "id": "1",
            "type": "input",
            "position": {"x": 0, "y": layout_y},
            "data": {"label": "Start", "nodeType": "start-topic-node"}
        })
        prev_id = "1"
        layout_y += 120
        for idx, step in enumerate(plan, start=1):
            node_id = f"tool-{idx}"
            nodes.append({
                "id": node_id,
                "type": "execution-node",
                "position": {"x": 0, "y": layout_y},
                "data": {
                    "label": (lambda n: (n[34:] if n.startswith("backend_tools_tool_registry_mcp_") else n)[:24] + ("â€¦" if len(n)>24 else ""))(step["tool_name"]),
                    "nodeType": "execution-node",
                    "params": { **step.get("arguments", {}), "tool_name": step["tool_name"] },
                    "inputs": [],
                    "outputs": ["out"]
                }
            })
            edges.append({"id": f"e-{prev_id}-{node_id}", "source": prev_id, "target": node_id})
            prev_id = node_id
            layout_y += 140

        return {"nodes": nodes, "edges": edges}

    node_id_counter = 1
    layout_x = 0

    # Phase 1: Create initial search nodes from the plan
    # å»é‡ï¼šåŒä¸€æŸ¥è¯¢åªä¿ç•™ä¸€æ¬¡
    seen_queries = set()
    search_tasks = []
    import re, unicodedata

    def normalize_query(text: str) -> str:
        """ç»Ÿä¸€åŒ–å¤„ç†ï¼šå…¨è§’è½¬åŠè§’ã€å»æ ‡ç‚¹ç©ºæ ¼ã€å»å°¾éƒ¨ä¿®é¥°è¯ã€å°å†™"""
        text = unicodedata.normalize('NFKC', text)  # å…¨è§’è½¬åŠè§’
        # å»æ‰€æœ‰ç©ºç™½å­—ç¬¦ï¼ˆåŒ…å«ä¸­æ–‡å…¨è§’ç©ºæ ¼ç­‰ï¼‰
        text = re.sub(r"\s+", "", text, flags=re.UNICODE)
        # å»å¸¸è§ä¸­è‹±æ–‡æ ‡ç‚¹ç¬¦å·
        text = re.sub(r"[\u3000-\u303F\u2000-\u206F\u0021-\u002F\u003A-\u0040\u005B-\u0060\u007B-\u007E]+", "", text)
        text = re.sub(r"(ä¼˜åŠ¿|ä¼˜ç¼ºç‚¹|åˆ†æ|è¯„ä»·|å¯¹æ¯”|æ¯”è¾ƒ)$", "", text)  # å»å¸¸è§ä¿®é¥°
        return text.lower()

    for item in plan:
        if "web_search" not in item.get("tool_name", ""):
            continue
        raw_q = item.get("arguments", {}).get("query", "")
        nq = normalize_query(raw_q)
        if nq and nq not in seen_queries:
            seen_queries.add(nq)
            search_tasks.append(item)

    # å¦‚æœæœªå¯ç”¨å¤šåœºæ™¯ï¼Œä»…ä¿ç•™ç¬¬ä¸€æ¡æŸ¥è¯¢
    if not multi_scene and len(search_tasks) > 1:
        search_tasks = search_tasks[:1]

    last_op_node_ids = []

    for task in search_tasks:
        search_node_id = f"search-node-{node_id_counter - 1}"
        query = task["arguments"].get("query", "Missing query")

        # --- Search Node ---
        nodes.append({
            "id": search_node_id,
            "type": "execution-node",
            "position": {"x": layout_x, "y": 100},
            "data": {
                "label": f"Web Search: '{query[:20]}...'",
                "nodeType": "execution-node",
                "params": {
                    "tool_name": task["tool_name"],
                    "query": query,
                    "limit": 5  # è¯·æ±‚æ›´å¤šç»“æœï¼Œä¾¿äº fallback
                },
                "inputs": [],
                "outputs": ["web_search_result"]
            },
            # é»˜è®¤éšè—ä¸­é—´æŠ€æœ¯èŠ‚ç‚¹ï¼Œå‰ç«¯ä»å¯é€šè¿‡"æ˜¾ç¤ºå…¨éƒ¨"æŒ‰é’®å±•å¼€
            "hidden": False
        })

        # --- Multiple Fetch Nodes per search result ---
        FETCH_PER_SEARCH = 3  # å°è¯•æŠ“å–å‰ 3 æ¡ç»“æœ
        for idx in range(FETCH_PER_SEARCH):
            fetch_node_id = f"fetch-node-{node_id_counter - 1}-{idx}"
            nodes.append({
                "id": fetch_node_id,
                "type": "execution-node",
                "position": {"x": layout_x, "y": 250 + idx * 140},
                "data": {
                    "label": f"Fetch {idx + 1}: '{query[:15]}...'",
                    "nodeType": "execution-node",
                    "params": {
                        "tool_name": "backend_tools_fetch_tools_safe_fetch_page",
                        "url": f"@{search_node_id}.web_search_result[{idx}].href"
                    },
                    "inputs": ["url"],
                    "outputs": ["content"]
                },
                # é»˜è®¤ä¸éšè—
                "hidden": False
            })
            edges.append({
                "id": f"e-{search_node_id}-fetch-{idx}",
                "source": search_node_id,
                "target": fetch_node_id
            })
            last_op_node_ids.append(fetch_node_id)

        layout_x += 320  # ä¸‹ä¸€ä¸ªæœç´¢åˆ†æ”¯æ°´å¹³åç§»
        node_id_counter += 1

    # Phase 2: Create the final summary node
    if last_op_node_ids:
        summary_node_id = "summary-node"
        summary_prompt = (
            "è¯·æ ¹æ®ä»¥ä¸‹æŠ“å–å†…å®¹ï¼Œå¯¹åä¸ºä¸è…¾è®¯åœ¨ AI é¢†åŸŸçš„ä¼˜ç‚¹ã€ç¼ºç‚¹è¿›è¡Œå¯¹æ¯”åˆ†æï¼Œ"
            "å¹¶ç”¨ä¸­æ–‡ä»¥é¡¹ç›®ç¬¦å· (bullet) å½¢å¼è¾“å‡ºï¼š\n"
            "- åä¸ºä¼˜ç‚¹â€¦ã€”1ã€•\n- åä¸ºç¼ºç‚¹â€¦ã€”2ã€•\n- è…¾è®¯ä¼˜ç‚¹â€¦ã€”3ã€•\n- è…¾è®¯ç¼ºç‚¹â€¦ã€”4ã€•\n"
            "è¯·åœ¨æ¯æ¡ bullet æœ«å°¾åŠ ä¸Šæ¥æºç¼–å·ã€”iã€•ï¼Œç¼–å·é¡ºåºæŒ‰ä¸‹æ–‡åˆ—è¡¨é¡ºåºå¯¹åº”ã€‚\n\n"
        )
        for i, fetch_node_id in enumerate(last_op_node_ids):
            summary_prompt += f"æ¥æº{i+1}: @{fetch_node_id}.content \\n\\n"

        nodes.append({
            "id": summary_node_id,
            "type": "output",
            "position": {"x": layout_x / 2 - 100, "y": 400},
            "data": {
                "label": "Final Summary",
                "nodeType": "end-topic-node",
                "params": {"final_prompt": summary_prompt},
                "inputs": [f"@{fetch_node_id}.content" for fetch_node_id in last_op_node_ids],
                "outputs": ["final_summary"]
            }
        })
        for fetch_node_id in last_op_node_ids:
            edges.append({"id": f"e-fetch-summary-{fetch_node_id}", "source": fetch_node_id, "target": summary_node_id})

    # Add a start node for clarity
    nodes.insert(0, {
        "id": "1",  # å‰ç«¯æœŸæœ›çš„ä¸»èŠ‚ç‚¹ ID
        "type": "input",
        "position": {"x": layout_x / 2 - 50, "y": 0},
        "data": {"label": "Start", "nodeType": "start-topic-node"}
    })
    
    for search_node in search_tasks:
        search_node_id = nodes[search_tasks.index(search_node) + 1]['id'] # +1 to account for start node
        edges.append({"id": f"e-start-search-{search_node_id}", "source": "1", "target": search_node_id})

    return {"nodes": nodes, "edges": edges}

# --- API Endpoint ---
@app.post("/chat")
async def handle_chat(request: ChatRequest):
    """
    Handles chat requests. Can either generate a workflow (mindmap) or provide simple chat responses.
    """
    try:
        logging.info(f"[RELOAD] æ”¶åˆ°èŠå¤©è¯·æ±‚ - prompt: {request.prompt[:100]}..., intent: {request.intent}")
        print(f"[RELOAD] æ”¶åˆ°èŠå¤©è¯·æ±‚ - prompt: {request.prompt[:100]}..., intent: {request.intent}")
    except Exception as e:
        logging.error(f"[ERROR] è¯·æ±‚å¤„ç†é”™è¯¯: {e}")
        print(f"[ERROR] è¯·æ±‚å¤„ç†é”™è¯¯: {e}")
        raise HTTPException(status_code=400, detail=f"è¯·æ±‚å¤„ç†é”™è¯¯: {str(e)}")

    # å¦‚æœintentæ˜¯chatï¼Œç›´æ¥è¿”å›èŠå¤©å“åº”
    if request.intent == 'chat':
        return await handle_simple_chat(request)

    # å¦‚æœintentæ˜¯mindmapã€taskæˆ–workflowï¼Œä½¿ç”¨AIå·¥ä½œæµæ„å»ºå™¨
    if request.intent in ['mindmap', 'task', 'workflow']:
        try:
            from ai.intelligent_workflow_orchestrator import IntelligentWorkflowOrchestrator

            # ä½¿ç”¨æ–°çš„æ™ºèƒ½å·¥ä½œæµç¼–æ’å™¨
            orchestrator = IntelligentWorkflowOrchestrator()
            await orchestrator.initialize()

            # ç”Ÿæˆå·¥ä½œæµ
            intelligent_workflow = await orchestrator.generate_intelligent_workflow(request.prompt)

            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            def node_to_dict(node):
                node_type_str = node.type.value if hasattr(node.type, 'value') else str(node.type)
                # ææ–™èŠ‚ç‚¹ä½¿ç”¨customNodeæ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯ï¼Œå…¶ä»–ä½¿ç”¨æ™®é€šèŠ‚ç‚¹ç±»å‹
                if node_type_str == "material-node":
                    frontend_type = "customNode"
                else:
                    frontend_type = node_type_str

                # èŠ‚ç‚¹ç±»å‹åˆ°å‹å¥½åç§°çš„æ˜ å°„
                friendly_names = {
                    "material-node": "ææ–™",
                    "execution-node": "æ‰§è¡Œ",
                    "result-node": "ç»“æœ",
                    "condition-node": "æ¡ä»¶"
                }

                # èŠ‚ç‚¹ç±»å‹åˆ°å›¾æ ‡çš„æ˜ å°„
                type_icons = {
                    "material-node": "[FILE_FOLDER]",
                    "execution-node": "[LIGHTNING]",
                    "result-node": "[PAGE_FACING_UP]",
                    "condition-node": "ğŸ”€"
                }

                friendly_label = friendly_names.get(node_type_str, f"æ™ºèƒ½{node_type_str}")
                type_icon = type_icons.get(node_type_str, "[LIGHTNING]")

                return {
                    "id": node.id,
                    "type": frontend_type,
                    "position": node.position,
                    "data": {
                        "label": friendly_label,
                        "nodeType": frontend_type,
                        "status": "default"  # æ™®é€šèŠ‚ç‚¹çš„é»˜è®¤çŠ¶æ€
                        # ç§»é™¤å¢å¼ºå‹èŠ‚ç‚¹ç‰¹æœ‰çš„å±æ€§ï¼Œä½¿ç”¨æ™®é€šèŠ‚ç‚¹æ ¼å¼
                        # "config": node.config,
                        # "intelligentConfig": True,
                        # "aiEnhanced": True,
                        # "typeIcon": type_icon,
                        # "progress": 0
                    }
                }

            def conn_to_dict(conn):
                return {
                    "id": conn.id,
                    "source": conn.source,
                    "target": conn.target,
                    "type": "enhanced",
                    "animated": False,
                    "className": "silky-smooth",
                    "style": {"strokeWidth": 2, "stroke": "#7c3aed"},
                    "markerEnd": "url(#smooth-arrow)",
                    "data": {
                        "connectionType": conn.type.value if hasattr(conn.type, 'value') else str(conn.type),
                        "conditions": conn.conditions or {},
                        "metadata": conn.metadata or {}
                    }
                }

            workflow_result = {
                "success": True,
                "workflow": {
                    "nodes": [node_to_dict(node) for node in intelligent_workflow.nodes],
                    "edges": [conn_to_dict(conn) for conn in intelligent_workflow.connections],
                    "metadata": intelligent_workflow.metadata
                }
            }

            if workflow_result.get("success"):
                # ç›´æ¥ä½¿ç”¨æ–°APIè¿”å›çš„å·¥ä½œæµæ•°æ®ï¼Œæ— éœ€é¢å¤–è½¬æ¢
                workflow = workflow_result.get("workflow", {})
                nodes = workflow.get("nodes", [])
                edges = workflow.get("edges", [])

                # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•AIç”Ÿæˆçš„èŠ‚ç‚¹ç±»å‹
                logging.info(f"AIå·¥ä½œæµæ„å»ºå™¨è¿”å›çš„èŠ‚ç‚¹ç±»å‹: {[n.get('type') for n in nodes]}")
                logging.info(f"AIå·¥ä½œæµæ„å»ºå™¨è¿”å›çš„èŠ‚ç‚¹æ•°æ®: {[n.get('data', {}).get('nodeType') for n in nodes]}")

                # æ„å»ºå“åº”
                graph = {
                    "nodes": nodes,
                    "edges": edges,
                    "description": workflow.get("metadata", {}).get("analysis", {}).get("workflow_name", "AIç”Ÿæˆçš„å·¥ä½œæµ")
                }

                logging.info(f"Successfully created mindmap with {len(nodes)} nodes using AI workflow builder")
                return JSONResponse(content=graph)
            else:
                # AIå·¥ä½œæµæ„å»ºå™¨å¤±è´¥ï¼Œè¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                logging.warning(f"AI workflow builder failed: {workflow_result.get('error', 'Unknown error')}")
                error_node = {
                    "id": "error-node",
                    "type": "enhanced",
                    "data": {
                        "label": "AIå·¥ä½œæµæ„å»ºå¤±è´¥",
                        "type": "error-node",
                        "nodeType": "error-node",
                        "typeIcon": "[ERROR]",
                        "status": "error",
                        "progress": 0,
                        "params": {"error": f"AIå·¥ä½œæµæ„å»ºå¤±è´¥: {workflow_result.get('error', 'æœªçŸ¥é”™è¯¯')}"},
                        "inputs": [], "outputs": []
                    },
                    "position": {"x": 200, "y": 200}
                }
                return JSONResponse(content={"nodes": [error_node], "edges": []})

        except Exception as e:
            logging.error(f"AI workflow builder failed: {e}")
            # è¿”å›é”™è¯¯ä¿¡æ¯è€Œä¸æ˜¯ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
            error_node = {
                "id": "error-node",
                "type": "enhanced",
                "data": {
                    "label": "AIå·¥ä½œæµæ„å»ºå¼‚å¸¸",
                    "type": "error-node",
                    "nodeType": "error-node",
                    "typeIcon": "[ERROR]",
                    "status": "error",
                    "progress": 0,
                    "params": {"error": f"AIå·¥ä½œæµæ„å»ºå¼‚å¸¸: {str(e)}"},
                    "inputs": [], "outputs": []
                },
                "position": {"x": 200, "y": 200}
            }
            return JSONResponse(content={"nodes": [error_node], "edges": []})




# --- New Workflow Execution Logic ---

# æ¨¡å‹å®šä¹‰å·²ç§»è‡³ schemas.py

# --- NEW: Pydantic Model for Approval Resumption ---
class ApprovalResumePayload(BaseModel):
    nodes: List[Node]
    edges: List[Edge]
    context: Dict[str, Any]
    resume_from_node_id: str # This will be the approval node's id
    approved_data: List[str] # The user-approved list

@app.post("/execute")
async def execute_workflow(payload: WorkflowPayload, user_id: Optional[str] = None):
    """
    Executes a workflow graph provided by the frontend.
    Enhanced with resource management and concurrent execution.
    """
    logging.info("Received workflow for execution.")

    try:
        from ai.concurrent_executor import concurrent_executor
        from utils.resource_manager import resource_manager
        import time

        # Validate the payload
        if not payload.nodes:
            return JSONResponse(status_code=400, content={"error": "No nodes provided"})

        # æ£€æŸ¥èµ„æºå’Œé…é¢
        estimated_duration = len(payload.nodes) * 30  # ä¼°ç®—æ¯ä¸ªèŠ‚ç‚¹30ç§’
        resource_check = await resource_manager.can_execute_task(user_id, estimated_duration)

        if not resource_check['can_execute']:
            return JSONResponse(status_code=429, content={
                "error": "Resource limit exceeded",
                "reason": resource_check['reason'],
                "checks": resource_check['checks']
            })

        # è·å–èµ„æº
        start_time = time.time()
        if not await resource_manager.acquire_resources(user_id):
            return JSONResponse(status_code=503, content={
                "error": "Unable to acquire resources"
            })

        try:
            # === Clarify phase: ask LLM for missing questions ===
            try:
                clarify_prompt = render_prompt("clarify_requirements", user_input=json.dumps([n.data.label for n in payload.nodes], ensure_ascii=False))
                from agent_system.agent_factory import AgentFactory  # local import to avoid circular
                clarify_resp = await AgentFactory.ask_llm(clarify_prompt)

                questions: list[str] = []
                try:
                    questions = json.loads(clarify_resp)
                    if not isinstance(questions, list):
                        questions = []
                except Exception:
                    # fallback: split by lines / numbering
                    for line in clarify_resp.splitlines():
                        clean = line.strip("- â€¢0123456789. ")
                        if clean:
                            questions.append(clean)

                questions = [q for q in questions if q]
                if questions:
                    return {
                        "need_clarify": True,
                        "questions": questions
                    }
            except Exception as e:
                logging.warning(f"Clarify phase skipped due to error: {e}")

            # ä½¿ç”¨å¹¶å‘æ‰§è¡Œå™¨æ‰§è¡Œå·¥ä½œæµ
            result = await concurrent_executor.execute_workflow(
                payload.nodes,
                payload.edges,
                context={},
                progress_callback=None  # å¯ä»¥æ·»åŠ WebSocketè¿›åº¦å›è°ƒ
            )

            # è®°å½•æ‰§è¡Œç»Ÿè®¡
            execution_time = time.time() - start_time
            resource_manager.tracker.record_execution(
                execution_time,
                result.get('stats', {}).get('peak_memory_usage', 0),
                result.get('stats', {}).get('peak_cpu_usage', 0)
            )

            return result

        finally:
            # é‡Šæ”¾èµ„æº
            execution_time = time.time() - start_time
            resource_manager.release_resources(user_id, execution_time)

    except Exception as e:
        logging.error(f"Workflow execution failed: {e}")
        return JSONResponse(status_code=500, content={"error": str(e)})


# --- Refactored Helper Functions for Execution ---

async def resolve_params_from_context(params: Dict, context: Dict, node_id: str) -> Dict:
    """Resolves parameter values that are references to the execution context."""
    from utils.data_flow import data_flow_manager

    resolved_params = {}
    for key, value in params.items():
        if isinstance(value, str) and value.startswith('@'):
            try:
                resolved_value = data_flow_manager.resolve_reference(value, context)
                resolved_params[key] = resolved_value
            except ValueError as e:
                raise ValueError(f"Failed to resolve reference '{value}' in node '{node_id}': {e}")
        elif isinstance(value, dict):
            # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
            resolved_params[key] = await resolve_params_from_context(value, context, node_id)
        elif isinstance(value, list):
            # å¤„ç†åˆ—è¡¨ä¸­çš„å¼•ç”¨
            resolved_list = []
            for item in value:
                if isinstance(item, str) and item.startswith('@'):
                    try:
                        resolved_item = data_flow_manager.resolve_reference(item, context)
                        resolved_list.append(resolved_item)
                    except ValueError as e:
                        raise ValueError(f"Failed to resolve reference '{item}' in node '{node_id}': {e}")
                elif isinstance(item, dict):
                    resolved_item = await resolve_params_from_context(item, context, node_id)
                    resolved_list.append(resolved_item)
                else:
                    resolved_list.append(item)
            resolved_params[key] = resolved_list
        else:
            resolved_params[key] = value
    return resolved_params

async def hydrate_final_prompt(params: Dict, context: Dict) -> str:
    """Substitutes all context references in the final prompt template."""
    prompt_template = params.get("final_prompt", "Please provide a final summary.")
    
    def substitute_context(match):
        ref_string = match.group(1)
        try:
            # Simplified resolver for prompt templates
            ref_node_id, ref_output_name = re.match(r"([\w-]+)\.([\w_]+)", ref_string).groups()
            context_key = f"{ref_node_id}.{ref_output_name}"
            return str(context.get(context_key, ""))
        except Exception:
            return ""
            
    return re.sub(r"@([\w\.-_]+)", substitute_context, prompt_template)


# --- NEW: Endpoint to resume a failed workflow ---
class ResumePayload(BaseModel):
    nodes: List[Node]
    edges: List[Edge]
    context: Dict[str, Any]
    resume_from_node_id: str
    manual_data: str

@app.post("/resume")
async def resume_workflow(payload: ResumePayload):
    """
    Resumes a workflow from a previously failed node, using manually provided data.
    """
    logging.info(f"Resuming workflow from node {payload.resume_from_node_id} with manual data.")
    
    nodes_by_id = {node.id: node for node in payload.nodes}
    adj = {node_id: [] for node_id in nodes_by_id}
    in_degree = {node_id: 0 for node_id in nodes_by_id}
    for edge in payload.edges:
        if edge.source in nodes_by_id and edge.target in nodes_by_id:
            adj[edge.source].append(edge.target)
            in_degree[edge.target] += 1

    # Find the execution path STARTING from the node AFTER the resumed one
    try:
        # Find the node that failed, which is our starting point
        start_node = nodes_by_id.get(payload.resume_from_node_id)
        if not start_node:
            return JSONResponse(status_code=404, content={"error": "Node to resume from not found."})

        # The output of the failed node is the manual data
        # We need to parse the manual data (e.g., comma-separated URLs)
        manual_results = [item.strip() for item in payload.manual_data.replace(',', '\n').split('\n') if item.strip()]
        
        # Inject the manual data into the context
        output_key = f"{start_node.id}.{start_node.data.outputs[0]}"
        execution_context = payload.context
        execution_context[output_key] = manual_results if len(manual_results) > 1 else (manual_results[0] if manual_results else "")
        logging.info(f"Injected manual data into context key: {output_key}")

        # Re-run the topological sort to find the execution order of remaining nodes
        all_node_ids = list(nodes_by_id.keys())
        sub_graph_nodes = []
        
        # We need to find the correct execution order of all nodes, but only execute the ones from the resume point onwards
        queue = [node_id for node_id in all_node_ids if in_degree[node_id] == 0]
        full_ordered_ids = []
        while queue:
            u = queue.pop(0)
            full_ordered_ids.append(u)
            for v in adj.get(u, []):
                in_degree[v] -= 1
                if in_degree[v] == 0:
                    queue.append(v)
        
        # Find where to start the execution
        try:
            start_index = full_ordered_ids.index(payload.resume_from_node_id)
            nodes_to_execute = [nodes_by_id[nid] for nid in full_ordered_ids[start_index + 1:]] # Execute nodes AFTER the manual one
        except ValueError:
             return JSONResponse(status_code=400, content={"error": "Resumed node not found in execution path."})

        # Now, execute the rest of the workflow with the updated context
        resumed_result = await execute_workflow_logic(nodes_to_execute, execution_context, payload)
        
        # The logic function already returns a serializable dict or a JSONResponse,
        # so we can return it directly.
        return resumed_result

    except Exception as e:
        logging.error(f"Error during workflow resumption: {e}", exc_info=True)
        return JSONResponse(status_code=500, content={"error": f"Failed to resume workflow: {e}"})

@app.post("/resume_from_approval")
async def resume_from_approval(payload: ApprovalResumePayload):
    """
    Resumes a workflow from an approval node using user-confirmed data.
    """
    logging.info(f"Resuming workflow from approval node {payload.resume_from_node_id}.")
    
    execution_context = payload.context
    approval_node_id = payload.resume_from_node_id
    
    # Find the approval node in the original payload to get its output key name
    approval_node_obj = None
    for n in payload.nodes:
        if n.id == approval_node_id:
            approval_node_obj = n
            break

    if not approval_node_obj or not approval_node_obj.data.outputs:
        return JSONResponse(status_code=400, content={"error": "Invalid approval node or output not defined."})

    # Inject the approved data into the context
    output_key = f"{approval_node_obj.id}.{approval_node_obj.data.outputs[0]}"
    execution_context[output_key] = payload.approved_data
    logging.info(f"Injected approved data into context key: {output_key}")

    # Now, figure out the rest of the execution path
    nodes_by_id = {node.id: node for node in payload.nodes}
    adj = {node.id: [] for node in nodes_by_id}
    in_degree = {node.id: 0 for node in nodes_by_id}
    for edge in payload.edges:
        if edge.source in nodes_by_id and edge.target in nodes_by_id:
            adj[edge.source].append(edge.target)
            in_degree[edge.target] += 1
    
    # Get full topological order using Kahn's algorithm
    queue = [node_id for node_id in nodes_by_id if in_degree[node_id] == 0]
    full_ordered_ids = []
    while queue:
        u = queue.pop(0)
        full_ordered_ids.append(u)
        for v in adj.get(u, []):
            in_degree[v] -= 1
            if in_degree[v] == 0:
                queue.append(v)
    
    if len(full_ordered_ids) != len(nodes_by_id):
         return JSONResponse(status_code=400, content={"error": "Workflow has a cycle."})

    # Find where to restart
    try:
        start_index = full_ordered_ids.index(approval_node_id)
        # We need to execute nodes AFTER the approval node
        nodes_to_execute = [nodes_by_id[nid] for nid in full_ordered_ids[start_index + 1:]]
    except ValueError:
        return JSONResponse(status_code=400, content={"error": "Approval node not found in execution path."})

    # Execute the rest of the workflow
    resumed_result = await execute_workflow_logic(nodes_to_execute, execution_context, payload)
    
    return resumed_result


async def execute_workflow_logic(ordered_nodes: List[Node], execution_context: Dict, payload: BaseModel, progress_cb: Optional[Callable[[Dict[str, Any]], Awaitable[None]]] = None) -> Dict:
    """
    The actual logic of executing a list of nodes. Refactored to be reusable.
    `payload` is the original request model (WorkflowPayload or ResumePayload) for history saving.
    """
    from workflow.workflow_state import workflow_state_manager
    import uuid

    # ç”Ÿæˆå·¥ä½œæµID
    workflow_id = str(uuid.uuid4())

    # å¼€å§‹å·¥ä½œæµæ‰§è¡Œ
    if isinstance(payload, WorkflowPayload):
        workflow_state_manager.start_workflow(workflow_id, payload)
    for node in ordered_nodes:
        node_type = node.data.nodeType
        logging.info(f"Executing node '{node.data.label}' (Type: {node_type})")

        # å¼€å§‹èŠ‚ç‚¹æ‰§è¡Œ
        workflow_state_manager.start_node(workflow_id, node.id)

        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        from utils.error_handling import debug_info
        debug_info.add_debug_point(
            "node_start",
            {"node_type": node_type, "params": node.data.params},
            node.id
        )

        if progress_cb:
            await progress_cb({"event": "node_start", "node_id": node.id, "label": node.data.label})

        # --- Resolve node parameters from context ---
        params = node.data.params.copy()
        try:
            resolved_params = await resolve_params_from_context(params, execution_context, node.id)
        except ValueError as e:
            logging.error(f"Parameter resolution failed for node {node.id}: {e}")
            return JSONResponse(status_code=400, content={
                "status": "failed",
                "error_node_id": node.id,
                "error_message": str(e),
                "context": execution_context,
            })

        # --- NEW: Handle Approval Node ---
        if node_type == 'approval-node':
            # This is where the workflow pauses. Find the data that needs approval.
            input_ref_name = node.data.inputs[0] if node.data.inputs else None
            
            source_node_id = None
            if isinstance(payload, (WorkflowPayload, ApprovalResumePayload, ResumePayload)):
                 for edge in payload.edges:
                    if edge.target == node.id:
                        source_node_id = edge.source
                        break
            
            if not source_node_id or not input_ref_name:
                 return JSONResponse(status_code=500, content={
                    "status": "failed", "error_node_id": node.id, 
                    "error_message": "Approval node is misconfigured (missing input or incoming edge).", "context": execution_context
                })

            context_key_to_approve = f"{source_node_id}.{input_ref_name}"
            data_to_approve = execution_context.get(context_key_to_approve, [])

            logging.info(f"Pausing workflow at node {node.id} for user approval.")
            return JSONResponse(status_code=200, content={
                "status": "paused_for_approval",
                "node_id": node.id,
                "data_to_approve": data_to_approve,
                "context": execution_context # Send the current context to the frontend
            })

        # --- é¦–å…ˆå°è¯•ä½¿ç”¨æ‰©å±•èŠ‚ç‚¹æ‰§è¡Œå™¨ ---
        from workflow.node_executors import execute_extended_node, NodeExecutionError
        try:
            result = await execute_extended_node(node, execution_context)

            # ä¿å­˜ç»“æœåˆ°ä¸Šä¸‹æ–‡ï¼ˆå¸¦æ•°æ®éªŒè¯ï¼‰
            if node.data.outputs and result is not None:
                context_key = f"{node.id}.{node.data.outputs[0]}"
                try:
                    from utils.data_flow import data_flow_manager
                    validated_result = data_flow_manager.validate_node_output(
                        node.id, node.data.outputs[0], result
                    )
                    execution_context[context_key] = validated_result
                except Exception as e:
                    logging.warning(f"Data validation failed for {context_key}: {e}")
                    execution_context[context_key] = result  # ä½¿ç”¨åŸå§‹ç»“æœ

            # å®ŒæˆèŠ‚ç‚¹æ‰§è¡Œ
            workflow_state_manager.complete_node(workflow_id, node.id, result)

            if progress_cb:
                await progress_cb({"event": "node_done", "node_id": node.id})

            continue  # ç»§ç»­ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

        except NodeExecutionError as e:
            # æ£€æŸ¥æ˜¯å¦æ˜¯"ä¸æ˜¯æ‰©å±•èŠ‚ç‚¹"çš„é”™è¯¯
            if e.error_type == "not_extended":
                # ç»§ç»­ä½¿ç”¨åŸæœ‰é€»è¾‘å¤„ç†
                pass
            else:
                # çœŸæ­£çš„æ‰§è¡Œé”™è¯¯
                from utils.error_handling import error_handler, ErrorSeverity, ErrorCategory

                # å¤„ç†é”™è¯¯
                error_info = error_handler.handle_error(
                    e,
                    severity=ErrorSeverity.HIGH,
                    category=ErrorCategory.EXECUTION,
                    node_id=node.id,
                    workflow_id=workflow_id,
                    context=execution_context
                )

                logging.error(f"Extended node execution failed: {e}")
                workflow_state_manager.fail_node(workflow_id, node.id, str(e))
                workflow_state_manager.fail_workflow(workflow_id, f"Node {node.id} failed: {str(e)}")

                return JSONResponse(status_code=500, content={
                    "status": "failed",
                    "error_node_id": node.id,
                    "error_message": str(e),
                    "error_id": error_info.error_id,
                    "suggestions": error_info.suggestions,
                    "context": execution_context
                })
        except Exception:
            # å¦‚æœä¸æ˜¯æ‰©å±•èŠ‚ç‚¹ç±»å‹ï¼Œç»§ç»­ä½¿ç”¨åŸæœ‰é€»è¾‘
            pass

        # --- Execute node based on its type ---
        if node_type == 'execution-node':
            # æ‰§è¡ŒèŠ‚ç‚¹å¯ä»¥å¤„ç†å·¥å…·è°ƒç”¨æˆ–AIä»»åŠ¡
            tool_name = resolved_params.pop("tool_name", None)

            if tool_name:
                # å¦‚æœæœ‰tool_nameï¼Œæ‰§è¡Œå·¥å…·è°ƒç”¨ï¼ˆç±»ä¼¼åŸæ¥çš„tool-nodeé€»è¾‘ï¼‰
                try:
                    # If tool params include promptx_template, render it first
                    tpl_id = resolved_params.pop("promptx_template", None)
                    if tpl_id:
                        vars_dict = resolved_params.pop("vars", {}) if isinstance(resolved_params.get("vars"), dict) else {}
                        rendered_prompt = render_prompt(tpl_id, **vars_dict)
                        # é»˜è®¤å†™å›åˆ° 'prompt' æˆ– 'value' å­—æ®µï¼Œè‹¥ä¸å­˜åœ¨åˆ™æ–°å¢
                        if "prompt" in resolved_params:
                            resolved_params["prompt"] = rendered_prompt
                        else:
                            resolved_params["value"] = rendered_prompt

                    logging.info(f"Executing tool: {tool_name} with args: {resolved_params}")
                    result = await tool_registry.execute_tool(tool_name, **resolved_params)
                    logging.info(f"Tool '{tool_name}' executed. Result: {str(result)[:200]}...")

                    if isinstance(result, str) and (result.lower().startswith("error") or "failed" in result.lower()):
                        logging.error(f"Tool '{tool_name}' returned a failure condition: {result}")
                        return JSONResponse(status_code=200, content={
                            "status": "failed",
                            "error_node_id": node.id,
                            "error_message": result,
                            "context": execution_context
                        })

                    if node.data.outputs and result and (not isinstance(result, str) or result.strip()):
                        # ç‰¹æ®Šå¤„ç†WordMCPå·¥å…·çš„è¿”å›å€¼
                        if tool_name == "wordmcp" and isinstance(result, dict) and "document_path" in result:
                            # ä¸ºWordMCPå·¥å…·ï¼Œä¿å­˜æ–‡æ¡£è·¯å¾„è€Œä¸æ˜¯æ¶ˆæ¯
                            context_key = f"{node.id}.{node.data.outputs[0]}"
                            execution_context[context_key] = result["document_path"]
                        else:
                            # å…¶ä»–å·¥å…·ï¼Œä¿å­˜åŸå§‹ç»“æœ
                            context_key = f"{node.id}.{node.data.outputs[0]}"
                            execution_context[context_key] = result

                    if progress_cb:
                        await progress_cb({"event": "node_done", "node_id": node.id})

                except Exception as e:
                    logging.error(f"Error executing tool '{tool_name}' in execution-node '{node.id}': {e}", exc_info=True)
                    return JSONResponse(status_code=500, content={
                        "status": "failed",
                        "error_node_id": node.id,
                        "error_message": f"An unexpected error occurred in tool '{tool_name}': {e}",
                        "context": execution_context
                    })
            else:
                # å¦‚æœæ²¡æœ‰tool_nameï¼Œæ‰§è¡ŒAIä»»åŠ¡æˆ–å…¶ä»–æ‰§è¡Œé€»è¾‘
                ai_tool = resolved_params.get("ai_tool", "gpt4")
                user_requirements = resolved_params.get("user_requirements", "")
                output_format = resolved_params.get("output_format", "text")

                # è¿™é‡Œå¯ä»¥æ·»åŠ AIä»»åŠ¡æ‰§è¡Œé€»è¾‘
                # æš‚æ—¶è¿”å›ä¸€ä¸ªæ¨¡æ‹Ÿç»“æœ
                result = f"æ‰§è¡ŒèŠ‚ç‚¹å®Œæˆä»»åŠ¡ï¼š{user_requirements}"

                if node.data.outputs:
                    context_key = f"{node.id}.{node.data.outputs[0]}"
                    execution_context[context_key] = result

                if progress_cb:
                    await progress_cb({"event": "node_done", "node_id": node.id})

        elif node_type == 'end-topic-node':
            try:
                # The final summary node execution
                final_prompt_hydrated = await hydrate_final_prompt(resolved_params, execution_context)
                
                response = await client.chat.completions.create(
                    model="deepseek-chat",
                    messages=[
                        {"role": "system", "content": "You are an AI assistant that summarizes results. Be concise and clear."},
                        {"role": "user", "content": final_prompt_hydrated}
                    ],
                    stream=False
                )
                final_result = response.choices[0].message.content
                logging.info(f"AI node '{node.data.label}' executed. Result: {final_result[:200]}...")

                if node.data.outputs:
                    context_key = f"{node.id}.{node.data.outputs[0]}"
                    execution_context[context_key] = final_result
                
                save_execution_to_history(payload, final_result)
                
                if progress_cb:
                    await progress_cb({"event": "node_done", "node_id": node.id})

                # å®Œæˆå·¥ä½œæµæ‰§è¡Œ
                workflow_state_manager.complete_workflow(workflow_id, final_result)

                # This is the final success response
                return {"status": "success", "result": final_result, "context": execution_context}

            except Exception as e:
                logging.error(f"Error in final AI node execution: {e}", exc_info=True)
                return JSONResponse(status_code=500, content={
                    "status": "failed",
                    "error_node_id": node.id,
                    "error_message": f"Error in final summarization: {e}",
                    "context": execution_context
                })

        elif node_type == 'langchain-node':
            try:
                chain_yaml = resolved_params.get("chain_yaml")
                if not chain_yaml:
                    raise ValueError("langchain-node requires 'chain_yaml' in params.")

                # å…è®¸ç›¸å¯¹è·¯å¾„ï¼›é»˜è®¤æŸ¥æ‰¾ backend/langchain_chains/
                from pathlib import Path
                yaml_path = Path(chain_yaml)
                if not yaml_path.is_absolute():
                    yaml_path = Path(__file__).parent / "langchain_chains" / yaml_path
                if not yaml_path.exists():
                    raise FileNotFoundError(f"Chain yaml not found: {yaml_path}")

                from langchain.load import load_chain

                try:
                    chain = load_chain(str(yaml_path))
                except Exception:
                    # fallback: treat as dotted python path to a Chain instance named `chain`
                    import importlib
                    mod_path, _, attr = str(chain_yaml).rpartition(":")
                    if not mod_path:
                        raise
                    module = importlib.import_module(mod_path)
                    chain = getattr(module, attr or "chain")

                # è¾“å…¥å˜é‡ï¼šç»Ÿä¸€æŠŠ execution_context åˆå¹¶è¿›å»ï¼Œå¯åœ¨ YAML é‡Œå¼•ç”¨
                inputs = {**resolved_params.get("inputs", {}), "context": execution_context}

                # Memory æ³¨å…¥ï¼ˆå¯é€‰ï¼‰
                memory_path = resolved_params.get("memory_path")
                if memory_path:
                    from ai.promptx_adapter import JSONFileMemory
                    try:
                        mem_obj = JSONFileMemory(memory_path)
                        if hasattr(chain, "memory"):
                            chain.memory = mem_obj
                        elif hasattr(chain, "assign_memory"):
                            chain.assign_memory(mem_obj)  # type: ignore[attr-defined]
                    except Exception as mem_err:
                        logging.warning(f"memory_path provided but failed to load: {mem_err}")

                # å›è°ƒï¼Œç”¨äºäº‹ä»¶é€ä¼ 
                callbacks = None
                if progress_cb:
                    from ai.langchain_utils import GraphForwardHandler  # local import to avoid heavy deps if unused
                    handler = GraphForwardHandler(progress_cb)
                    callbacks = [handler]

                # async invoke if supported
                if hasattr(chain, "ainvoke"):
                    result = await chain.ainvoke(inputs, callbacks=callbacks)
                else:
                    result = await asyncio.get_event_loop().run_in_executor(None, lambda: chain.invoke(inputs, callbacks=callbacks))

                if node.data.outputs:
                    context_key = f"{node.id}.{node.data.outputs[0]}"
                    execution_context[context_key] = result

                if progress_cb:
                    await progress_cb({"event": "node_done", "node_id": node.id})
            except Exception as e:
                logging.error(f"Error executing langchain-node '{node.id}': {e}", exc_info=True)
                return JSONResponse(status_code=500, content={
                    "status": "failed",
                    "error_node_id": node.id,
                    "error_message": f"Error in langchain-node execution: {e}",
                    "context": execution_context
                })

    logging.warning("Workflow finished without reaching a summary node.")
    return {"status": "success", "result": "Workflow finished, but no final result was generated.", "context": execution_context}


# --- NEW: Streaming execution endpoint ---
@app.post("/execute_stream")
async def execute_workflow_stream(request: Request):
    """Executes the workflow and streams progress events via Server-Sent Events (SSE)."""
    try:
        # å¼ºåŠ›ä¿®å¤ç¼–ç é—®é¢˜ï¼šå¤šç§æ–¹å¼å°è¯•è§£ç 
        raw_bytes = await request.body()
        raw_data = None
                
                # æ–¹æ³•1: ç›´æ¥ä½¿ç”¨FastAPIçš„request.json()
        try:
            raw_data = await request.json()
            logging.info(f"âœ… [ç¼–ç ä¿®å¤-æ–¹æ³•1] ç›´æ¥JSONè§£ææˆåŠŸï¼ŒèŠ‚ç‚¹æ•°é‡: {len(raw_data.get('nodes', []))}")
        except Exception as e1:
            logging.warning(f"âš ï¸ [ç¼–ç ä¿®å¤-æ–¹æ³•1] ç›´æ¥JSONè§£æå¤±è´¥: {e1}")
            
            # æ–¹æ³•2: æ‰‹åŠ¨UTF-8è§£ç 
            try:
                raw_text = raw_bytes.decode('utf-8')
                raw_data = json.loads(raw_text)
                logging.info(f"âœ… [ç¼–ç ä¿®å¤-æ–¹æ³•2] UTF-8è§£ç æˆåŠŸï¼ŒèŠ‚ç‚¹æ•°é‡: {len(raw_data.get('nodes', []))}")
            except Exception as e2:
                logging.warning(f"âš ï¸ [ç¼–ç ä¿®å¤-æ–¹æ³•2] UTF-8è§£ç å¤±è´¥: {e2}")
                
                # æ–¹æ³•3: å°è¯•å…¶ä»–ç¼–ç 
                for encoding in ['gbk', 'gb2312', 'iso-8859-1']:
                    try:
                        raw_text = raw_bytes.decode(encoding)
                        raw_data = json.loads(raw_text)
                        logging.info(f"âœ… [ç¼–ç ä¿®å¤-æ–¹æ³•3] {encoding}è§£ç æˆåŠŸï¼ŒèŠ‚ç‚¹æ•°é‡: {len(raw_data.get('nodes', []))}")
                        break
                    except Exception as e3:
                        logging.warning(f"âš ï¸ [ç¼–ç ä¿®å¤-æ–¹æ³•3] {encoding}è§£ç å¤±è´¥: {e3}")
                        continue
        
        if raw_data is None:
            raise ValueError("æ— æ³•è§£æè¯·æ±‚æ•°æ®ï¼Œæ‰€æœ‰ç¼–ç æ–¹å¼éƒ½å¤±è´¥äº†")
        
        # ä¿®å¤èŠ‚ç‚¹æ•°æ®ä¸­çš„ä¹±ç é—®é¢˜
        if 'nodes' in raw_data:
            for node in raw_data['nodes']:
                if 'data' in node and 'task' in node['data']:
                    original_task = node['data']['task']
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¹±ç 
                    if '?' in original_task or 'ord' in original_task:
                        logging.warning(f"âš ï¸ [ç¼–ç ä¿®å¤] æ£€æµ‹åˆ°ä¹±ç ä»»åŠ¡æè¿°: {original_task}")
                        # å°è¯•ä¿®å¤å¸¸è§çš„ä¹±ç æ¨¡å¼
                        if "ord" in original_task:
                            node['data']['task'] = "åˆ›å»ºWordæ–‡æ¡£"  # è®¾ç½®é»˜è®¤ä»»åŠ¡
                            logging.info(f"ğŸ”§ [ç¼–ç ä¿®å¤] å·²ä¿®å¤ä»»åŠ¡æè¿°ä¸º: åˆ›å»ºWordæ–‡æ¡£")

        # å°è¯•è§£æä¸ºWorkflowPayload
        payload = WorkflowPayload(**raw_data)
        logging.info(f"Received streaming workflow request with {len(payload.nodes)} nodes and {len(payload.edges)} edges")

    except json.JSONDecodeError as e:
        logging.error(f"âŒ [ç¼–ç ä¿®å¤] JSONè§£æå¤±è´¥: {e}")
        return JSONResponse(status_code=400, content={"error": f"Invalid JSON format: {str(e)}"})
    except UnicodeDecodeError as e:
        logging.error(f"âŒ [ç¼–ç ä¿®å¤] ç¼–ç è§£æå¤±è´¥: {e}")
        return JSONResponse(status_code=400, content={"error": f"Encoding error: {str(e)}"})
    except Exception as e:
        logging.error(f"Failed to parse request: {e}")
        return JSONResponse(status_code=400, content={"error": f"Invalid request format: {str(e)}"})

    queue: asyncio.Queue = asyncio.Queue()

    async def progress_cb(event: Dict[str, Any]):
        await queue.put(event)

    async def worker():
        try:
            logging.info(f"Starting streaming workflow execution with {len(payload.nodes)} nodes")
            await queue.put({"event": "start", "message": "å¼€å§‹æ‰§è¡Œå·¥ä½œæµ"})

            # Re-use logic from /execute to determine execution order
            nodes_by_id = {node.id: node for node in payload.nodes}
            adj = {nid: [] for nid in nodes_by_id}
            in_degree = {nid: 0 for nid in nodes_by_id}
            for edge in payload.edges:
                if edge.source in nodes_by_id and edge.target in nodes_by_id:
                    adj[edge.source].append(edge.target)
                    in_degree[edge.target] += 1
            q = [nid for nid in nodes_by_id if in_degree[nid] == 0]
            ordered_ids = []
            while q:
                u = q.pop(0)
                ordered_ids.append(u)
                for v in adj.get(u, []):
                    in_degree[v] -= 1
                    if in_degree[v] == 0:
                        q.append(v)
            if len(ordered_ids) != len(payload.nodes):
                logging.error("Workflow has a cycle")
                await queue.put({"event": "error", "message": "Workflow has a cycle."})
                await queue.put("[DONE]")
                return

            logging.info(f"Execution order: {ordered_ids}")
            ordered_nodes = [nodes_by_id[nid] for nid in ordered_ids]
            execution_context: Dict[str, Any] = {}
            result = await execute_workflow_logic(ordered_nodes, execution_context, payload, progress_cb)
            logging.info(f"Workflow execution completed with result: {result}")
            await queue.put({"event": "final", "data": result})
        except Exception as e:
            logging.error(f"Streaming workflow failed: {e}", exc_info=True)
            await queue.put({"event": "error", "message": str(e)})
        finally:
            await queue.put("[DONE]")

    # Launch worker task
    asyncio.create_task(worker())

    async def event_generator():
        while True:
            evt = await queue.get()
            if evt == "[DONE]":
                yield "data: [DONE]\n\n"
                break
            try:
                payload = json.dumps(evt, ensure_ascii=False, default=str)
            except TypeError:
                payload = json.dumps(str(evt), ensure_ascii=False)
            yield f"data: {payload}\n\n"

    return StreamingResponse(event_generator(), media_type="text/event-stream")


# --- NEW: LangGraph streaming endpoint ---
from agent_system.langgraph_compiler import run_langgraph_workflow


@app.post("/execute_langgraph_stream")
async def execute_langgraph_stream(payload: WorkflowPayload):
    """Stream execution events powered by LangGraph v2 events."""
    from ai.langgraph_executor import stream_events  # å»¶è¿Ÿå¯¼å…¥é¿å…å¾ªç¯

    async def event_generator():
        try:
            async for ev in stream_events(payload):
                yield json.dumps(ev, ensure_ascii=False) + "\n"
        except asyncio.CancelledError:
            # client disconnected
            pass

    return StreamingResponse(event_generator(), media_type="application/json")


@app.post("/validate_workflow")
async def validate_workflow(request: ValidationRequest):
    """
    Validates the given workflow against loaded rules.
    """
    node_labels = [node.get('data', {}).get('label', '').lower() for node in request.nodes]
    all_labels_text = " ".join(node_labels)

    missing_suggestions = []

    # ç¡®ä¿WORKFLOW_RULESä¸ä¸ºç©º
    if WORKFLOW_RULES is None:
        logging.warning("WORKFLOW_RULES is None, reloading rules...")
        load_workflow_rules()

    if not WORKFLOW_RULES:
        logging.warning("No workflow rules loaded")
        return {"suggestions": []}

    for rule in WORKFLOW_RULES:
        required_keywords = rule.get("required_nodes_keywords", [])
        missing_keywords = []
        for keyword in required_keywords:
            if keyword.lower() not in all_labels_text:
                missing_keywords.append(keyword)
        
        if missing_keywords:
            suggestion = f"æ‚¨çš„ã€Œ{rule.get('workflow_name', 'é€šç”¨')}ã€æµç¨‹å¯èƒ½ç¼ºå¤±ä»¥ä¸‹å…³é”®æ­¥éª¤: {', '.join(missing_keywords)}ã€‚å»ºè®®æ·»åŠ ç›¸å…³èŠ‚ç‚¹ä»¥ä¿è¯æµç¨‹å®Œæ•´æ€§ã€‚"
            missing_suggestions.append({
                "rule_name": rule.get('workflow_name'),
                "suggestion_text": suggestion,
                "missing_keywords": missing_keywords
            })

    return {"suggestions": missing_suggestions}

# --- Dev Orchestrate endpoint: user prompt -> DecompositionEngine -> Orchestrator ---
class DevOrchRequest(BaseModel):
    prompt: str
    # modeå­—æ®µå·²åˆ é™¤ï¼Œç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„æ‰§è¡Œé€»è¾‘

from agent_system.decomposition_engine import DecompositionEngine

@app.post("/dev_orchestrate")
async def dev_orchestrate(request: DevOrchRequest):
    """Experimental endpoint that turns a raw user prompt into high-level tasks, then executes them via Orchestrator (non-stream)."""
    tasks = await DecompositionEngine.decompose(request.prompt)
    logging.info(f"[DevOrch] Decomposed tasks: {tasks}")
    orchestrator = Orchestrator()
    result = await orchestrator.run(tasks)
    return result

# === MCPç›¸å…³é…ç½®å·²ç§»é™¤ ===

@app.get("/node-types")
async def get_node_types():
    """è·å–æ‰€æœ‰å¯ç”¨çš„èŠ‚ç‚¹ç±»å‹é…ç½®"""
    try:
        import json
        from pathlib import Path

        config_path = Path(__file__).parent.parent / "node_types_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)
            return config
        else:
            return {"nodeTypes": {}, "categories": []}
    except Exception as e:
        logging.error(f"Error loading node types config: {e}")
        return {"error": "Failed to load node types configuration"}


def get_available_node_types():
    """è·å–å¯ç”¨çš„èŠ‚ç‚¹ç±»å‹ä½œä¸ºå·¥å…·åˆ—è¡¨"""
    try:
        import json
        from pathlib import Path

        config_path = Path(__file__).parent.parent / "node_types_config.json"
        if config_path.exists():
            with open(config_path, 'r', encoding='utf-8') as f:
                config = json.load(f)

            # è½¬æ¢ä¸ºå·¥å…·æ ¼å¼
            tools = []
            for node_type, node_config in config.get("nodeTypes", {}).items():
                tools.append({
                    "name": node_type,
                    "display_name": node_config.get("name", node_type),
                    "description": node_config.get("description", ""),
                    "category": node_config.get("category", "general"),
                    "icon": node_config.get("icon", "[LIGHTNING]"),
                    "inputs": node_config.get("inputs", []),
                    "outputs": node_config.get("outputs", []),
                    "params": node_config.get("params", {})
                })
            return tools
        else:
            return []
    except Exception as e:
        logging.error(f"Error loading node types: {e}")
        return []


class AIWorkflowRequest(BaseModel):
    requirement: str
    selectedPattern: str = ""
    preferences: dict = {}

@app.post("/api/ai/build-workflow-test")
async def test_ai_endpoint(request: dict):
    """æµ‹è¯•AIç«¯ç‚¹æ˜¯å¦å·¥ä½œ"""
    return {
        "success": True,
        "message": f"æ”¶åˆ°éœ€æ±‚: {request.get('requirement', '')}",
        "data": request
    }

@app.post("/api/ai/build-workflow")
async def build_workflow_with_ai(request: AIWorkflowRequest):
    """AIè‡ªåŠ¨æ„å»ºå·¥ä½œæµï¼ˆçœŸæ­£çš„AIç”Ÿæˆï¼‰"""
    try:
        requirement = request.requirement
        if not requirement:
            return {
                "success": False,
                "error": "éœ€æ±‚æè¿°ä¸èƒ½ä¸ºç©º",
                "message": "è¯·æä¾›è¯¦ç»†çš„éœ€æ±‚æè¿°"
            }

        # åˆ›å»ºOpenAIå®¢æˆ·ç«¯
        api_key = os.getenv("OPENAI_API_KEY") or os.getenv("DEEPSEEK_API_KEY")
        base_url = None

        if os.getenv("DEEPSEEK_API_KEY"):
            base_url = "https://api.deepseek.com"
            api_key = os.getenv("DEEPSEEK_API_KEY")

        if not api_key:
            return {
                "success": False,
                "error": "AIæœåŠ¡æœªé…ç½®",
                "message": "è¯·é…ç½®OPENAI_API_KEYæˆ–DEEPSEEK_API_KEYç¯å¢ƒå˜é‡"
            }

        # åˆ›å»ºçœŸæ­£çš„AIå·¥ä½œæµç”Ÿæˆå™¨
        from ai.enhanced_workflow_generator import EnhancedWorkflowGenerator
        from openai import AsyncOpenAI

        client = AsyncOpenAI(api_key=api_key, base_url=base_url)
        generator = EnhancedWorkflowGenerator(client)

        # è·å–å¯ç”¨çš„å·¥å…·/èŠ‚ç‚¹ç±»å‹
        available_tools = get_available_node_types()

        # ä½¿ç”¨çœŸæ­£çš„AIç”Ÿæˆå·¥ä½œæµ
        ai_result = await generator.generate_workflow(
            user_input=requirement,
            available_tools=available_tools,
            user_preferences=request.preferences
        )

        if ai_result.get('success'):
            # è½¬æ¢ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            workflow = ai_result.get('workflow', {})
            nodes = workflow.get('nodes', [])
            edges = workflow.get('edges', [])

            # è½¬æ¢èŠ‚ç‚¹æ ¼å¼ä¸ºå‰ç«¯æœŸæœ›çš„æ ¼å¼
            formatted_nodes = []
            for i, node in enumerate(nodes):
                formatted_nodes.append({
                    "id": node.get('id', f"ai_node_{i}"),
                    "name": node.get('label', f"èŠ‚ç‚¹ {i+1}"),
                    "nodeType": node.get('type', 'default'),
                    "icon": "ğŸ¤–",
                    "reason": f"AIåˆ†æ: {node.get('description', 'æ™ºèƒ½ç”Ÿæˆçš„èŠ‚ç‚¹')}"
                })

            return {
                "success": True,
                "nodes": formatted_nodes,
                "connections": edges,
                "workflow_description": f"åŸºäºAIåˆ†æç”Ÿæˆçš„å·¥ä½œæµï¼ŒåŒ…å«{len(formatted_nodes)}ä¸ªæ™ºèƒ½èŠ‚ç‚¹",
                "analysis": {
                    "suggested_pattern": ai_result.get('context', {}).get('intent_type', 'intelligent'),
                    "required_capabilities": ["AIç”Ÿæˆ", "æ™ºèƒ½åˆ†æ"],
                    "estimated_complexity": ai_result.get('context', {}).get('complexity', 'medium')
                }
            }
        else:
            # AIç”Ÿæˆå¤±è´¥ï¼Œè¿”å›é”™è¯¯
            return {
                "success": False,
                "error": ai_result.get('error', 'æœªçŸ¥é”™è¯¯'),
                "message": "AIå·¥ä½œæµç”Ÿæˆå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
            }

    except Exception as e:
        logging.error(f"AI workflow building failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "message": "AIå·¥ä½œæµæ„å»ºæœåŠ¡æš‚æ—¶ä¸å¯ç”¨"
        }


@app.get("/api/ai/workflow-patterns")
async def get_workflow_patterns():
    """è·å–å¯ç”¨çš„å·¥ä½œæµæ¨¡å¼"""
    try:
        from services.ai_workflow_builder import ai_workflow_builder

        patterns = []
        for pattern_name, pattern_info in ai_workflow_builder.workflow_patterns.items():
            patterns.append({
                "name": pattern_name,
                "description": pattern_info["description"],
                "keywords": pattern_info["keywords"],
                "nodeCount": len(pattern_info["pattern"])
            })

        return {
            "success": True,
            "patterns": patterns
        }

    except Exception as e:
        logging.error(f"Failed to get workflow patterns: {e}")
        return {
            "success": False,
            "error": str(e)
        }


@app.get("/workflow-status/{workflow_id}")
async def get_workflow_status(workflow_id: str):
    """è·å–å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€"""
    from workflow.workflow_state import workflow_state_manager

    execution = workflow_state_manager.get_workflow_status(workflow_id)
    if execution:
        return execution.to_dict()
    else:
        raise HTTPException(status_code=404, detail="Workflow not found")


@app.get("/workflow-status")
async def list_workflow_status():
    """è·å–æ‰€æœ‰å·¥ä½œæµçŠ¶æ€"""
    from workflow.workflow_state import workflow_state_manager

    executions = workflow_state_manager.get_all_workflows()
    return [execution.to_dict() for execution in executions]


@app.post("/workflow-control/{workflow_id}/{action}")
async def control_workflow(workflow_id: str, action: str, reason: str = "User requested"):
    """æ§åˆ¶å·¥ä½œæµæ‰§è¡Œï¼ˆæš‚åœã€æ¢å¤ã€å–æ¶ˆï¼‰"""
    from workflow.workflow_state import workflow_state_manager

    execution = workflow_state_manager.get_workflow_status(workflow_id)
    if not execution:
        raise HTTPException(status_code=404, detail="Workflow not found")

    if action == "pause":
        workflow_state_manager.pause_workflow(workflow_id, reason)
        return {"status": "paused", "message": f"Workflow {workflow_id} paused"}
    elif action == "resume":
        workflow_state_manager.resume_workflow(workflow_id)
        return {"status": "resumed", "message": f"Workflow {workflow_id} resumed"}
    elif action == "cancel":
        workflow_state_manager.fail_workflow(workflow_id, f"Cancelled: {reason}")
        return {"status": "cancelled", "message": f"Workflow {workflow_id} cancelled"}
    else:
        raise HTTPException(status_code=400, detail="Invalid action. Use 'pause', 'resume', or 'cancel'")

@app.post("/api/workflow/{workflow_id}/reset")
async def reset_workflow_execution(workflow_id: str, payload: WorkflowPayload):
    """é‡ç½®å·¥ä½œæµåˆ°åˆå§‹çŠ¶æ€å¹¶é‡æ–°æ‰§è¡Œ"""
    try:
        from workflow.mindmap_execution_engine import mindmap_execution_engine

        new_workflow_id = await mindmap_execution_engine.reset_and_restart_workflow(workflow_id, payload)
        return {
            "status": "success",
            "message": "Workflow reset and restarted",
            "workflow_id": new_workflow_id
        }
    except Exception as e:
        return {"status": "error", "message": str(e)}

@app.post("/api/workflow/execute")
async def execute_workflow_api(request: Request, user_id: Optional[str] = None):
    """
    API endpoint for workflow execution - compatible with frontend expectations
    """
    try:
        # æ‰‹åŠ¨è§£æè¯·æ±‚æ•°æ®ä»¥å¤„ç†selectedToolå¯¹è±¡æ ¼å¼
        raw_data = await request.json()
        
        # ğŸ” MVPè°ƒè¯•ï¼šæ£€æŸ¥ææ–™èŠ‚ç‚¹æ•°æ®æ˜¯å¦æ­£ç¡®ä¼ é€’
        logger.info("ğŸ” [å·¥ä½œæµæ‰§è¡Œ] å¼€å§‹æ£€æŸ¥æ¥æ”¶åˆ°çš„èŠ‚ç‚¹æ•°æ®")
        if 'nodes' in raw_data:
            for node in raw_data['nodes']:
                if node.get('type') == 'material-node' or node.get('data', {}).get('nodeType') == 'material-node':
                    logger.info(f"ğŸ” [ææ–™èŠ‚ç‚¹] å‘ç°ææ–™èŠ‚ç‚¹: {node['id']}")
                    node_data = node.get('data', {})
                    
                    # æ£€æŸ¥å…³é”®å­—æ®µ
                    target_file = node_data.get('targetFile')
                    selected_files = node_data.get('selectedFiles')
                    
                    logger.info(f"ğŸ” [ææ–™èŠ‚ç‚¹] targetFile: {target_file}")
                    logger.info(f"ğŸ” [ææ–™èŠ‚ç‚¹] selectedFiles: {selected_files}")
                    
                    if selected_files:
                        logger.info(f"ğŸ¯ [ææ–™èŠ‚ç‚¹] æ£€æµ‹åˆ° {len(selected_files)} ä¸ªæ–‡ä»¶!")
                        for i, file_info in enumerate(selected_files):
                            logger.info(f"ğŸ” [æ–‡ä»¶{i+1}] {file_info}")
                    else:
                        logger.warning(f"âš ï¸ [ææ–™èŠ‚ç‚¹] æœªæ£€æµ‹åˆ°selectedFilesæ•°æ®!")
        
        # ğŸ”§ ä¿®å¤selectedToolå¯¹è±¡æ ¼å¼é—®é¢˜
        if 'nodes' in raw_data:
            for node in raw_data['nodes']:
                if 'data' in node and 'selectedTool' in node['data']:
                    selected_tool = node['data']['selectedTool']
                    # å¦‚æœselectedToolæ˜¯å¯¹è±¡ï¼Œæå–nameå­—æ®µ
                    if isinstance(selected_tool, dict) and 'name' in selected_tool:
                        node['data']['selectedTool'] = selected_tool['name']
                        logger.info(f"ğŸ”§ [selectedToolä¿®å¤] è½¬æ¢ {selected_tool} -> {selected_tool['name']}")
        
        # åˆ›å»ºWorkflowPayloadå¯¹è±¡
        from utils.schemas import WorkflowPayload
        payload = WorkflowPayload(**raw_data)
        
        # ä½¿ç”¨æ€ç»´å¯¼å›¾æ‰§è¡Œå¼•æ“
        from workflow.mindmap_execution_engine import mindmap_execution_engine
        import uuid
        
        # ç”Ÿæˆå·¥ä½œæµID
        workflow_id = str(uuid.uuid4())
        
        # å¯åŠ¨å¼‚æ­¥æ‰§è¡Œ - ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼
        try:
            # ç›´æ¥è°ƒç”¨æ‰§è¡Œæ–¹æ³•ï¼Œä¸ä½¿ç”¨create_task
            workflow_id = await mindmap_execution_engine.execute_workflow(payload)
        except Exception as exec_error:
            logger.error(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {exec_error}")
            return JSONResponse(status_code=500, content={
                "success": False,
                "error": f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(exec_error)}"
            })
        
        return {
            "success": True,
            "message": "å·¥ä½œæµå·²å¯åŠ¨",
            "workflow_id": workflow_id
        }
        
    except Exception as e:
        logger.error(f"å·¥ä½œæµå¯åŠ¨å¤±è´¥: {e}")
        return JSONResponse(status_code=500, content={
            "success": False,
            "error": str(e)
        })

@app.get("/api/workflow/{workflow_id}/status")
async def get_workflow_status(workflow_id: str):
    """
    è·å–å·¥ä½œæµæ‰§è¡ŒçŠ¶æ€
    """
    try:
        from workflow.mindmap_execution_engine import mindmap_execution_engine
        
        # è·å–å·¥ä½œæµçŠ¶æ€
        status = await mindmap_execution_engine.get_workflow_status(workflow_id)
        
        if status is None:
            return JSONResponse(status_code=404, content={
                "error": "å·¥ä½œæµä¸å­˜åœ¨"
            })
        
        return status
        
    except Exception as e:
        logger.error(f"è·å–å·¥ä½œæµçŠ¶æ€å¤±è´¥: {e}")
        return JSONResponse(status_code=500, content={
            "error": str(e)
        })

@app.post("/api/workflow/save-result-file")
async def save_result_file(request: Dict[str, Any]):
    """
    ç›´æ¥ä¿å­˜ç»“æœæ–‡ä»¶åˆ°æŒ‡å®šè·¯å¾„ï¼Œè€Œä¸æ˜¯é€šè¿‡æµè§ˆå™¨ä¸‹è½½
    """
    try:
        content = request.get('content', '')
        file_name = request.get('fileName', 'result.txt')
        output_format = request.get('outputFormat', 'txt')
        storage_path = request.get('storagePath', './output')
        
        # ç¡®ä¿å­˜å‚¨ç›®å½•å­˜åœ¨
        import os
        from pathlib import Path
        
        storage_dir = Path(storage_path)
        storage_dir.mkdir(parents=True, exist_ok=True)
        
        # æ„å»ºå®Œæ•´çš„æ–‡ä»¶è·¯å¾„
        file_path = storage_dir / file_name
        
        # æ ¹æ®æ ¼å¼ä¿å­˜æ–‡ä»¶
        if output_format.lower() in ['docx']:
            # å¯¹äºWordæ–‡æ¡£ï¼Œéœ€è¦åˆ›å»ºçœŸæ­£çš„Wordæ–‡ä»¶
            try:
                from docx import Document
                
                doc = Document()
                # æ·»åŠ å†…å®¹åˆ°æ–‡æ¡£
                for line in content.split('\n'):
                    if line.strip():
                        doc.add_paragraph(line)
                
                # ä¿å­˜æ–‡æ¡£
                doc.save(str(file_path))
                print(f"ğŸ“ Wordæ–‡æ¡£å·²ä¿å­˜åˆ°: {file_path}")
                
            except ImportError:
                # å¦‚æœæ²¡æœ‰python-docxï¼Œåˆ™ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(content)
                print(f"ğŸ“ æ–‡æœ¬æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
        else:
            # å…¶ä»–æ ¼å¼ç›´æ¥ä¿å­˜ä¸ºæ–‡æœ¬
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            print(f"ğŸ“ æ–‡ä»¶å·²ä¿å­˜åˆ°: {file_path}")
        
        return {
            "status": "success",
            "message": "æ–‡ä»¶å·²æˆåŠŸä¿å­˜",
            "saved_file_path": str(file_path),
            "filePath": str(file_path),
            "directory": str(storage_dir),
            "fileName": file_name,
            "fileSize": len(content.encode('utf-8'))
        }
        
    except Exception as e:
        logger.error(f"ä¿å­˜ç»“æœæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        raise HTTPException(status_code=500, detail=f"ä¿å­˜æ–‡ä»¶å¤±è´¥: {str(e)}")


@app.get("/debug/errors")
async def get_error_statistics():
    """è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯"""
    from utils.error_handling import error_handler
    return error_handler.get_error_statistics()


@app.get("/debug/recent-errors")
async def get_recent_errors(limit: int = 20):
    """è·å–æœ€è¿‘çš„é”™è¯¯"""
    from utils.error_handling import error_handler
    return error_handler.get_recent_errors(limit)


@app.get("/debug/execution-trace")
async def get_execution_trace():
    """è·å–æ‰§è¡Œè½¨è¿¹"""
    from utils.error_handling import debug_info
    return debug_info.get_execution_trace()


@app.get("/debug/node/{node_id}")
async def get_node_debug_info(node_id: str):
    """è·å–ç‰¹å®šèŠ‚ç‚¹çš„è°ƒè¯•ä¿¡æ¯"""
    from utils.error_handling import debug_info
    return debug_info.get_node_debug_info(node_id)


@app.post("/debug/clear")
async def clear_debug_info():
    """æ¸…ç©ºè°ƒè¯•ä¿¡æ¯"""
    from utils.error_handling import error_handler, debug_info
    error_handler.clear_error_history()
    debug_info.execution_trace.clear()
    debug_info.debug_data.clear()
    return {"status": "cleared", "message": "Debug information cleared"}


@app.get("/performance/stats")
async def get_performance_stats():
    """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
    from ai.concurrent_executor import concurrent_executor, performance_optimizer
    from utils.resource_manager import resource_manager

    stats = concurrent_executor.get_performance_stats()
    analysis = performance_optimizer.analyze_performance(stats)
    resource_status = resource_manager.get_resource_status()

    return {
        "performance_stats": stats,
        "performance_analysis": analysis,
        "resource_status": resource_status
    }


@app.get("/performance/optimize")
async def optimize_performance():
    """æ‰§è¡Œæ€§èƒ½ä¼˜åŒ–"""
    from ai.concurrent_executor import concurrent_executor, performance_optimizer

    stats = concurrent_executor.get_performance_stats()
    analysis = performance_optimizer.analyze_performance(stats)

    applied_optimizations = []
    for recommendation in analysis.get('recommendations', []):
        if performance_optimizer.apply_optimization(recommendation):
            applied_optimizations.append(recommendation)

    return {
        "optimizations_applied": applied_optimizations,
        "new_config": performance_optimizer.current_config,
        "performance_score": analysis.get('performance_score', 0)
    }


@app.get("/resource/status/{user_id}")
async def get_user_resource_status(user_id: str):
    """è·å–ç”¨æˆ·èµ„æºçŠ¶æ€"""
    from utils.resource_manager import resource_manager
    return resource_manager.get_resource_status(user_id)


@app.get("/resource/quota/{user_id}")
async def get_user_quota(user_id: str):
    """è·å–ç”¨æˆ·é…é¢ä¿¡æ¯"""
    from utils.resource_manager import resource_manager
    return resource_manager.quota_manager.get_quota_status(user_id)


@app.get("/templates")
async def get_workflow_templates(category: Optional[str] = None):
    """è·å–å·¥ä½œæµæ¨¡æ¿"""
    from workflow.workflow_templates import template_manager

    if category:
        templates = template_manager.get_templates_by_category(category)
    else:
        templates = template_manager.get_all_templates()

    return {
        "templates": [template.to_dict() for template in templates],
        "categories": template_manager.get_categories()
    }


@app.get("/templates/{template_id}")
async def get_workflow_template(template_id: str):
    """è·å–ç‰¹å®šå·¥ä½œæµæ¨¡æ¿"""
    from workflow.workflow_templates import template_manager

    template = template_manager.get_template(template_id)
    if not template:
        raise HTTPException(status_code=404, detail="Template not found")

    return template.to_dict()


@app.get("/templates/search/{query}")
async def search_workflow_templates(query: str):
    """æœç´¢å·¥ä½œæµæ¨¡æ¿"""
    from workflow.workflow_templates import template_manager

    templates = template_manager.search_templates(query)
    return {
        "templates": [template.to_dict() for template in templates],
        "query": query,
        "count": len(templates)
    }


@app.post("/templates")
async def create_workflow_template(template_data: Dict[str, Any]):
    """åˆ›å»ºæ–°çš„å·¥ä½œæµæ¨¡æ¿"""
    from workflow.workflow_templates import template_manager, WorkflowTemplate

    try:
        template = WorkflowTemplate.from_dict(template_data)
        template_manager.save_template(template)
        return {"status": "success", "template_id": template.id}
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Failed to create template: {str(e)}")


@app.get("/tutorials")
async def get_tutorials(difficulty: Optional[str] = None):
    """è·å–æ•™ç¨‹åˆ—è¡¨"""
    from workflow.tutorial_generator import tutorial_generator

    if difficulty:
        tutorials = tutorial_generator.get_tutorials_by_difficulty(difficulty)
    else:
        tutorials = tutorial_generator.get_all_tutorials()

    return {
        "tutorials": [tutorial.to_dict() for tutorial in tutorials],
        "difficulties": ["åˆçº§", "ä¸­çº§", "é«˜çº§"]
    }


@app.get("/tutorials/{tutorial_id}")
async def get_tutorial(tutorial_id: str):
    """è·å–ç‰¹å®šæ•™ç¨‹"""
    from workflow.tutorial_generator import tutorial_generator

    tutorial = tutorial_generator.get_tutorial(tutorial_id)
    if not tutorial:
        raise HTTPException(status_code=404, detail="Tutorial not found")

    return tutorial.to_dict()


@app.get("/tutorials/search/{query}")
async def search_tutorials(query: str):
    """æœç´¢æ•™ç¨‹"""
    from workflow.tutorial_generator import tutorial_generator

    tutorials = tutorial_generator.search_tutorials(query)
    return {
        "tutorials": [tutorial.to_dict() for tutorial in tutorials],
        "query": query,
        "count": len(tutorials)
    }


@app.get("/documentation/nodes")
async def get_node_documentation():
    """è·å–èŠ‚ç‚¹æ–‡æ¡£"""
    from workflow.tutorial_generator import documentation_generator
    return documentation_generator.generate_node_documentation()


@app.get("/documentation/api")
async def get_api_documentation():
    """è·å–APIæ–‡æ¡£"""
    from workflow.tutorial_generator import documentation_generator
    return documentation_generator.generate_api_documentation()


# --- æ€ç»´å¯¼å›¾ç®¡ç†API ---
@app.get("/api/mindmap/current")
async def get_current_mindmap():
    """è·å–å½“å‰æ€ç»´å¯¼å›¾æ•°æ®"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä»æ•°æ®åº“æˆ–ç¼“å­˜è·å–å½“å‰æ€ç»´å¯¼å›¾çš„é€»è¾‘
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        return {
            "success": True,
            "mindmap": {
                "nodes": [],
                "edges": []
            },
            "message": "è·å–å½“å‰æ€ç»´å¯¼å›¾æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å½“å‰æ€ç»´å¯¼å›¾å¤±è´¥: {str(e)}")

@app.post("/api/mindmap/current")
async def update_current_mindmap(mindmap: dict):
    """æ›´æ–°å½“å‰æ€ç»´å¯¼å›¾æ•°æ®"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ ä¿å­˜åˆ°æ•°æ®åº“æˆ–ç¼“å­˜çš„é€»è¾‘
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿçš„æˆåŠŸå“åº”
        return {
            "success": True,
            "message": "å½“å‰æ€ç»´å¯¼å›¾æ›´æ–°æˆåŠŸ",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å½“å‰æ€ç»´å¯¼å›¾å¤±è´¥: {str(e)}")

@app.post("/api/workflows")
async def create_workflow(workflow: dict):
    """åˆ›å»ºæ–°çš„å·¥ä½œæµ"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“å­˜å‚¨é€»è¾‘
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿçš„æˆåŠŸå“åº”
        workflow_id = f"workflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        return {
            "success": True,
            "id": workflow_id,
            "message": "å·¥ä½œæµåˆ›å»ºæˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ›å»ºå·¥ä½œæµå¤±è´¥: {str(e)}")

@app.put("/api/workflows/{workflow_id}")
async def update_workflow(workflow_id: str, workflow: dict):
    """æ›´æ–°å·¥ä½œæµ"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“æ›´æ–°é€»è¾‘
        return {
            "success": True,
            "id": workflow_id,
            "message": "å·¥ä½œæµæ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°å·¥ä½œæµå¤±è´¥: {str(e)}")

@app.post("/api/workflows/{workflow_id}/access")
async def update_workflow_access(workflow_id: str):
    """æ›´æ–°å·¥ä½œæµè®¿é—®æ—¶é—´"""
    try:
        return {
            "success": True,
            "id": workflow_id,
            "access_time": datetime.now().isoformat(),
            "message": "è®¿é—®æ—¶é—´æ›´æ–°æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"æ›´æ–°è®¿é—®æ—¶é—´å¤±è´¥: {str(e)}")

@app.get("/api/workflows")
async def get_workflows():
    """è·å–å·¥ä½œæµåˆ—è¡¨"""
    try:
        # è¿™é‡Œå¯ä»¥ä»æ•°æ®åº“è·å–çœŸå®çš„å·¥ä½œæµæ•°æ®
        # ç›®å‰è¿”å›æ¨¡æ‹Ÿæ•°æ®
        sample_workflows = [
            {
                "id": "workflow_001",
                "title": "æ•°æ®å¤„ç†è‡ªåŠ¨åŒ–",
                "description": "è‡ªåŠ¨å¤„ç†Excelæ•°æ®å¹¶ç”ŸæˆæŠ¥è¡¨",
                "node_count": 6,
                "edge_count": 5,
                "category": "data",
                "created_at": "2024-01-15T10:30:00",
                "updated_at": "2024-01-20T14:20:00",
                "is_public": False,
                "tags": ["æ•°æ®å¤„ç†", "è‡ªåŠ¨åŒ–", "Excel"]
            },
            {
                "id": "workflow_002", 
                "title": "é‚®ä»¶é€šçŸ¥æµç¨‹",
                "description": "æ£€æµ‹æ–‡ä»¶å˜åŒ–å¹¶å‘é€é‚®ä»¶é€šçŸ¥",
                "node_count": 4,
                "edge_count": 3,
                "category": "notification",
                "created_at": "2024-01-10T09:15:00",
                "updated_at": "2024-01-18T16:45:00",
                "is_public": True,
                "tags": ["é‚®ä»¶", "é€šçŸ¥", "ç›‘æ§"]
            }
        ]
        return sample_workflows
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–å·¥ä½œæµåˆ—è¡¨å¤±è´¥: {str(e)}")

@app.delete("/api/workflows/{workflow_id}")
async def delete_workflow(workflow_id: str):
    """åˆ é™¤å·¥ä½œæµ"""
    try:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“åˆ é™¤é€»è¾‘
        return {
            "success": True,
            "id": workflow_id,
            "message": "å·¥ä½œæµåˆ é™¤æˆåŠŸ"
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å·¥ä½œæµå¤±è´¥: {str(e)}")


# --- æ¨¡æ¿å¸‚åœºAPI ---
@app.get("/api/template-marketplace")
async def get_marketplace_templates():
    """è·å–æ¨¡æ¿å¸‚åœºä¸­çš„æ¨¡æ¿"""
    try:
        # è¯»å–æ¨¡æ¿æ•°æ®æ–‡ä»¶ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›ç©ºåˆ—è¡¨
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        if templates_file.exists():
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates_data = json.load(f)
            return {
                "success": True,
                "templates": templates_data.get("templates", []),
                "total": len(templates_data.get("templates", []))
            }
        else:
            return {
                "success": True,
                "templates": [],
                "total": 0
            }
    except Exception as e:
        logger.error(f"è·å–æ¨¡æ¿å¸‚åœºæ•°æ®å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "templates": [],
            "total": 0,
            "error": str(e)
        }


@app.get("/api/my-templates")
async def get_my_templates():
    """è·å–ç”¨æˆ·ä¸Šä¼ çš„æ¨¡æ¿"""
    try:
        # è¯»å–ç”¨æˆ·æ¨¡æ¿æ•°æ®ï¼Œæš‚æ—¶ä»åŒä¸€ä¸ªæ–‡ä»¶ä¸­ç­›é€‰
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        if templates_file.exists():
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates_data = json.load(f)
            # è¿™é‡Œå¯ä»¥æ ¹æ®ç”¨æˆ·IDç­›é€‰ï¼Œæš‚æ—¶è¿”å›æ‰€æœ‰
            user_templates = templates_data.get("templates", [])
            return {
                "success": True,
                "templates": user_templates,
                "total": len(user_templates)
            }
        else:
            return {
                "success": True,
                "templates": [],
                "total": 0
            }
    except Exception as e:
        logger.error(f"è·å–ç”¨æˆ·æ¨¡æ¿å¤±è´¥: {str(e)}")
        return {
            "success": False,
            "templates": [],
            "total": 0,
            "error": str(e)
        }


# --- æ¨¡æ¿è¯¦æƒ…å’Œäº’åŠ¨API ---
@app.get("/api/template/{template_id}")
async def get_template_detail(template_id: str):
    """è·å–æ¨¡æ¿è¯¦æƒ…"""
    try:
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        if not templates_file.exists():
            return {"success": False, "error": "æ¨¡æ¿æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"}
        
        with open(templates_file, 'r', encoding='utf-8') as f:
            templates_data = json.load(f)
        
        # æŸ¥æ‰¾æŒ‡å®šIDçš„æ¨¡æ¿
        template = None
        for t in templates_data.get("templates", []):
            if t.get("id") == template_id:
                template = t
                break
        
        if not template:
            return {"success": False, "error": "æ¨¡æ¿ä¸å­˜åœ¨"}
        
        # è·å–æ¨¡æ¿çš„è¯„è®ºå’Œäº’åŠ¨æ•°æ®
        comments_file = Path(__file__).parent.parent / "config" / "template_comments.json"
        comments_data = {"comments": []}
        if comments_file.exists():
            with open(comments_file, 'r', encoding='utf-8') as f:
                comments_data = json.load(f)
        
        # ç­›é€‰å‡ºæ­¤æ¨¡æ¿çš„è¯„è®º
        template_comments = [c for c in comments_data.get("comments", []) if c.get("template_id") == template_id]
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        total_ratings = len([c for c in template_comments if c.get("rating")])
        avg_rating = sum(c.get("rating", 0) for c in template_comments if c.get("rating")) / max(total_ratings, 1)
        
        return {
            "success": True,
            "template": {
                **template,
                "stats": {
                    **template.get("stats", {}),
                    "rating": round(avg_rating, 1),
                    "total_ratings": total_ratings,
                    "comments_count": len(template_comments)
                },
                "comments": template_comments[-5:]  # è¿”å›æœ€æ–°5æ¡è¯„è®º
            }
        }
        
    except Exception as e:
        logger.error(f"è·å–æ¨¡æ¿è¯¦æƒ…å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.post("/api/template/{template_id}/comment")
async def add_template_comment(template_id: str, request: Request):
    """æ·»åŠ æ¨¡æ¿è¯„è®º"""
    try:
        data = await request.json()
        comment_text = data.get("comment", "").strip()
        rating = data.get("rating")
        user_name = data.get("user_name", "åŒ¿åç”¨æˆ·")
        
        if not comment_text and not rating:
            return {"success": False, "error": "è¯„è®ºå†…å®¹æˆ–è¯„åˆ†ä¸èƒ½ä¸ºç©º"}
        
        # éªŒè¯è¯„åˆ†èŒƒå›´
        if rating is not None and (rating < 1 or rating > 5):
            return {"success": False, "error": "è¯„åˆ†å¿…é¡»åœ¨1-5ä¹‹é—´"}
        
        # è¯»å–ç°æœ‰è¯„è®ºæ•°æ®
        comments_file = Path(__file__).parent.parent / "config" / "template_comments.json"
        comments_data = {"comments": []}
        if comments_file.exists():
            with open(comments_file, 'r', encoding='utf-8') as f:
                comments_data = json.load(f)
        
        # åˆ›å»ºæ–°è¯„è®º
        new_comment = {
            "id": f"comment_{int(time.time() * 1000)}_{random.randint(1000, 9999)}",
            "template_id": template_id,
            "user_name": user_name,
            "comment": comment_text,
            "rating": rating,
            "created_at": datetime.now().isoformat(),
            "likes": 0,
            "replies": []
        }
        
        # æ·»åŠ åˆ°è¯„è®ºåˆ—è¡¨
        comments_data["comments"].append(new_comment)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        comments_file.parent.mkdir(parents=True, exist_ok=True)
        with open(comments_file, 'w', encoding='utf-8') as f:
            json.dump(comments_data, f, ensure_ascii=False, indent=2)
        
        return {
            "success": True,
            "comment": new_comment,
            "message": "è¯„è®ºæ·»åŠ æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"æ·»åŠ è¯„è®ºå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.get("/api/template/{template_id}/comments")
async def get_template_comments(template_id: str, page: int = 1, limit: int = 10):
    """è·å–æ¨¡æ¿è¯„è®ºåˆ—è¡¨"""
    try:
        comments_file = Path(__file__).parent.parent / "config" / "template_comments.json"
        if not comments_file.exists():
            return {
                "success": True,
                "comments": [],
                "total": 0,
                "page": page,
                "limit": limit
            }
        
        with open(comments_file, 'r', encoding='utf-8') as f:
            comments_data = json.load(f)
        
        # ç­›é€‰æ­¤æ¨¡æ¿çš„è¯„è®º
        template_comments = [c for c in comments_data.get("comments", []) if c.get("template_id") == template_id]
        
        # æŒ‰æ—¶é—´å€’åºæ’åº
        template_comments.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        
        # åˆ†é¡µ
        start_idx = (page - 1) * limit
        end_idx = start_idx + limit
        paginated_comments = template_comments[start_idx:end_idx]
        
        return {
            "success": True,
            "comments": paginated_comments,
            "total": len(template_comments),
            "page": page,
            "limit": limit,
            "has_more": end_idx < len(template_comments)
        }
        
    except Exception as e:
        logger.error(f"è·å–è¯„è®ºåˆ—è¡¨å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.post("/api/template/{template_id}/like")
async def toggle_template_like(template_id: str, request: Request):
    """ç‚¹èµ/å–æ¶ˆç‚¹èµæ¨¡æ¿"""
    try:
        data = await request.json()
        user_id = data.get("user_id", "anonymous")
        
        # è¯»å–ç‚¹èµæ•°æ®
        likes_file = Path(__file__).parent.parent / "config" / "template_likes.json"
        likes_data = {"likes": []}
        if likes_file.exists():
            with open(likes_file, 'r', encoding='utf-8') as f:
                likes_data = json.load(f)
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç»ç‚¹èµ
        existing_like = None
        for like in likes_data["likes"]:
            if like.get("template_id") == template_id and like.get("user_id") == user_id:
                existing_like = like
                break
        
        if existing_like:
            # å–æ¶ˆç‚¹èµ
            likes_data["likes"].remove(existing_like)
            liked = False
        else:
            # æ·»åŠ ç‚¹èµ
            new_like = {
                "template_id": template_id,
                "user_id": user_id,
                "created_at": datetime.now().isoformat()
            }
            likes_data["likes"].append(new_like)
            liked = True
        
        # ä¿å­˜æ•°æ®
        likes_file.parent.mkdir(parents=True, exist_ok=True)
        with open(likes_file, 'w', encoding='utf-8') as f:
            json.dump(likes_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ€»ç‚¹èµæ•°
        total_likes = len([l for l in likes_data["likes"] if l.get("template_id") == template_id])
        
        return {
            "success": True,
            "liked": liked,
            "total_likes": total_likes,
            "message": "ç‚¹èµæˆåŠŸ" if liked else "å–æ¶ˆç‚¹èµæˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"ç‚¹èµæ“ä½œå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.post("/api/template/{template_id}/favorite")
async def toggle_template_favorite(template_id: str, request: Request):
    """æ”¶è—/å–æ¶ˆæ”¶è—æ¨¡æ¿"""
    try:
        data = await request.json()
        user_id = data.get("user_id", "anonymous")
        
        # è¯»å–æ”¶è—æ•°æ®
        favorites_file = Path(__file__).parent.parent / "config" / "template_favorites.json"
        favorites_data = {"favorites": []}
        if favorites_file.exists():
            with open(favorites_file, 'r', encoding='utf-8') as f:
                favorites_data = json.load(f)
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç»æ”¶è—
        existing_favorite = None
        for favorite in favorites_data["favorites"]:
            if favorite.get("template_id") == template_id and favorite.get("user_id") == user_id:
                existing_favorite = favorite
                break
        
        if existing_favorite:
            # å–æ¶ˆæ”¶è—
            favorites_data["favorites"].remove(existing_favorite)
            favorited = False
        else:
            # æ·»åŠ æ”¶è—
            new_favorite = {
                "template_id": template_id,
                "user_id": user_id,
                "created_at": datetime.now().isoformat()
            }
            favorites_data["favorites"].append(new_favorite)
            favorited = True
        
        # ä¿å­˜æ•°æ®
        favorites_file.parent.mkdir(parents=True, exist_ok=True)
        with open(favorites_file, 'w', encoding='utf-8') as f:
            json.dump(favorites_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ€»æ”¶è—æ•°
        total_favorites = len([f for f in favorites_data["favorites"] if f.get("template_id") == template_id])
        
        return {
            "success": True,
            "favorited": favorited,
            "total_favorites": total_favorites,
            "message": "æ”¶è—æˆåŠŸ" if favorited else "å–æ¶ˆæ”¶è—æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"æ”¶è—æ“ä½œå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.get("/api/my-favorites")
async def get_my_favorites():
    """è·å–ç”¨æˆ·æ”¶è—çš„æ¨¡æ¿"""
    try:
        # è¯»å–æ”¶è—æ•°æ®
        favorites_file = Path(__file__).parent.parent / "config" / "template_favorites.json"
        if not favorites_file.exists():
            return {
                "success": True,
                "templates": [],
                "total": 0
            }
        
        with open(favorites_file, 'r', encoding='utf-8') as f:
            favorites_data = json.load(f)
        
        # è¯»å–æ¨¡æ¿æ•°æ®
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        templates_data = {"templates": []}
        if templates_file.exists():
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates_data = json.load(f)
        
        # è·å–ç”¨æˆ·æ”¶è—çš„æ¨¡æ¿IDåˆ—è¡¨
        user_id = "anonymous"  # TODO: ä»sessionæˆ–JWTä¸­è·å–çœŸå®ç”¨æˆ·ID
        favorited_template_ids = [
            f.get("template_id") for f in favorites_data.get("favorites", [])
            if f.get("user_id") == user_id
        ]
        
        # ç­›é€‰å‡ºæ”¶è—çš„æ¨¡æ¿
        favorited_templates = [
            template for template in templates_data.get("templates", [])
            if template.get("id") in favorited_template_ids
        ]
        
        return {
            "success": True,
            "templates": favorited_templates,
            "total": len(favorited_templates)
        }
        
    except Exception as e:
        logger.error(f"è·å–æ”¶è—æ¨¡æ¿å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.post("/api/template-upload")
async def upload_template(template_data: dict):
    """ä¸Šä¼ æ¨¡æ¿åˆ°å¸‚åœº"""
    try:
        # ç”Ÿæˆæ¨¡æ¿IDå’Œæ—¶é—´æˆ³
        template_id = f"template_{int(time.time() * 1000)}"
        current_time = datetime.now().isoformat()
        
        # æ„å»ºæ¨¡æ¿å¯¹è±¡
        template = {
            "id": template_id,
            "title": template_data.get("title", "æœªå‘½åæ¨¡æ¿"),
            "description": template_data.get("description", ""),
            "category": template_data.get("category", "other"),
            "tags": template_data.get("tags", []),
            "marketType": template_data.get("marketType", "free"),
            "isPublic": template_data.get("isPublic", True),
            "price": template_data.get("price", 0),
            "nodes": template_data.get("nodes", []),
            "edges": template_data.get("edges", []),
            "metadata": {
                "nodeCount": template_data.get("metadata", {}).get("nodeCount", 0),
                "edgeCount": template_data.get("metadata", {}).get("edgeCount", 0),
                "originalTitle": template_data.get("metadata", {}).get("originalTitle", ""),
                "source": template_data.get("metadata", {}).get("source", "unknown"),
                "uploadTime": current_time,
                "lastModified": current_time
            },
            "stats": {
                "downloads": 0,
                "likes": 0,
                "views": 0,
                "rating": 0.0,
                "reviews": []
            },
            "author": {
                "id": "user_001",  # æš‚æ—¶ä½¿ç”¨é»˜è®¤ç”¨æˆ·ID
                "name": "ç”¨æˆ·",
                "avatar": ""
            }
        }
        
        # è¯»å–ç°æœ‰æ¨¡æ¿æ•°æ®
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        if templates_file.exists():
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates_data = json.load(f)
        else:
            templates_data = {"templates": []}
        
        # æ·»åŠ æ–°æ¨¡æ¿
        templates_data["templates"].append(template)
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        templates_file.parent.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        with open(templates_file, 'w', encoding='utf-8') as f:
            json.dump(templates_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"æ¨¡æ¿ä¸Šä¼ æˆåŠŸ: {template_id}")
        
        return {
            "success": True,
            "templateId": template_id,
            "message": "æ¨¡æ¿ä¸Šä¼ æˆåŠŸ",
            "template": template
        }
        
    except Exception as e:
        logger.error(f"æ¨¡æ¿ä¸Šä¼ å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¨¡æ¿ä¸Šä¼ å¤±è´¥: {str(e)}")


@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    return await health_checker.get_health_status()


@app.get("/metrics")
async def get_metrics():
    """è·å–ç³»ç»ŸæŒ‡æ ‡"""
    return metrics_collector.get_all_metrics()


@app.get("/metrics/{metric_name}")
async def get_metric(metric_name: str, duration_minutes: int = 60):
    """è·å–ç‰¹å®šæŒ‡æ ‡"""
    points = metrics_collector.get_metric(metric_name, duration_minutes)
    return {
        "metric": metric_name,
        "duration_minutes": duration_minutes,
        "points": [
            {
                "timestamp": point.timestamp.isoformat(),
                "value": point.value,
                "labels": point.labels
            }
            for point in points
        ]
    }


@app.get("/alerts")
async def get_alerts():
    """è·å–å‘Šè­¦ä¿¡æ¯"""
    return {
        "active_alerts": [
            {
                "id": alert.id,
                "name": alert.name,
                "description": alert.description,
                "severity": alert.severity,
                "current_value": alert.current_value,
                "threshold": alert.threshold,
                "triggered_at": alert.triggered_at.isoformat()
            }
            for alert in alert_manager.get_active_alerts()
        ],
        "alert_history": [
            {
                "id": alert.id,
                "name": alert.name,
                "severity": alert.severity,
                "triggered_at": alert.triggered_at.isoformat(),
                "resolved_at": alert.resolved_at.isoformat() if alert.resolved_at else None,
                "status": alert.status
            }
            for alert in alert_manager.get_alert_history()
        ]
    }


@app.get("/config")
async def get_configuration():
    """è·å–åº”ç”¨é…ç½®"""
    config = get_config()
    # éšè—æ•æ„Ÿä¿¡æ¯
    safe_config = {
        "name": config.name,
        "version": config.version,
        "debug": config.debug,
        "resources": {
            "max_memory_mb": config.resources.max_memory_mb,
            "max_cpu_percent": config.resources.max_cpu_percent,
            "max_concurrent_tasks": config.resources.max_concurrent_tasks
        },
        "monitoring": {
            "enable_metrics": config.monitoring.enable_metrics,
            "enable_health_check": config.monitoring.enable_health_check
        }
    }
    return safe_config


@app.get("/tools")
async def list_tools():
    """è¿”å›æ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·åˆ—è¡¨"""
    return tool_registry.get_tools()

@app.get("/tools/debug")
async def debug_tools():
    """è°ƒè¯•å·¥å…·æ³¨å†ŒçŠ¶æ€"""
    tools = tool_registry.get_tools()
    return {
        "total_tools": len(tools),
        "tools": [
            {
                "name": tool["function"]["name"],
                "description": tool["function"]["description"]
            } for tool in tools
        ],
        "tool_names": [tool["function"]["name"] for tool in tools]
    }

@app.post("/tools/{tool_name}/call")
async def call_tool(tool_name: str, payload: dict):
    """è°ƒç”¨æŒ‡å®šçš„å·¥å…·"""
    try:
        result = await tool_registry.execute_tool(tool_name, **payload)
        return {"status": "success", "result": result}
    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/api/tools/execute")
async def execute_tool_from_library(request: dict):
    """ä»å·¥å…·åº“æ‰§è¡Œå·¥å…· - å‰ç«¯å·¥å…·åº“é¡µé¢ä½¿ç”¨"""
    try:
        tool_id = request.get("tool_id")
        tool_type = request.get("tool_type", "function")
        parameters = request.get("parameters", {})

        # å¯¹äºWordMCPå·¥å…·ï¼Œä¸éœ€è¦tool_idï¼Œåªéœ€è¦action
        if tool_type == "wordmcp":
            action = request.get("action")
            if not action:
                return {"status": "error", "error": "ç¼ºå°‘actionå‚æ•°"}
        elif not tool_id:
            return {"status": "error", "error": "ç¼ºå°‘tool_idå‚æ•°"}

        # æ ¹æ®å·¥å…·ç±»å‹æ‰§è¡Œä¸åŒçš„é€»è¾‘
        if tool_type == "function":
            # ç³»ç»Ÿå†…ç½®å‡½æ•°å·¥å…·
            result = await tool_registry.execute_tool(tool_id, **parameters)
            return {
                "status": "success",
                "tool_id": tool_id,
                "tool_type": tool_type,
                "result": result,
                "execution_time": "0.1s"  # ç®€åŒ–çš„æ‰§è¡Œæ—¶é—´
            }

        elif tool_type == "mcp_server":
            # MCPæœåŠ¡å™¨å·¥å…·å·²ç¦ç”¨
            return {
                "status": "error",
                "tool_id": tool_id,
                "tool_type": tool_type,
                "result": "MCPæœåŠ¡å™¨å·¥å…·å·²è¢«ç¦ç”¨",
                "execution_time": "0.0s"
            }

        elif tool_type == "platform_service":
            # æ‰˜ç®¡å¹³å°æœåŠ¡
            platform_info = request.get("platform_info", {})
            platform_id = platform_info.get("platform_id")
            service_id = platform_info.get("service_id")

            if not platform_id or not service_id:
                return {"status": "error", "error": "ç¼ºå°‘å¹³å°ä¿¡æ¯"}

            # è°ƒç”¨æ‰˜ç®¡å¹³å°æœåŠ¡
            from backend.integrations.mcp_manager import mcp_manager
            await mcp_manager.initialize()

            from backend.integrations.mcp_platforms.registry import platform_registry
            platform = platform_registry.get_platform(platform_id)

            if not platform:
                return {"status": "error", "error": f"æœªæ‰¾åˆ°å¹³å°: {platform_id}"}

            # è¿™é‡Œéœ€è¦å®ç°å…·ä½“çš„å¹³å°æœåŠ¡è°ƒç”¨é€»è¾‘
            # æš‚æ—¶è¿”å›æ¨¡æ‹Ÿç»“æœ
            return {
                "status": "success",
                "tool_id": tool_id,
                "tool_type": tool_type,
                "result": f"å¹³å°æœåŠ¡ {service_id} æ‰§è¡ŒæˆåŠŸ (æ¨¡æ‹Ÿç»“æœ)",
                "execution_time": "1.0s",
                "platform": platform.platform_name
            }

        elif tool_type == "user_config":
            # ç”¨æˆ·é…ç½®å·¥å…·
            return {
                "status": "error",
                "error": "ç”¨æˆ·é…ç½®å·¥å…·éœ€è¦å…ˆè¿›è¡Œé…ç½®æ‰èƒ½ä½¿ç”¨",
                "config_required": True
            }

        elif tool_type == "wordmcp":
            # WordMCPå·¥å…· - æ–°çš„MCPæ ‡å‡†åè®®
            action = request.get("action")
            if not action:
                return {"status": "error", "error": "ç¼ºå°‘actionå‚æ•°"}
            
            # ä½¿ç”¨å·¥å…·è·¯ç”±å™¨æ‰§è¡ŒWordMCPå·¥å…·
            tool_call = {
                "tool": "wordmcp",
                "action": action,
                "parameters": request.get("parameters", {})
            }
            
            result = await tool_router.route_tool_call(tool_call)
            
            # è½¬æ¢è¿”å›æ ¼å¼ä¸ºæ ‡å‡†æ ¼å¼
            if result.get("status") == "success":
                response = {
                    "status": "success",
                    "tool_id": f"wordmcp_{action}",
                    "tool_type": tool_type,
                    "result": result.get("result", result.get("message", "Tool executed successfully")),
                    "execution_time": "1.0s"
                }
                
                # å¦‚æœæœ‰æ–‡æ¡£è·¯å¾„ï¼Œæ·»åŠ åˆ°è¿”å›å€¼ä¸­ï¼ˆç”¨äºå·¥ä½œæµæ•°æ®ä¼ é€’ï¼‰
                if "document_path" in result:
                    response["document_path"] = result["document_path"]
                
                return response
            else:
                return {
                    "status": "error",
                    "tool_id": f"wordmcp_{action}",
                    "tool_type": tool_type,
                    "error": result.get("error", "å·¥å…·æ‰§è¡Œå¤±è´¥")
                }

        else:
            return {"status": "error", "error": f"ä¸æ”¯æŒçš„å·¥å…·ç±»å‹: {tool_type}"}

    except Exception as e:
        print(f"æ‰§è¡Œå·¥å…·å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e),
            "tool_id": request.get("tool_id"),
            "tool_type": request.get("tool_type")
        }

# ===== WordMCP ä¸“ç”¨APIç«¯ç‚¹ =====

@app.post("/api/tools/wordmcp")
async def execute_wordmcp_tool(request: dict):
    """æ‰§è¡ŒWordMCPå·¥å…· - ä¸“ç”¨ç«¯ç‚¹æ”¯æŒæ–°çš„MCPæ ‡å‡†åè®®"""
    try:
        action = request.get("action")
        parameters = request.get("parameters", {})
        
        if not action:
            return {"success": False, "error": "ç¼ºå°‘actionå‚æ•°"}
        
        # ä½¿ç”¨å·¥å…·è·¯ç”±å™¨æ‰§è¡Œ
        tool_call = {
            "tool": "wordmcp",
            "action": action,
            "parameters": parameters
        }
        
        result = await tool_router.route_tool_call(tool_call)
        
        # è½¬æ¢è¿”å›æ ¼å¼
        if result.get("status") == "success":
            return {
                "success": True,
                "result": result.get("result", result.get("message", "Tool executed successfully")),
                "message": result.get("message", "å·¥å…·æ‰§è¡ŒæˆåŠŸ")
            }
        else:
            return {
                "success": False,
                "error": result.get("error", "å·¥å…·æ‰§è¡Œå¤±è´¥")
            }
            
    except Exception as e:
        logger.error(f"WordMCPå·¥å…·æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}

@app.get("/api/tools/{tool_id}/info")
async def get_tool_info(tool_id: str):
    """è·å–å·¥å…·è¯¦ç»†ä¿¡æ¯"""
    try:
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç³»ç»Ÿå·¥å…·
        registered_tools = tool_registry.list_tools()
        if tool_id in registered_tools:
            tool_info = registered_tools[tool_id]
            return {
                "status": "success",
                "tool": {
                    "id": tool_id,
                    "name": tool_info.get("display_name", tool_id),
                    "description": tool_info.get("description", ""),
                    "parameters": tool_info.get("parameters", {}),
                    "tool_type": "function",
                    "source": "system"
                }
            }

        # MCPå·¥å…·æ£€æŸ¥å·²ç§»é™¤

        return {"status": "error", "error": f"æœªæ‰¾åˆ°å·¥å…·: {tool_id}"}

    except Exception as e:
        return {"status": "error", "error": str(e)}

@app.post("/test-mcp-tool")
async def test_mcp_tool(config: dict):
    """å®Œæ•´æµ‹è¯•MCPå·¥å…·çš„å¯ç”¨æ€§"""
    try:
        import subprocess
        import tempfile
        import json
        import time
        import asyncio

        # æ„å»ºå‘½ä»¤
        command = [config.get("command", "npx")]
        if config.get("args"):
            command.extend(config["args"])

        # åˆ›å»ºä¸´æ—¶æµ‹è¯•è¿›ç¨‹
        env = os.environ.copy()
        if config.get("env"):
            env.update(config["env"])

        # ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨è¿›ç¨‹æµ‹è¯•
        try:
            process = subprocess.Popen(
                command,
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                text=True
            )

            # ç¬¬äºŒæ­¥ï¼šå‘é€MCPåˆå§‹åŒ–æ¶ˆæ¯
            init_message = {
                "jsonrpc": "2.0",
                "id": 1,
                "method": "initialize",
                "params": {
                    "protocolVersion": "2024-11-05",
                    "capabilities": {},
                    "clientInfo": {"name": "test-client", "version": "1.0.0"}
                }
            }

            # å‘é€åˆå§‹åŒ–æ¶ˆæ¯
            process.stdin.write(json.dumps(init_message) + "\n")
            process.stdin.flush()

            # ç­‰å¾…å“åº”
            time.sleep(1)

            # ç¬¬ä¸‰æ­¥ï¼šå‘é€å·¥å…·åˆ—è¡¨è¯·æ±‚
            tools_message = {
                "jsonrpc": "2.0",
                "id": 2,
                "method": "tools/list"
            }

            process.stdin.write(json.dumps(tools_message) + "\n")
            process.stdin.flush()

            # ç­‰å¾…å“åº”
            time.sleep(1)

            # ç¬¬å››æ­¥ï¼šæµ‹è¯•ä¸€ä¸ªç®€å•çš„å·¥å…·è°ƒç”¨ï¼ˆå¦‚æœæ˜¯office-editorï¼Œæµ‹è¯•create_empty_txtï¼‰
            if "office" in config.get("name", "").lower() or "word" in str(command).lower():
                test_tool_message = {
                    "jsonrpc": "2.0",
                    "id": 3,
                    "method": "tools/call",
                    "params": {
                        "name": "create_empty_txt",
                        "arguments": {
                            "filename": "test_file.txt"
                        }
                    }
                }

                process.stdin.write(json.dumps(test_tool_message) + "\n")
                process.stdin.flush()
                time.sleep(1)

            # ç»ˆæ­¢è¿›ç¨‹å¹¶è¯»å–è¾“å‡º
            process.terminate()
            try:
                stdout, stderr = process.communicate(timeout=3)
            except subprocess.TimeoutExpired:
                process.kill()
                stdout, stderr = process.communicate()

            # åˆ†æè¾“å‡º
            if stdout:
                # æ£€æŸ¥æ˜¯å¦æœ‰æ­£ç¡®çš„JSON-RPCå“åº”
                lines = stdout.strip().split('\n')
                valid_responses = 0

                for line in lines:
                    if line.strip():
                        try:
                            response = json.loads(line)
                            if "jsonrpc" in response and "id" in response:
                                valid_responses += 1
                        except json.JSONDecodeError:
                            continue

                if valid_responses >= 1:  # è‡³å°‘æœ‰ä¸€ä¸ªæœ‰æ•ˆå“åº”
                    return {"success": True, "message": "MCP tool test successful, function normal"}
                else:
                    return {"success": False, "error": f"Tool response format incorrect: {stdout[:200]}"}
            else:
                return {"success": False, "error": f"Tool no response: {stderr[:200] if stderr else 'No error info'}"}

        except subprocess.TimeoutExpired:
            process.kill()
            return {"success": False, "error": "Tool response timeout"}
        except Exception as e:
            return {"success": False, "error": f"Test process error: {str(e)}"}

    except Exception as e:
        return {"success": False, "error": f"Test config error: {str(e)}"}

async def verify_tool_functionality(config: dict, tool_id: str):
    """Verify the actual functionality of the tool"""
    try:
        import subprocess
        import json
        import tempfile
        import os
        import time

        print(f"[WRENCH2] å¼€å§‹åŠŸèƒ½éªŒè¯: {tool_id}")

        # æ ¹æ®å·¥å…·ç±»å‹é€‰æ‹©éªŒè¯ç­–ç•¥
        if "excel" in tool_id.lower():
            return await verify_excel_functionality(config)
        elif "file" in tool_id.lower():
            return await verify_file_functionality(config)
        else:
            # é€šç”¨åŠŸèƒ½éªŒè¯
            return await verify_generic_functionality(config)

    except Exception as e:
        return {
            "success": False,
            "message": f"åŠŸèƒ½éªŒè¯å‡ºé”™: {str(e)}",
            "steps": []
        }

async def verify_excel_functionality(config: dict):
    """éªŒè¯Excelå·¥å…·åŠŸèƒ½"""
    steps = []
    test_file = f"test_verification_{int(time.time())}.xlsx"

    try:
        # æ„å»ºå‘½ä»¤
        command = [config.get("command", "uvx")]
        if config.get("args"):
            command.extend(config["args"])

        print(f"[MEMO2] å¯åŠ¨Excelå·¥å…·è¿›è¡ŒåŠŸèƒ½éªŒè¯...")

        # å¯åŠ¨MCPæœåŠ¡å™¨è¿›ç¨‹
        process = subprocess.Popen(
            command,
            stdin=subprocess.PIPE,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=0
        )

        # ç­‰å¾…å¯åŠ¨
        await asyncio.sleep(2)

        # æ­¥éª¤1: åˆå§‹åŒ–
        init_msg = {
            "jsonrpc": "2.0",
            "id": 1,
            "method": "initialize",
            "params": {
                "protocolVersion": "2024-11-05",
                "capabilities": {},
                "clientInfo": {"name": "test-client", "version": "1.0.0"}
            }
        }

        process.stdin.write(json.dumps(init_msg) + "\n")
        process.stdin.flush()

        # è¯»å–åˆå§‹åŒ–å“åº”
        response_line = process.stdout.readline()
        if response_line:
            try:
                response = json.loads(response_line)
                if response.get("result"):
                    steps.append({
                        "name": "åˆå§‹åŒ–è¿æ¥",
                        "status": "success",
                        "message": "MCPè¿æ¥å»ºç«‹æˆåŠŸ"
                    })
                else:
                    raise Exception("åˆå§‹åŒ–å¤±è´¥")
            except:
                steps.append({
                    "name": "åˆå§‹åŒ–è¿æ¥",
                    "status": "failed",
                    "message": "åˆå§‹åŒ–å“åº”æ ¼å¼é”™è¯¯"
                })
                raise Exception("åˆå§‹åŒ–å¤±è´¥")

        # æ­¥éª¤2: è·å–å·¥å…·åˆ—è¡¨
        list_tools_msg = {
            "jsonrpc": "2.0",
            "id": 2,
            "method": "tools/list"
        }

        process.stdin.write(json.dumps(list_tools_msg) + "\n")
        process.stdin.flush()

        # è¯»å–å·¥å…·åˆ—è¡¨å“åº”
        response_line = process.stdout.readline()
        available_tools = []
        if response_line:
            try:
                response = json.loads(response_line)
                tools = response.get("result", {}).get("tools", [])
                available_tools = [tool.get("name") for tool in tools]
                steps.append({
                    "name": "è·å–å·¥å…·åˆ—è¡¨",
                    "status": "success",
                    "message": f"å‘ç° {len(available_tools)} ä¸ªå¯ç”¨å·¥å…·"
                })
            except:
                steps.append({
                    "name": "è·å–å·¥å…·åˆ—è¡¨",
                    "status": "failed",
                    "message": "æ— æ³•è·å–å·¥å…·åˆ—è¡¨"
                })
                raise Exception("è·å–å·¥å…·åˆ—è¡¨å¤±è´¥")

        # æ­¥éª¤3: æµ‹è¯•åˆ›å»ºå·¥ä½œç°¿åŠŸèƒ½
        if "create_workbook" in available_tools:
            create_msg = {
                "jsonrpc": "2.0",
                "id": 3,
                "method": "tools/call",
                "params": {
                    "name": "create_workbook",
                    "arguments": {
                        "filename": test_file
                    }
                }
            }

            process.stdin.write(json.dumps(create_msg) + "\n")
            process.stdin.flush()

            # è¯»å–åˆ›å»ºå“åº”
            response_line = process.stdout.readline()
            if response_line:
                try:
                    response = json.loads(response_line)
                    if response.get("result"):
                        steps.append({
                            "name": "åˆ›å»ºExcelå·¥ä½œç°¿",
                            "status": "success",
                            "message": f"æˆåŠŸåˆ›å»º {test_file}"
                        })
                    else:
                        steps.append({
                            "name": "åˆ›å»ºExcelå·¥ä½œç°¿",
                            "status": "failed",
                            "message": "åˆ›å»ºå·¥ä½œç°¿å¤±è´¥"
                        })
                except:
                    steps.append({
                        "name": "åˆ›å»ºExcelå·¥ä½œç°¿",
                        "status": "failed",
                        "message": "åˆ›å»ºå“åº”æ ¼å¼é”™è¯¯"
                    })
        else:
            steps.append({
                "name": "åˆ›å»ºExcelå·¥ä½œç°¿",
                "status": "skipped",
                "message": "å·¥å…·ä¸æ”¯æŒcreate_workbookåŠŸèƒ½"
            })

        # æ¸…ç†è¿›ç¨‹
        process.terminate()
        process.wait(timeout=5)

        # åˆ¤æ–­æ•´ä½“æˆåŠŸçŠ¶æ€
        success_count = len([s for s in steps if s["status"] == "success"])
        total_count = len([s for s in steps if s["status"] != "skipped"])

        overall_success = success_count >= total_count * 0.8  # 80%æˆåŠŸç‡

        return {
            "success": overall_success,
            "message": f"ExcelåŠŸèƒ½éªŒè¯å®Œæˆï¼Œ{success_count}/{total_count} é¡¹é€šè¿‡",
            "steps": steps
        }

    except subprocess.TimeoutExpired:
        process.kill()
        return {
            "success": False,
            "message": "åŠŸèƒ½éªŒè¯è¶…æ—¶",
            "steps": steps
        }
    except Exception as e:
        if 'process' in locals():
            process.terminate()
        return {
            "success": False,
            "message": f"åŠŸèƒ½éªŒè¯å¤±è´¥: {str(e)}",
            "steps": steps
        }

async def verify_generic_functionality(config: dict):
    """é€šç”¨åŠŸèƒ½éªŒè¯"""
    return {
        "success": True,
        "message": "é€šç”¨åŠŸèƒ½éªŒè¯é€šè¿‡ï¼ˆåŸºç¡€åè®®æµ‹è¯•ï¼‰",
        "steps": [
            {
                "name": "åè®®å…¼å®¹æ€§",
                "status": "success",
                "message": "MCPåè®®å…¼å®¹"
            }
        ]
    }

async def verify_file_functionality(config: dict):
    """æ–‡ä»¶å·¥å…·åŠŸèƒ½éªŒè¯"""
    # å¯ä»¥åç»­æ‰©å±•
    return await verify_generic_functionality(config)

# ç³»ç»Ÿå·¥å…·ç«¯ç‚¹å·²ç§»é™¤ - MCPåŠŸèƒ½ä¸å¯ç”¨

# MCPåŒæ­¥ç«¯ç‚¹å·²ç§»é™¤ - MCPåŠŸèƒ½ä¸å¯ç”¨

@app.get("/api/tools/library")
async def get_tools_library():
    """è·å–å·¥å…·åº“åˆ—è¡¨ - ä»ç³»ç»Ÿå·¥å…·ä¸é…ç½®æ–‡ä»¶åŠ è½½æ‰€æœ‰å¯ç”¨çš„å·¥å…·"""
    try:
        tools_library = []
        seen_tool_ids = set()

        # 1) ç³»ç»Ÿå·¥å…·
        try:
            system_tools = _load_system_tools()
            for system_tool in system_tools:
                if not system_tool.get("enabled", True):
                    continue
                tool_id = system_tool.get("id", "")
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)
                    functional_category = system_tool.get("functionalCategory", "system_tools")
                    if isinstance(functional_category, list) and len(functional_category) > 0:
                        functional_category = functional_category[0]
                    elif not functional_category:
                        functional_category = "system_tools"
                    tools_library.append({
                        "id": tool_id,
                        "name": system_tool.get("name", "ç³»ç»Ÿå·¥å…·"),
                        "description": system_tool.get("description", "ç³»ç»Ÿå·¥å…·"),
                        "category": "ç³»ç»Ÿå·¥å…·",
                        "functionalCategory": functional_category,
                        "icon": "[GEAR2]",
                        "source": "system_tools",
                        "enabled": True,
                        "tool_type": "system",
                        "approved": True,
                        "tested": system_tool.get("tested", True)
                    })
        except Exception as e:
            logging.error(f"åŠ è½½ç³»ç»Ÿå·¥å…·å¤±è´¥: {e}")

        # 2) AIå·¥å…·ï¼ˆå…¨éƒ¨å·²å¯ç”¨çš„ï¼‰
        try:
            from utils.frontend_config import get_ai_tools
            ai_tools_response = await get_ai_tools()
            if isinstance(ai_tools_response, list):
                for ai_tool in ai_tools_response:
                    if not ai_tool.get("enabled", True):
                        continue
                    tool_id = ai_tool.get("id") or ai_tool.get("name")
                    if tool_id and tool_id not in seen_tool_ids:
                        seen_tool_ids.add(tool_id)
                        tools_library.append({
                            "id": tool_id,
                            "name": ai_tool.get("display_name") or ai_tool.get("name", "AIæ¨¡å‹"),
                            "description": ai_tool.get("description", "AIæ¨¡å‹"),
                            "category": "AIå·¥å…·",
                            "functionalCategory": ai_tool.get("functionalCategory") or "ai_assistant",
                            "icon": "ğŸ¤–",
                            "source": "ai",
                            "enabled": True,
                            "tool_type": "ai",
                            "approved": True,
                            "tested": ai_tool.get("tested", True)
                        })
        except Exception as e:
            logging.error(f"åŠ è½½AIå·¥å…·å¤±è´¥: {e}")

        # 3) MCPå·¥å…·ï¼ˆå…¨éƒ¨å·²å¯ç”¨çš„ï¼‰
        try:
            from utils.frontend_config import load_config, MCP_TOOLS_CONFIG
            mcp_tools = load_config(MCP_TOOLS_CONFIG)
            if not isinstance(mcp_tools, list):
                mcp_tools = []
            for mcp_tool in mcp_tools:
                if not mcp_tool.get("enabled", True):
                    continue
                tool_id = mcp_tool.get("id") or f"mcp_{mcp_tool.get('name','unknown')}"
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)
                    functional_category = mcp_tool.get("functionalCategory") or "system_tools"
                    if isinstance(functional_category, list) and len(functional_category) > 0:
                        functional_category = functional_category[0]
                    tools_library.append({
                        "id": tool_id,
                        "name": mcp_tool.get("name", "MCPå·¥å…·"),
                        "description": mcp_tool.get("description", "MCPå·¥å…·"),
                        "category": "MCPå·¥å…·",
                        "functionalCategory": functional_category,
                        "icon": "[WRENCH2]",
                        "source": "mcp",
                        "enabled": True,
                        "tool_type": "mcp",
                        "approved": mcp_tool.get("approved", False),
                        "tested": mcp_tool.get("tested", False)
                    })
        except Exception as e:
            logging.error(f"åŠ è½½MCPå·¥å…·å¤±è´¥: {e}")

        # 4) APIå·¥å…·ï¼ˆå…¨éƒ¨å·²å¯ç”¨çš„ï¼‰
        try:
            from utils.frontend_config import load_config, API_TOOLS_CONFIG
            api_tools = load_config(API_TOOLS_CONFIG)
            if not isinstance(api_tools, list):
                api_tools = []
            for api_tool in api_tools:
                if not api_tool.get("enabled", True):
                    continue
                tool_id = api_tool.get("id") or f"api_{api_tool.get('name','unknown')}"
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)
                    functional_category = api_tool.get("functionalCategory") or "network_communication"
                    if isinstance(functional_category, list) and len(functional_category) > 0:
                        functional_category = functional_category[0]
                    tools_library.append({
                        "id": tool_id,
                        "name": api_tool.get("name", "APIå·¥å…·"),
                        "description": api_tool.get("description", "APIå·¥å…·"),
                        "category": "APIå·¥å…·",
                        "functionalCategory": functional_category,
                        "icon": "[NETWORK]",
                        "source": "api",
                        "enabled": True,
                        "tool_type": "api",
                        "approved": api_tool.get("approved", False),
                        "tested": api_tool.get("tested", False)
                    })
        except Exception as e:
            logging.error(f"åŠ è½½APIå·¥å…·å¤±è´¥: {e}")

        return {
            "tools": tools_library,
            "total": len(tools_library),
            "sources": {
                "ai": len([t for t in tools_library if t["source"] == "ai"]),
                "api": len([t for t in tools_library if t["source"] == "api"]),
                "mcp": len([t for t in tools_library if t["source"] == "mcp"]),
                "system": len([t for t in tools_library if t["source"] == "system_tools"])
            }
        }

    except Exception as e:
        logging.error(f"è·å–å·¥å…·åº“å¤±è´¥: {e}")
        return {"error": str(e), "tools": [], "total": 0}

@app.get("/api/tools/library/old")
async def get_tools_library_simplified():
    """è·å–å·¥å…·åº“åˆ—è¡¨ - åªä»å››ä¸ªå·¥å…·é¡µé¢çš„é…ç½®æ–‡ä»¶åŠ è½½å®é™…é…ç½®çš„å·¥å…·"""
    try:
        tools_library = []
        seen_tool_ids = set()  # ç”¨äºå»é‡

        # 1. è·å–AIå·¥å…·é…ç½®
        try:
            from utils.frontend_config import load_config, AI_TOOLS_CONFIG
            ai_tools = load_config(AI_TOOLS_CONFIG)

            for ai_tool in ai_tools:
                if not ai_tool.get("enabled", True):
                    continue

                # ä½¿ç”¨idå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨nameå­—æ®µä½œä¸ºid
                tool_id = ai_tool.get("id") or ai_tool.get("name", "")
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)

                    # åˆ¤æ–­æ˜¯å¦ä¸ºæ™ºèƒ½æ¨¡å¼æ£€æµ‹ç³»ç»Ÿï¼ˆç‰¹æ®Šå¤„ç†ï¼‰
                    if ai_tool.get("name") == "intelligent_mode_detection":
                        tools_library.append({
                            "id": tool_id,
                            "name": ai_tool.get("display_name", ai_tool.get("name", "AIå·¥å…·")),
                            "description": ai_tool.get("description", "æ™ºèƒ½æ¨¡å¼æ£€æµ‹å’Œå¤„ç†ç³»ç»Ÿ"),
                            "category": ai_tool.get("category", "AIå·¥å…·"),
                            "functionalCategory": ai_tool.get("functionalCategory", "ai_assistant"),
                            "icon": "[BRAIN]",
                            "source": "ai_config",
                            "enabled": True,
                            "tool_type": "intelligent_system",
                            "config": {
                                "api_base": ai_tool.get("api_base", "/brain"),
                                "endpoints": ai_tool.get("endpoints", {}),
                                "version": ai_tool.get("version", "1.0.0")
                            },
                            "approved": True,
                            "tested": True
                        })
                    else:
                        # æ ‡å‡†AIå·¥å…·å¤„ç†
                        tools_library.append({
                            "id": tool_id,
                            "name": ai_tool.get("display_name", ai_tool.get("name", "AIå·¥å…·")),
                            "description": ai_tool.get("description", f"{ai_tool.get('provider', 'AI')} - {ai_tool.get('model', 'æ¨¡å‹')}"),
                            "category": "AIå·¥å…·",
                            "functionalCategory": ai_tool.get("functionalCategory", "ai_assistant"),
                            "icon": "ğŸ¤–",
                            "source": "ai_config",
                            "enabled": True,
                            "tool_type": "ai",
                            "config": {
                                "provider": ai_tool.get("provider"),
                                "model": ai_tool.get("model"),
                                "max_tokens": ai_tool.get("max_tokens", 4000),
                                "temperature": ai_tool.get("temperature", 0.7),
                                "base_url": ai_tool.get("base_url", "")
                            },
                            "approved": True,
                            "tested": True
                        })
        except Exception as e:
            print(f"è·å–AIå·¥å…·é…ç½®å¤±è´¥: {e}")

        # 2. è·å–APIå·¥å…·é…ç½®
        try:
            from utils.frontend_config import API_TOOLS_CONFIG
            api_tools = load_config(API_TOOLS_CONFIG)

            for api_tool in api_tools:
                if not api_tool.get("enabled", True):
                    continue

                tool_id = api_tool.get("id", "")
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)
                    tools_library.append({
                        "id": tool_id,
                        "name": api_tool.get("name", "APIå·¥å…·"),
                        "description": api_tool.get("description", f"APIå·¥å…· - {api_tool.get('base_url', '')}"),
                        "category": "APIå·¥å…·",
                        "icon": "[LINK3]",
                        "source": "api_config",
                        "enabled": True,
                        "tool_type": "api",
                        "config": {
                            "base_url": api_tool.get("base_url"),
                            "protocol": api_tool.get("protocol", "HTTPS"),
                            "timeout": api_tool.get("timeout", 60000),
                            "auth_type": api_tool.get("auth_type", "Bearer Token"),
                            "supported_methods": api_tool.get("supported_methods", ["GET", "POST"]),
                            "custom_headers": api_tool.get("custom_headers", {})
                        },
                        "approved": True,
                        "tested": api_tool.get("tested", False)
                    })
        except Exception as e:
            print(f"è·å–APIå·¥å…·é…ç½®å¤±è´¥: {e}")

        # 3. è·å–MCPå·¥å…·é…ç½®
        try:
            from utils.frontend_config import MCP_TOOLS_CONFIG
            mcp_tools = load_config(MCP_TOOLS_CONFIG)

            for mcp_tool in mcp_tools:
                if not mcp_tool.get("enabled", True):
                    continue

                tool_id = mcp_tool.get("id", "")
                if tool_id and tool_id not in seen_tool_ids:
                    seen_tool_ids.add(tool_id)
                    tools_library.append({
                        "id": tool_id,
                        "name": mcp_tool.get("name", "MCPå·¥å…·"),
                        "description": mcp_tool.get("description", f"MCPå·¥å…· - {mcp_tool.get('server_type', 'stdio')}"),
                        "category": "MCPå·¥å…·",
                        "icon": "[LIGHTNING]",
                        "source": "mcp_config",
                        "enabled": True,
                        "tool_type": "mcp",
                        "config": {
                            "server_type": mcp_tool.get("server_type"),
                            "command": mcp_tool.get("command"),
                            "args": mcp_tool.get("args", []),
                            "url": mcp_tool.get("url")
                        },
                        "approved": True,
                        "tested": mcp_tool.get("tested", False)
                    })
        except Exception as e:
            print(f"è·å–MCPå·¥å…·é…ç½®å¤±è´¥: {e}")

        # 4. è·å–çœŸå®æ³¨å†Œçš„ç³»ç»Ÿå·¥å…·ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        try:
            registered_tools = tool_registry.get_tools()

            for tool_schema in registered_tools:
                function_info = tool_schema.get("function", {})
                tool_name = function_info.get("name", "unknown")

                # å»é‡æ£€æŸ¥
                if tool_name in seen_tool_ids:
                    continue
                seen_tool_ids.add(tool_name)

                # ç¡¬ç¼–ç å·¥å…·æè¿°ï¼Œé¿å…ç¼–ç é—®é¢˜
                tool_descriptions = {
                    "tools_file_operations_file_operations": "æ–‡ä»¶ç®¡ç†å·¥å…·",
                    "tools_simple_document_create_html_document": "åˆ›å»ºç½‘é¡µæ–‡æ¡£",
                    "tools_simple_document_create_simple_document": "åˆ›å»ºæ–‡æœ¬æ–‡æ¡£"
                }
                description = tool_descriptions.get(tool_name, "ç³»ç»Ÿå·¥å…·")

                tools_library.append({
                    "id": tool_name,
                    "name": function_info.get("name", tool_name).replace('_', ' ').title(),
                    "description": description,
                    "category": "ç³»ç»Ÿå·¥å…·",
                    "icon": _get_tool_icon(tool_name),
                    "source": "system",
                    "enabled": True,
                    "parameters": function_info.get("parameters", {}),
                    "tool_type": "function",
                    "approved": True,
                    "tested": True
                })
        except Exception as e:
            print(f"è·å–ç³»ç»Ÿå·¥å…·å¤±è´¥: {e}")
            # å¦‚æœè·å–ç³»ç»Ÿå·¥å…·å¤±è´¥ï¼Œä¸æ·»åŠ ä»»ä½•æ¨¡æ‹Ÿå·¥å…·

        # 4. è·å–ç³»ç»Ÿå·¥å…·é…ç½®
        try:
            from pathlib import Path
            # ä»backend/core/main.pyå‘ä¸Šä¸¤çº§åˆ°é¡¹ç›®æ ¹ç›®å½•ï¼Œç„¶ååˆ°dataç›®å½•
            system_tools_config = Path(__file__).parent.parent.parent / "data" / "configs" / "backend-config" / "system_tools.json"
            print(f"ç³»ç»Ÿå·¥å…·é…ç½®è·¯å¾„: {system_tools_config}")
            print(f"è·¯å¾„æ˜¯å¦å­˜åœ¨: {system_tools_config.exists()}")

            if system_tools_config.exists():
                with open(system_tools_config, 'r', encoding='utf-8') as f:
                    system_tools = json.load(f)
                print(f"åŠ è½½äº† {len(system_tools)} ä¸ªç³»ç»Ÿå·¥å…·")

                for system_tool in system_tools:
                    if not system_tool.get("enabled", True):
                        continue

                    tool_id = system_tool.get("id", "")
                    if tool_id and tool_id not in seen_tool_ids:
                        seen_tool_ids.add(tool_id)
                        # å¤„ç†åŠŸèƒ½åˆ†ç±» - å¦‚æœæ˜¯æ•°ç»„ï¼Œå–ç¬¬ä¸€ä¸ªï¼›å¦‚æœæ˜¯å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
                        functional_category = system_tool.get("functionalCategory", "system_tools")
                        if isinstance(functional_category, list) and len(functional_category) > 0:
                            functional_category = functional_category[0]
                        elif not functional_category:
                            functional_category = "system_tools"

                        tools_library.append({
                            "id": tool_id,
                            "name": system_tool.get("name", "ç³»ç»Ÿå·¥å…·"),
                            "description": system_tool.get("description", "ç³»ç»Ÿå†…ç½®å·¥å…·"),
                            "category": system_tool.get("category", "ç³»ç»Ÿå·¥å…·"),
                            "functionalCategory": functional_category,
                            "icon": system_tool.get("icon", "[GEAR2]"),
                            "source": system_tool.get("source", "ç³»ç»Ÿå·¥å…·"),
                            "enabled": True,
                            "tool_type": system_tool.get("tool_type", "system_tool"),
                            "tags": system_tool.get("tags", []),
                            "config": system_tool.get("config", {}),
                            "approved": system_tool.get("approved", True),
                            "tested": system_tool.get("tested", True)
                        })
        except Exception as e:
            print(f"åŠ è½½ç³»ç»Ÿå·¥å…·é…ç½®å¤±è´¥: {e}")

        return {
            "status": "success",
            "tools": tools_library,
            "total": len(tools_library),
            "categories": {
                "AIå·¥å…·": len([t for t in tools_library if t["category"] == "AIå·¥å…·"]),
                "APIå·¥å…·": len([t for t in tools_library if t["category"] == "APIå·¥å…·"]),
                "MCPå·¥å…·": len([t for t in tools_library if t["category"] == "MCPå·¥å…·"]),
                "ç³»ç»Ÿå·¥å…·": len([t for t in tools_library if t["category"] == "ç³»ç»Ÿå·¥å…·"])
            },
            "last_updated": datetime.now().isoformat()
        }

    except Exception as e:
        print(f"è·å–å·¥å…·åº“å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {
            "status": "error",
            "error": str(e),
            "tools": [],
            "total": 0,
            "last_updated": datetime.now().isoformat()
        }

@app.get("/api/tools/sync-status")
async def get_tools_sync_status():
    """è·å–å·¥å…·åŒæ­¥çŠ¶æ€"""
    try:
        from utils.frontend_config import load_config, AI_TOOLS_CONFIG, API_TOOLS_CONFIG, MCP_TOOLS_CONFIG

        ai_tools = load_config(AI_TOOLS_CONFIG)
        api_tools = load_config(API_TOOLS_CONFIG)
        mcp_tools = load_config(MCP_TOOLS_CONFIG)

        return {
            "status": "success",
            "sync_status": {
                "ai_tools_count": len(ai_tools),
                "api_tools_count": len(api_tools),
                "mcp_tools_count": len(mcp_tools),
                "total_configured_tools": len(ai_tools) + len(api_tools) + len(mcp_tools),
                "last_sync": datetime.now().isoformat()
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "message": str(e),
            "sync_status": {}
        }

def _get_tool_icon(tool_name: str) -> str:
    """æ ¹æ®å·¥å…·åç§°è·å–å›¾æ ‡"""
    icon_map = {
        # æœç´¢ç›¸å…³
        "web_search": "[MAGNIFYING_GLASS_LEFT]",
        "search": "[MAGNIFYING_GLASS_LEFT]",
        "fetch": "[INBOX_TRAY]",
        "fetch_page": "[PAGE_FACING_UP]",

        # æ–‡ä»¶æ“ä½œ
        "file": "[FILE_FOLDER]",
        "document": "[PAGE_FACING_UP]",
        "word": "[MEMO2]",
        "excel": "[BAR_CHART2]",
        "pdf": "[CLIPBOARD2]",

        # ç½‘ç»œè¯·æ±‚
        "http": "[NETWORK]",
        "api": "[LINK3]",
        "request": "[SATELLITE_ANTENNA2]",

        # æ•°æ®å¤„ç†
        "data": "[RELOAD]",
        "convert": "[RELOAD]",
        "process": "[GEAR2]",
        "analyze": "[BAR_CHART2]",

        # AIç›¸å…³
        "ai": "ğŸ¤–",
        "chat": "ğŸ’¬",
        "llm": "[BRAIN]",
        "intelligent": "[BRAIN]",

        # MCPç›¸å…³
        "mcp": "[WRENCH2]",
        "server": "[DESKTOP_COMPUTER]",

        # å…¶ä»–
        "tool": "[HAMMER_WRENCH2]",
        "utility": "[GEAR2]"
    }

    tool_lower = tool_name.lower()

    # æ ¹æ®å·¥å…·åç§°å…³é”®è¯åŒ¹é…å›¾æ ‡
    for keyword, icon in icon_map.items():
        if keyword in tool_lower:
            return icon

    # é»˜è®¤å›¾æ ‡
    return "[GEAR2]"

# POSTç³»ç»Ÿå·¥å…·ç«¯ç‚¹å·²ç§»é™¤ - MCPåŠŸèƒ½ä¸å¯ç”¨

@app.get("/mcp-platforms/status")
async def get_mcp_platforms_status():
    """è¿”å›MCPæ‰˜ç®¡å¹³å°çŠ¶æ€"""
    # è¿”å›é»˜è®¤çš„å¹³å°çŠ¶æ€
    platforms = [
        {
            "id": "modao",
            "name": "å¢¨åˆ€ç¤¾åŒº",
            "status": "disconnected",
            "description": "ä»å¢¨åˆ€ç¤¾åŒºåŒæ­¥ MCP æœåŠ¡"
        },
        {
            "id": "smithery",
            "name": "Smithery",
            "status": "disconnected",
            "description": "ä» Smithery åŒæ­¥ MCP æœåŠ¡"
        },
        {
            "id": "aliyun",
            "name": "é˜¿é‡Œäº‘å‡½æ•°è®¡ç®—",
            "status": "disconnected",
            "description": "ä»é˜¿é‡Œäº‘å‡½æ•°è®¡ç®—åŒæ­¥ MCP æœåŠ¡"
        },
        {
            "id": "tencent",
            "name": "è…¾è®¯äº‘å‡½æ•°",
            "status": "disconnected",
            "description": "ä»è…¾è®¯äº‘å‡½æ•°åŒæ­¥ MCP æœåŠ¡"
        }
    ]
    return platforms

@app.post("/mcp-platforms/{platform_id}/config")
async def configure_mcp_platform(platform_id: str, config_data: dict):
    """é…ç½®MCPæ‰˜ç®¡å¹³å°"""
    print(f"æ”¶åˆ°å¹³å°é…ç½®è¯·æ±‚: {platform_id}, æ•°æ®: {config_data}")

    if platform_id == "tencent_scf":
        # éªŒè¯è…¾è®¯äº‘é…ç½®
        required_fields = ["secret_id", "secret_key", "region"]
        for field in required_fields:
            if field not in config_data or not config_data[field]:
                return {"error": f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}"}, 400

        # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„è…¾è®¯äº‘è¿æ¥æµ‹è¯•é€»è¾‘
        print(f"é…ç½®è…¾è®¯äº‘å‡½æ•°è®¡ç®—: Secret ID: {config_data['secret_id'][:8]}..., Region: {config_data['region']}")

        # æ¨¡æ‹Ÿé…ç½®æˆåŠŸ
        return {
            "status": "success",
            "message": "è…¾è®¯äº‘å‡½æ•°è®¡ç®—é…ç½®æˆåŠŸ",
            "platform_id": platform_id,
            "config": {
                "region": config_data["region"],
                "secret_id_masked": config_data["secret_id"][:8] + "..."
            }
        }

    elif platform_id == "aliyun":
        # é˜¿é‡Œäº‘é…ç½®é€»è¾‘
        return {"status": "success", "message": "é˜¿é‡Œäº‘é…ç½®æˆåŠŸ"}

    elif platform_id == "modao":
        # å¢¨åˆ€é…ç½®é€»è¾‘
        return {"status": "success", "message": "å¢¨åˆ€é…ç½®æˆåŠŸ"}

    else:
        return {"error": f"ä¸æ”¯æŒçš„å¹³å°: {platform_id}"}, 400

@app.post("/api/mcp/sync/{platform_type}")
async def sync_mcp_platform(platform_type: str, account_config: dict):
    """åŒæ­¥MCPå¹³å°å·¥å…·"""
    print(f"æ”¶åˆ°åŒæ­¥è¯·æ±‚: {platform_type}, é…ç½®: {account_config}")

    try:
        if platform_type == "tencent_scf":
            # å¯¼å…¥è…¾è®¯äº‘å¹³å°é€‚é…å™¨
            from integrations.mcp_platforms.tencent_scf import TencentSCFPlatform
            from integrations.mcp_platforms.base import PlatformConfig

            # åˆ›å»ºå¹³å°é…ç½®
            platform_config = PlatformConfig(
                platform_id="tencent_scf",
                platform_name="è…¾è®¯äº‘å‡½æ•°è®¡ç®—",
                base_url="https://scf.tencentcloudapi.com",
                region=account_config.get("config", {}).get("region", "ap-guangzhou"),
                timeout=30,
                enabled=True,
                extra_config=account_config.get("config", {})
            )

            # åˆ›å»ºå¹³å°å®ä¾‹å¹¶åŒæ­¥
            platform = TencentSCFPlatform(platform_config)

            # æµ‹è¯•è¿æ¥
            connection_ok = await platform.test_connection()
            if not connection_ok:
                return {"error": "å¹³å°è¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®"}, 400

            # è·å–æœåŠ¡åˆ—è¡¨
            services = await platform.list_services()

            # è½¬æ¢ä¸ºå‰ç«¯éœ€è¦çš„æ ¼å¼
            synced_tools = []
            for service in services:
                synced_tools.append({
                    "id": service.id,
                    "name": service.display_name,
                    "description": service.description,
                    "category": service.metadata.get("category", "å…¶ä»–"),
                    "source": "è…¾è®¯äº‘å¼€å‘è€…ç¤¾åŒº",
                    "transport": service.metadata.get("transport", "http"),
                    "platform": "tencent_scf",
                    "syncedAt": "2024-01-01T00:00:00.000Z",
                    "endpoint": service.endpoint,
                    "metadata": service.metadata
                })

            await platform.close()

            return synced_tools

        elif platform_type == "modelscope":
            # é­”æ­ç¤¾åŒºåŒæ­¥é€»è¾‘
            return []

        else:
            return {"error": f"ä¸æ”¯æŒçš„å¹³å°ç±»å‹: {platform_type}"}, 400

    except Exception as e:
        print(f"åŒæ­¥å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"error": f"åŒæ­¥å¤±è´¥: {str(e)}"}, 500

# --- ä½œå“å­˜æ”¾è®¾ç½®API ---
@app.get("/api/workspace-settings")
async def get_workspace_settings():
    """è·å–ç”¨æˆ·çš„ä½œå“å­˜æ”¾è®¾ç½®"""
    try:
        settings_file = "memory/workspace_settings.json"
        if os.path.exists(settings_file):
            with open(settings_file, 'r', encoding='utf-8') as f:
                settings = json.load(f)
            return settings
        else:
            # è¿”å›é»˜è®¤è®¾ç½®
            default_settings = {
                "defaultOutputPath": "C:\\Users\\ç”¨æˆ·å\\Documents\\AIå·¥ä½œæµä½œå“\\",
                "enableDateFolders": True,
                "enableProjectFolders": True,
                "fileNamingPattern": "document_{timestamp}_{type}",
                "fileTypes": {
                    "documents": "\\Documents\\",
                    "data": "\\Data\\",
                    "images": "\\Images\\",
                    "media": "\\Media\\"
                },
                "cloudSync": {
                    "enabled": False,
                    "provider": "none",
                    "syncPath": ""
                }
            }
            return default_settings
    except Exception as e:
        logging.error(f"è·å–ä½œå“å­˜æ”¾è®¾ç½®å¤±è´¥: {e}")
        return {"error": str(e)}, 500

@app.post("/api/workspace-settings")
async def save_workspace_settings(settings: dict):
    """ä¿å­˜ç”¨æˆ·çš„ä½œå“å­˜æ”¾è®¾ç½®"""
    try:
        # ç¡®ä¿memoryç›®å½•å­˜åœ¨
        os.makedirs("memory", exist_ok=True)

        settings_file = "memory/workspace_settings.json"
        with open(settings_file, 'w', encoding='utf-8') as f:
            json.dump(settings, f, ensure_ascii=False, indent=2)

        logging.info(f"ä½œå“å­˜æ”¾è®¾ç½®å·²ä¿å­˜: {settings}")
        return {"success": True, "message": "è®¾ç½®ä¿å­˜æˆåŠŸ"}
    except Exception as e:
        logging.error(f"ä¿å­˜ä½œå“å­˜æ”¾è®¾ç½®å¤±è´¥: {e}")
        return {"error": str(e)}, 500

def get_output_path(file_type: str = "document", project_name: str = None) -> str:
    """æ ¹æ®ç”¨æˆ·è®¾ç½®ç”Ÿæˆè¾“å‡ºè·¯å¾„"""
    try:
        settings_file = "memory/workspace_settings.json"
        if os.path.exists(settings_file):
            with open(settings_file, 'r', encoding='utf-8') as f:
                settings = json.load(f)
        else:
            # ä½¿ç”¨é»˜è®¤è®¾ç½®
            settings = {
                "defaultOutputPath": "C:\\Users\\ç”¨æˆ·å\\Documents\\AIå·¥ä½œæµä½œå“\\",
                "enableDateFolders": True,
                "enableProjectFolders": True,
                "fileTypes": {
                    "documents": "\\Documents\\",
                    "data": "\\Data\\",
                    "images": "\\Images\\",
                    "media": "\\Media\\"
                }
            }

        # æ„å»ºåŸºç¡€è·¯å¾„
        base_path = settings.get("defaultOutputPath", "C:\\Users\\ç”¨æˆ·å\\Documents\\AIå·¥ä½œæµä½œå“\\")

        # æ·»åŠ æ—¥æœŸæ–‡ä»¶å¤¹
        if settings.get("enableDateFolders", True):
            from datetime import datetime
            date_folder = datetime.now().strftime("%Y\\%m\\%d")
            base_path = os.path.join(base_path, date_folder)

        # æ·»åŠ é¡¹ç›®æ–‡ä»¶å¤¹
        if settings.get("enableProjectFolders", True) and project_name:
            base_path = os.path.join(base_path, project_name)

        # æ·»åŠ æ–‡ä»¶ç±»å‹æ–‡ä»¶å¤¹
        file_type_mapping = {
            "document": "documents",
            "data": "data",
            "image": "images",
            "media": "media"
        }

        type_key = file_type_mapping.get(file_type, "documents")
        type_folder = settings.get("fileTypes", {}).get(type_key, "\\Documents\\")
        if type_folder:
            base_path = os.path.join(base_path, type_folder.strip("\\"))

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(base_path, exist_ok=True)

        return base_path

    except Exception as e:
        logging.error(f"ç”Ÿæˆè¾“å‡ºè·¯å¾„å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤è·¯å¾„
        default_path = os.path.join(os.path.expanduser("~"), "Documents", "AIå·¥ä½œæµä½œå“")
        os.makedirs(default_path, exist_ok=True)
        return default_path

@app.on_event("startup")
async def startup_event():
    """åº”ç”¨å¯åŠ¨äº‹ä»¶"""
    logging.info("Starting Workflow Platform...")

    # å¯åŠ¨ç›‘æ§
    try:
        await metrics_collector.start_collection()
        await alert_manager.start_monitoring()
        logging.info("Monitoring services started")
    except Exception as e:
        logging.error(f"Failed to start monitoring: {e}")


# AIæè¿°æ€»ç»“ç›¸å…³æ¨¡å‹
class DescriptionSummaryRequest(BaseModel):
    description: str
    maxLength: int = 20

class DescriptionSummaryResponse(BaseModel):
    summary: str
    originalLength: int
    summaryLength: int

@app.get("/api/ai/services")
async def get_ai_services():
    """è·å–AIæœåŠ¡å·¥å…·æ•°æ®"""
    try:
        from utils.frontend_config import load_config, AI_TOOLS_CONFIG
        ai_tools = load_config(AI_TOOLS_CONFIG)

        # ç»Ÿè®¡æ•°æ®
        configured_services = len([tool for tool in ai_tools if tool.get("enabled", True)])
        active_connections = len([tool for tool in ai_tools if tool.get("enabled", True) and tool.get("tested", False)])
        supported_providers = 7  # æ”¯æŒçš„AIæä¾›å•†æ•°é‡

        # å¤„ç†å·¥å…·æ•°æ®ï¼Œæ·»åŠ åŠŸèƒ½åˆ†ç±»
        processed_tools = []
        for tool in ai_tools:
            if tool.get("enabled", True):
                # æ ¹æ®å·¥å…·ç‰¹æ€§è‡ªåŠ¨åˆ†é…åŠŸèƒ½åˆ†ç±»
                functional_category = tool.get("functionalCategory", "ai_assistant")
                if not functional_category or functional_category == "ai_assistant":
                    # æ ¹æ®å·¥å…·åç§°å’Œæè¿°è‡ªåŠ¨åˆ†ç±»
                    name_desc = (tool.get("name", "") + " " + tool.get("description", "")).lower()
                    if "æ™ºèƒ½æ¨¡å¼" in name_desc or "æ£€æµ‹" in name_desc:
                        functional_category = "ai_assistant"
                    elif "èŠå¤©" in name_desc or "å¯¹è¯" in name_desc or "deepseek" in name_desc:
                        functional_category = "ai_assistant"
                    else:
                        functional_category = "ai_assistant"

                processed_tools.append({
                    "name": tool.get("display_name", tool.get("name", "AIå·¥å…·")),
                    "description": tool.get("description", "AIæœåŠ¡å·¥å…·"),
                    "functionalCategory": functional_category,
                    "icon": "ğŸ¤–",
                    "enabled": tool.get("enabled", True),
                    "source": "ai_config"
                })

        return {
            "status": "success",
            "data": {
                "configuredServices": configured_services,
                "activeConnections": active_connections,
                "supportedProviders": supported_providers,
                "tools": processed_tools
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–AIæœåŠ¡å¤±è´¥: {str(e)}",
            "data": {
                "configuredServices": 0,
                "activeConnections": 0,
                "supportedProviders": 0,
                "tools": []
            }
        }

@app.get("/api/mcp/tools")
@app.get("/api/tools/mcp-tools")
async def get_mcp_tools():
    """è·å–MCPå·¥å…·æ•°æ®"""
    try:
        from utils.frontend_config import load_config, MCP_TOOLS_CONFIG

        # åŠ è½½MCPå·¥å…·é…ç½®ï¼Œä¸ä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
        mcp_tools = load_config(MCP_TOOLS_CONFIG)

        # ç»Ÿè®¡æ•°æ®
        all_tools = len(mcp_tools)
        enabled_tools = len([tool for tool in mcp_tools if tool.get("enabled", True)])

        # å¤„ç†å·¥å…·æ•°æ®ï¼Œæ·»åŠ åŠŸèƒ½åˆ†ç±»
        processed_tools = []
        for tool in mcp_tools:
            if tool.get("enabled", True):
                # æ ¹æ®å·¥å…·ç‰¹æ€§è‡ªåŠ¨åˆ†é…åŠŸèƒ½åˆ†ç±»
                functional_category = tool.get("functionalCategory", "automation")
                if not functional_category:
                    # æ ¹æ®å·¥å…·åç§°å’Œæè¿°è‡ªåŠ¨åˆ†ç±»
                    name_desc = (tool.get("name", "") + " " + tool.get("description", "")).lower()
                    if "æ–‡ä»¶" in name_desc:
                        functional_category = "file_management"
                    elif "æ•°æ®" in name_desc or "åˆ†æ" in name_desc:
                        functional_category = "data_analysis"
                    else:
                        functional_category = "automation"

                processed_tools.append({
                    "name": tool.get("name", "MCPå·¥å…·"),
                    "description": tool.get("description", "MCPæœåŠ¡å·¥å…·"),
                    "functionalCategory": functional_category,
                    "icon": "[ELECTRIC_PLUG]",
                    "enabled": tool.get("enabled", True),
                    "source": "mcp"
                })

        # å…¼å®¹å‰ç«¯MCPConnectionsPageæœŸæœ›çš„æ ¼å¼
        return {
            "tools": processed_tools,
            "total": len(processed_tools),
            "message": "MCPå·¥å…·æ•°æ®åŠ è½½æˆåŠŸ"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–MCPå·¥å…·å¤±è´¥: {str(e)}",
            "data": {
                "allTools": 0,
                "myTools": 0,
                "platformTools": 0,
                "approvedTools": 0,
                "pendingTools": 0,
                "tools": []
            }
        }

@app.get("/api/api/tools")
@app.get("/api/api-tools")
async def get_api_tools():
    """è·å–APIå·¥å…·æ•°æ®"""
    try:
        from utils.frontend_config import load_config, API_TOOLS_CONFIG
        api_tools = load_config(API_TOOLS_CONFIG)

        # ç»Ÿè®¡æ•°æ®
        total_tools = len(api_tools)
        running_tools = len([tool for tool in api_tools if tool.get("enabled", True)])
        supported_types = 7  # æ”¯æŒçš„APIç±»å‹æ•°é‡

        # å¤„ç†å·¥å…·æ•°æ®ï¼Œæ·»åŠ åŠŸèƒ½åˆ†ç±»
        processed_tools = []
        for tool in api_tools:
            if tool.get("enabled", True):
                # æ ¹æ®å·¥å…·ç‰¹æ€§è‡ªåŠ¨åˆ†é…åŠŸèƒ½åˆ†ç±»
                functional_category = tool.get("functionalCategory", "web_services")
                if not functional_category or functional_category == "web_services":
                    # æ ¹æ®å·¥å…·åç§°å’Œæè¿°è‡ªåŠ¨åˆ†ç±»
                    name_desc = (tool.get("name", "") + " " + tool.get("description", "")).lower()
                    if "http" in name_desc or "è¯·æ±‚" in name_desc:
                        functional_category = "network_communication"
                    elif "æ–‡ä»¶" in name_desc or "å¤„ç†" in name_desc:
                        functional_category = "file_management"
                    elif "æ•°æ®" in name_desc or "åˆ†æ" in name_desc:
                        functional_category = "data_processing"
                    else:
                        functional_category = "network_communication"

                processed_tools.append({
                    "name": tool.get("name", "APIå·¥å…·"),
                    "description": tool.get("description", "APIæœåŠ¡å·¥å…·"),
                    "functionalCategory": functional_category,
                    "icon": "[LINK3]",
                    "enabled": tool.get("enabled", True),
                    "source": "api_config"
                })

        # å…¼å®¹å‰ç«¯APIToolsPageæœŸæœ›çš„æ ¼å¼
        return {
            "tools": processed_tools,
            "total": len(processed_tools),
            "message": "APIå·¥å…·æ•°æ®åŠ è½½æˆåŠŸ"
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–APIå·¥å…·å¤±è´¥: {str(e)}",
            "data": {
                "apiTools": 0,
                "runningTools": 0,
                "supportedTypes": 0,
                "tools": []
            }
        }

@app.get("/api/system/tools")
async def get_system_tools():
    """è·å–ç³»ç»Ÿå·¥å…·æ•°æ®"""
    try:
        from utils.frontend_config import load_config
        from pathlib import Path

        # åŠ è½½ç³»ç»Ÿå·¥å…·é…ç½®æ–‡ä»¶
        PROJECT_ROOT = Path(__file__).parent.parent.parent
        SYSTEM_TOOLS_CONFIG = PROJECT_ROOT / "data" / "configs" / "backend-config" / "system_tools.json"

        system_tools = load_config(SYSTEM_TOOLS_CONFIG)

        # å¤„ç†å·¥å…·æ•°æ®ï¼Œæ·»åŠ åŠŸèƒ½åˆ†ç±»
        processed_tools = []
        for tool in system_tools:
            if tool.get("enabled", True):
                # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„åŠŸèƒ½åˆ†ç±»ï¼Œå¦‚æœæ²¡æœ‰åˆ™æ ¹æ®åç§°è‡ªåŠ¨åˆ†ç±»
                functional_category = tool.get("functionalCategory", "system_tools")
                if not functional_category or functional_category == "system_tools":
                    # æ ¹æ®å·¥å…·åç§°å’Œæè¿°è‡ªåŠ¨åˆ†ç±»
                    name_desc = (tool.get("name", "") + " " + tool.get("description", "")).lower()
                    if "æ–‡ä»¶" in name_desc and "ç®¡ç†" in name_desc:
                        functional_category = "file_management"
                    elif "ç½‘ç»œ" in name_desc or "ping" in name_desc:
                        functional_category = "network_communication"
                    elif "æ–‡æœ¬" in name_desc or "å¤„ç†" in name_desc:
                        functional_category = "document_processing"
                    elif "æ—¶é—´" in name_desc:
                        functional_category = "system_tools"
                    elif "ç³»ç»Ÿ" in name_desc and "ä¿¡æ¯" in name_desc:
                        functional_category = "system_tools"
                    else:
                        functional_category = "system_tools"

                processed_tools.append({
                    "name": tool.get("name", "ç³»ç»Ÿå·¥å…·"),
                    "description": tool.get("description", "ç³»ç»ŸæœåŠ¡å·¥å…·"),
                    "functionalCategory": functional_category,
                    "icon": tool.get("icon", "[HAMMER_WRENCH2]"),
                    "enabled": tool.get("enabled", True),
                    "source": "system"
                })

        return {
            "status": "success",
            "data": {
                "tools": processed_tools
            }
        }
    except Exception as e:
        return {
            "status": "error",
            "message": f"è·å–ç³»ç»Ÿå·¥å…·å¤±è´¥: {str(e)}",
            "data": {
                "tools": []
            }
        }

@app.post("/api/ai/summarize-description", response_model=DescriptionSummaryResponse)
async def summarize_description(request: DescriptionSummaryRequest):
    """ä½¿ç”¨AIæ€»ç»“å·¥å…·æè¿°ä¸º50å­—ä»¥å†…çš„ç®€æ´æè¿°"""
    try:
        original_desc = request.description.strip()
        max_length = min(request.maxLength, 50)  # ç¡®ä¿ä¸è¶…è¿‡50å­—

        if not original_desc:
            return DescriptionSummaryResponse(
                summary="",
                originalLength=0,
                summaryLength=0
            )

        # å¦‚æœåŸæè¿°å·²ç»å¾ˆçŸ­ï¼Œç›´æ¥è¿”å›
        if len(original_desc) <= max_length:
            return DescriptionSummaryResponse(
                summary=original_desc,
                originalLength=len(original_desc),
                summaryLength=len(original_desc)
            )

        # ä½¿ç”¨AIæ€»ç»“
        summary = await _ai_summarize_description(original_desc, max_length)

        return DescriptionSummaryResponse(
            summary=summary,
            originalLength=len(original_desc),
            summaryLength=len(summary)
        )

    except Exception as e:
        logging.error(f"AIæ€»ç»“æè¿°å¤±è´¥: {e}")
        # é™çº§å¤„ç†
        fallback_summary = _fallback_summarize(request.description, request.maxLength)
        return DescriptionSummaryResponse(
            summary=fallback_summary,
            originalLength=len(request.description),
            summaryLength=len(fallback_summary)
        )

async def _ai_summarize_description(description: str, max_length: int = 80) -> str:
    """ä½¿ç”¨AIæ€»ç»“æè¿°"""
    try:
        # æ£€æŸ¥æ˜¯å¦æœ‰OpenAI API Key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logging.warning("æœªé…ç½®OPENAI_API_KEYï¼Œä½¿ç”¨æ™ºèƒ½é™çº§æ€»ç»“æ–¹æ¡ˆ")
            return _intelligent_fallback_summarize(description, max_length)

        # æ„å»ºä¸“ä¸šçš„å·¥å…·æè¿°æ€»ç»“æç¤º
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·æè¿°æ€»ç»“ä¸“å®¶ã€‚è¯·å°†ä»¥ä¸‹å·¥å…·æè¿°æ€»ç»“ä¸º{max_length}å­—ä»¥å†…çš„ç²¾å‡†æè¿°ã€‚

åŸå§‹æè¿°ï¼š
{description}

æ€»ç»“è¦æ±‚ï¼š
1. å­—æ•°é™åˆ¶ï¼šä¸è¶…è¿‡{max_length}ä¸ªå­—ç¬¦
2. æ ¸å¿ƒä¿¡æ¯ï¼šå¿…é¡»ä¿ç•™å·¥å…·çš„ä¸»è¦åŠŸèƒ½å’Œç”¨é€”
3. å…³é”®ç‰¹æ€§ï¼šæå–æœ€é‡è¦çš„2-3ä¸ªç‰¹è‰²åŠŸèƒ½
4. é€‚ç”¨åœºæ™¯ï¼šç®€è¦è¯´æ˜ä¸»è¦åº”ç”¨é¢†åŸŸ
5. è¯­è¨€é£æ ¼ï¼šç®€æ´ä¸“ä¸šï¼Œä¾¿äºç³»ç»ŸåŒ¹é…

æ€»ç»“æ ¼å¼ï¼š[å·¥å…·ç±»å‹]ï¼š[æ ¸å¿ƒåŠŸèƒ½]ï¼Œ[å…³é”®ç‰¹æ€§]ï¼Œé€‚ç”¨äº[ä¸»è¦åœºæ™¯]

è¯·ç›´æ¥è¿”å›æ€»ç»“ç»“æœï¼Œä¸è¦åŒ…å«å…¶ä»–å†…å®¹ï¼š"""

        # ä½¿ç”¨OpenAI APIè¿›è¡Œæ€»ç»“
        client = AsyncOpenAI(api_key=api_key)

        response = await client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å·¥å…·æè¿°æ€»ç»“ä¸“å®¶ï¼Œæ“…é•¿æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆç®€æ´å‡†ç¡®çš„æè¿°ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=120,
            temperature=0.2
        )

        summary = response.choices[0].message.content.strip()

        # æ¸…ç†å¯èƒ½çš„æ ¼å¼æ ‡è®°
        summary = summary.replace("æ€»ç»“ï¼š", "").replace("æ€»ç»“ç»“æœï¼š", "").strip()

        # ç¡®ä¿ä¸è¶…è¿‡æœ€å¤§é•¿åº¦
        if len(summary) > max_length:
            # æ™ºèƒ½æˆªå–ï¼Œå°½é‡ä¿æŒå®Œæ•´æ€§
            summary = _smart_truncate(summary, max_length)

        return summary

    except Exception as e:
        logging.error(f"AIæ€»ç»“å¤±è´¥: {e}")
        return _intelligent_fallback_summarize(description, max_length)

def _intelligent_fallback_summarize(text: str, max_length: int = 80) -> str:
    """æ™ºèƒ½é™çº§æ€»ç»“æ–¹æ¡ˆï¼šåŸºäºè§„åˆ™çš„æ–‡æœ¬å¤„ç†"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦å’Œç‰¹æ®Šç¬¦å·
    cleaned = re.sub(r'\s+', ' ', text.strip())

    # æå–å…³é”®ä¿¡æ¯çš„æ¨¡å¼
    patterns = [
        r'([^ï¼š:]*(?:å·¥å…·|æœåŠ¡|ç³»ç»Ÿ|å¹³å°|åº”ç”¨))[ï¼š:]([^ã€‚ï¼ï¼Ÿ.!?]*)',  # å·¥å…·ç±»å‹ï¼šåŠŸèƒ½æè¿°
        r'(æ”¯æŒ|æä¾›|åŒ…æ‹¬|å…·æœ‰)([^ã€‚ï¼ï¼Ÿ.!?]*)',  # åŠŸèƒ½æè¿°
        r'(é€‚ç”¨äº|ç”¨äº|é¢å‘)([^ã€‚ï¼ï¼Ÿ.!?]*)',  # åº”ç”¨åœºæ™¯
    ]

    extracted_info = []

    # å°è¯•æå–ç»“æ„åŒ–ä¿¡æ¯
    for pattern in patterns:
        matches = re.findall(pattern, cleaned)
        for match in matches:
            if isinstance(match, tuple):
                info = ''.join(match).strip()
                if info and len(info) > 5:  # è¿‡æ»¤å¤ªçŸ­çš„ä¿¡æ¯
                    extracted_info.append(info)

    # å¦‚æœæå–åˆ°ç»“æ„åŒ–ä¿¡æ¯ï¼Œç»„åˆä½¿ç”¨
    if extracted_info:
        summary = 'ã€'.join(extracted_info[:3])  # æœ€å¤š3ä¸ªå…³é”®ä¿¡æ¯
        if len(summary) <= max_length:
            return summary
        else:
            return _smart_truncate(summary, max_length)

    # å¦åˆ™ä½¿ç”¨æ”¹è¿›çš„å¥å­æå–
    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', cleaned)

    # æ‰¾åˆ°æœ€æœ‰ä¿¡æ¯é‡çš„å¥å­
    best_sentence = ""
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) > len(best_sentence) and len(sentence) <= max_length:
            # ä¼˜å…ˆé€‰æ‹©åŒ…å«å…³é”®è¯çš„å¥å­
            if any(keyword in sentence for keyword in ['å·¥å…·', 'æ”¯æŒ', 'åŠŸèƒ½', 'ç”¨äº', 'é€‚ç”¨']):
                best_sentence = sentence

    if best_sentence:
        return best_sentence

    # æœ€åçš„é™çº§æ–¹æ¡ˆï¼šæ™ºèƒ½æˆªå–
    return _smart_truncate(cleaned, max_length)

def _smart_truncate(text: str, max_length: int) -> str:
    """æ™ºèƒ½æˆªå–æ–‡æœ¬ï¼Œå°½é‡ä¿æŒå®Œæ•´æ€§"""
    if len(text) <= max_length:
        return text

    # å°è¯•åœ¨æ ‡ç‚¹ç¬¦å·å¤„æˆªå–
    punctuation_marks = ['ï¼Œ', 'ã€', 'ï¼›', ',', ';', ' ']

    for i in range(max_length - 1, max_length // 2, -1):
        if text[i] in punctuation_marks:
            return text[:i]

    # å¦‚æœæ²¡æœ‰åˆé€‚çš„æ ‡ç‚¹ç¬¦å·ï¼Œç›´æ¥æˆªå–
    return text[:max_length]

def _fallback_summarize(text: str, max_length: int = 50) -> str:
    """åŸå§‹é™çº§æ€»ç»“æ–¹æ¡ˆï¼šç®€å•çš„æ–‡æœ¬å¤„ç†ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
    if not text:
        return ""

    # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
    cleaned = text.strip().replace(r'\s+', ' ')

    # å°è¯•æ‰¾åˆ°ç¬¬ä¸€å¥è¯
    first_sentence = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', cleaned)[0]

    # å¦‚æœç¬¬ä¸€å¥è¯åˆé€‚ï¼Œä½¿ç”¨ç¬¬ä¸€å¥è¯
    if len(first_sentence) <= max_length and len(first_sentence) > 0:
        return first_sentence

    # å¦åˆ™æˆªå–å‰max_lengthä¸ªå­—ç¬¦
    return cleaned[:max_length]

@app.on_event("shutdown")
async def shutdown_event():
    """åº”ç”¨å…³é—­äº‹ä»¶"""
    logging.info("Shutting down Workflow Platform...")

    # åœæ­¢ç›‘æ§
    try:
        await metrics_collector.stop_collection()
        await alert_manager.stop_monitoring()
        logging.info("Monitoring services stopped")
    except Exception as e:
        logging.error(f"Failed to stop monitoring: {e}")


# --- WebSocketç«¯ç‚¹ ---
# å¯¼å…¥WebSocketç®¡ç†å™¨
from utils.websocket_manager import websocket_manager

@app.websocket("/ws/tools")
async def websocket_endpoint(websocket: WebSocket):
    """å·¥å…·åº“å®æ—¶åŒæ­¥WebSocketç«¯ç‚¹"""
    try:
        await websocket_manager.connect(websocket)

        # ä¿æŒè¿æ¥æ´»è·ƒ
        while True:
            try:
                # ç­‰å¾…å®¢æˆ·ç«¯æ¶ˆæ¯ï¼ˆå¿ƒè·³åŒ…ç­‰ï¼‰
                data = await websocket.receive_text()
                message = json.loads(data)

                # å¤„ç†å¿ƒè·³åŒ…
                if message.get("type") == "ping":
                    await websocket.send_text(json.dumps({
                        "type": "pong",
                        "timestamp": datetime.now().isoformat()
                    }))

            except WebSocketDisconnect:
                break
            except Exception as e:
                logger.error(f"WebSocketå¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
                break

    except Exception as e:
        logger.error(f"WebSocketè¿æ¥å¤±è´¥: {e}")
    finally:
        websocket_manager.disconnect(websocket)

@app.get("/api/websocket/status")
async def get_websocket_status():
    """è·å–WebSocketè¿æ¥çŠ¶æ€"""
    return {
        "active_connections": websocket_manager.get_connection_count(),
        "connections": websocket_manager.get_connection_info()
    }

@app.post("/api/websocket/test-notification")
async def test_websocket_notification():
    """æµ‹è¯•WebSocketé€šçŸ¥åŠŸèƒ½"""
    try:
        from utils.websocket_manager import notify_api_tools_updated, notify_tools_library_refresh

        # å‘é€æµ‹è¯•é€šçŸ¥
        await notify_api_tools_updated("updated", {
            "id": "test_tool",
            "name": "æµ‹è¯•å·¥å…·",
            "description": "è¿™æ˜¯ä¸€ä¸ªWebSocketæµ‹è¯•é€šçŸ¥"
        })

        await notify_tools_library_refresh("manual_test")

        return {
            "success": True,
            "message": "æµ‹è¯•é€šçŸ¥å·²å‘é€",
            "active_connections": websocket_manager.get_connection_count()
        }
    except Exception as e:
        return {
            "success": False,
            "error": str(e)
        }

# AIå·¥å…·é¡µé¢ä¸“ç”¨è·¯ç”±
@app.get("/ai-tools")
async def get_ai_tools_for_page():
    """è·å–AIå·¥å…·æ•°æ® - ç”¨äºAIConnectionsPage"""
    try:
        from utils.frontend_config import load_config, AI_TOOLS_CONFIG
        ai_tools = load_config(AI_TOOLS_CONFIG)
        
        # è¿‡æ»¤å¯ç”¨çš„å·¥å…·å¹¶æ ¼å¼åŒ–æ•°æ®
        processed_tools = []
        for tool in ai_tools:
            if tool.get("enabled", True):
                processed_tools.append({
                    "id": tool.get("id", tool.get("name")),
                    "name": tool.get("name", tool.get("display_name", "")),
                    "description": tool.get("description", ""),
                    "provider": tool.get("provider", ""),
                    "model": tool.get("model", ""),
                    "api_key": tool.get("api_key", ""),
                    "base_url": tool.get("base_url", ""),
                    "max_tokens": tool.get("max_tokens", 4000),
                    "temperature": tool.get("temperature", 0.7),
                    "enabled": tool.get("enabled", True),
                    "functionalCategory": tool.get("functionalCategory", "ai_assistant"),
                    "category": tool.get("category", "AIå·¥å…·"),
                    "icon": tool.get("icon", "ğŸ¤–"),
                    "tested": tool.get("tested", False),
                    "approved": tool.get("approved", True),
                    "status": "active" if tool.get("enabled", True) else "inactive"
                })
        
        return processed_tools
        
    except Exception as e:
        logger.error(f"è·å–AIå·¥å…·æ•°æ®å¤±è´¥: {e}")
        return []

# --- Main Execution ---
if __name__ == "__main__":
    config = get_config()
    uvicorn.run(
        app,
        host=config.host,
        port=config.port,
        reload=config.reload,
        workers=config.workers if not config.debug else 1
    )

@app.get("/ai-tools")
async def get_ai_tools_for_page():
    """è·å–AIå·¥å…·æ•°æ® - ç”¨äºAIConnectionsPage"""
    try:
        from utils.frontend_config import load_config, AI_TOOLS_CONFIG
        ai_tools = load_config(AI_TOOLS_CONFIG)
        
        # è¿‡æ»¤å¯ç”¨çš„å·¥å…·å¹¶æ ¼å¼åŒ–æ•°æ®
        processed_tools = []
        for tool in ai_tools:
            if tool.get("enabled", True):
                processed_tools.append({
                    "id": tool.get("id", tool.get("name")),
                    "name": tool.get("name", tool.get("display_name", "")),
                    "description": tool.get("description", ""),
                    "provider": tool.get("provider", ""),
                    "model": tool.get("model", ""),
                    "api_key": tool.get("api_key", ""),
                    "base_url": tool.get("base_url", ""),
                    "max_tokens": tool.get("max_tokens", 4000),
                    "temperature": tool.get("temperature", 0.7),
                    "enabled": tool.get("enabled", True),
                    "functionalCategory": tool.get("functionalCategory", "ai_assistant"),
                    "category": tool.get("category", "AIå·¥å…·"),
                    "icon": tool.get("icon", "ğŸ¤–"),
                    "tested": tool.get("tested", False),
                    "approved": tool.get("approved", True),
                    "status": "active" if tool.get("enabled", True) else "inactive"
                })
        
        return processed_tools
        
    except Exception as e:
        logger.error(f"è·å–AIå·¥å…·æ•°æ®å¤±è´¥: {e}")
        return []

@app.post("/api/template/{template_id}/favorite")
async def toggle_template_favorite(template_id: str, request: Request):
    """æ”¶è—/å–æ¶ˆæ”¶è—æ¨¡æ¿"""
    try:
        data = await request.json()
        user_id = data.get("user_id", "anonymous")
        
        # è¯»å–æ”¶è—æ•°æ®
        favorites_file = Path(__file__).parent.parent / "config" / "template_favorites.json"
        favorites_data = {"favorites": []}
        if favorites_file.exists():
            with open(favorites_file, 'r', encoding='utf-8') as f:
                favorites_data = json.load(f)
        
        # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²ç»æ”¶è—
        existing_favorite = None
        for favorite in favorites_data["favorites"]:
            if favorite.get("template_id") == template_id and favorite.get("user_id") == user_id:
                existing_favorite = favorite
                break
        
        if existing_favorite:
            # å–æ¶ˆæ”¶è—
            favorites_data["favorites"].remove(existing_favorite)
            favorited = False
        else:
            # æ·»åŠ æ”¶è—
            new_favorite = {
                "template_id": template_id,
                "user_id": user_id,
                "created_at": datetime.now().isoformat()
            }
            favorites_data["favorites"].append(new_favorite)
            favorited = True
        
        # ä¿å­˜æ•°æ®
        favorites_file.parent.mkdir(parents=True, exist_ok=True)
        with open(favorites_file, 'w', encoding='utf-8') as f:
            json.dump(favorites_data, f, ensure_ascii=False, indent=2)
        
        # è®¡ç®—æ€»æ”¶è—æ•°
        total_favorites = len([f for f in favorites_data["favorites"] if f.get("template_id") == template_id])
        
        return {
            "success": True,
            "favorited": favorited,
            "total_favorites": total_favorites,
            "message": "æ”¶è—æˆåŠŸ" if favorited else "å–æ¶ˆæ”¶è—æˆåŠŸ"
        }
        
    except Exception as e:
        logger.error(f"æ”¶è—æ“ä½œå¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


@app.get("/api/my-favorites")
async def get_my_favorites():
    """è·å–ç”¨æˆ·æ”¶è—çš„æ¨¡æ¿"""
    try:
        # è¯»å–æ”¶è—æ•°æ®
        favorites_file = Path(__file__).parent.parent / "config" / "template_favorites.json"
        if not favorites_file.exists():
            return {
                "success": True,
                "templates": [],
                "total": 0
            }
        
        with open(favorites_file, 'r', encoding='utf-8') as f:
            favorites_data = json.load(f)
        
        # è¯»å–æ¨¡æ¿æ•°æ®
        templates_file = Path(__file__).parent.parent / "config" / "templates.json"
        templates_data = {"templates": []}
        if templates_file.exists():
            with open(templates_file, 'r', encoding='utf-8') as f:
                templates_data = json.load(f)
        
        # è·å–ç”¨æˆ·æ”¶è—çš„æ¨¡æ¿IDåˆ—è¡¨
        user_id = "anonymous"  # TODO: ä»sessionæˆ–JWTä¸­è·å–çœŸå®ç”¨æˆ·ID
        favorited_template_ids = [
            f.get("template_id") for f in favorites_data.get("favorites", [])
            if f.get("user_id") == user_id
        ]
        
        # ç­›é€‰å‡ºæ”¶è—çš„æ¨¡æ¿
        favorited_templates = [
            template for template in templates_data.get("templates", [])
            if template.get("id") in favorited_template_ids
        ]
        
        return {
            "success": True,
            "templates": favorited_templates,
            "total": len(favorited_templates)
        }
        
    except Exception as e:
        logger.error(f"è·å–æ”¶è—æ¨¡æ¿å¤±è´¥: {str(e)}")
        return {"success": False, "error": str(e)}


# MCPä¸“ç”¨ç«¯ç‚¹å·²ç§»åŠ¨åˆ°ä¸»è¦APIåŒºåŸŸ

# æ·»åŠ ç¼“å­˜æ¸…ç†APIç«¯ç‚¹
@app.post("/api/clear-cache")
async def clear_cache():
    """
    æ¸…é™¤æ‰€æœ‰ç¼“å­˜ï¼šPythonç¼“å­˜ã€å·¥ä½œæµçŠ¶æ€ã€ç»“æœå†å²
    """
    try:
        import shutil
        import os
        import glob
        
        results = {
            "python_cache": False,
            "workflow_states": False,
            "result_history": False,
            "error": None
        }
        
        try:
            # 1. æ¸…é™¤Pythonç¼“å­˜
            cache_dirs = []
            for root, dirs, files in os.walk("backend"):
                for dir_name in dirs:
                    if dir_name == "__pycache__":
                        cache_dir = os.path.join(root, dir_name)
                        cache_dirs.append(cache_dir)
            
            for cache_dir in cache_dirs:
                try:
                    shutil.rmtree(cache_dir)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤: {cache_dir}")
                except:
                    pass
            
            # æ¸…é™¤.pycæ–‡ä»¶
            pyc_files = glob.glob("backend/**/*.pyc", recursive=True)
            for pyc_file in pyc_files:
                try:
                    os.remove(pyc_file)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤: {pyc_file}")
                except:
                    pass
            
            results["python_cache"] = True
            print(f"âœ… [ç¼“å­˜æ¸…ç†] Pythonç¼“å­˜æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ [ç¼“å­˜æ¸…ç†] Pythonç¼“å­˜æ¸…ç†å¤±è´¥: {e}")
        
        try:
            # 2. æ¸…é™¤å·¥ä½œæµçŠ¶æ€
            state_files = glob.glob("backend/workflow_states/*.json")
            for state_file in state_files:
                try:
                    os.remove(state_file)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤å·¥ä½œæµçŠ¶æ€: {state_file}")
                except:
                    pass
            
            # æ¸…é™¤æ ¹ç›®å½•çš„å·¥ä½œæµçŠ¶æ€
            root_state_files = glob.glob("workflow_states/*.json")
            for state_file in root_state_files:
                try:
                    os.remove(state_file)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤æ ¹ç›®å½•å·¥ä½œæµçŠ¶æ€: {state_file}")
                except:
                    pass
            
            results["workflow_states"] = True
            print(f"âœ… [ç¼“å­˜æ¸…ç†] å·¥ä½œæµçŠ¶æ€æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ [ç¼“å­˜æ¸…ç†] å·¥ä½œæµçŠ¶æ€æ¸…ç†å¤±è´¥: {e}")
        
        try:
            # 3. æ¸…é™¤è¾“å‡ºæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
            output_files = glob.glob("backend/output/*.docx")
            for output_file in output_files:
                try:
                    os.remove(output_file)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤è¾“å‡ºæ–‡ä»¶: {output_file}")
                except:
                    pass
                    
            # æ¸…é™¤æ ¹ç›®å½•è¾“å‡ºæ–‡ä»¶
            root_output_files = glob.glob("output/*.docx")
            for output_file in root_output_files:
                try:
                    os.remove(output_file)
                    print(f"ğŸ§¹ [ç¼“å­˜æ¸…ç†] å·²åˆ é™¤æ ¹ç›®å½•è¾“å‡ºæ–‡ä»¶: {output_file}")
                except:
                    pass
            
            results["result_history"] = True
            print(f"âœ… [ç¼“å­˜æ¸…ç†] ç»“æœå†å²æ¸…ç†å®Œæˆ")
            
        except Exception as e:
            print(f"âŒ [ç¼“å­˜æ¸…ç†] ç»“æœå†å²æ¸…ç†å¤±è´¥: {e}")
        
        # 4. æ¸…é™¤å†…å­˜ä¸­çš„çŠ¶æ€ç®¡ç†å™¨
        try:
            if hasattr(app.state, 'workflow_state_manager'):
                # é‡ç½®çŠ¶æ€ç®¡ç†å™¨
                app.state.workflow_state_manager = {}
            print(f"âœ… [ç¼“å­˜æ¸…ç†] å†…å­˜çŠ¶æ€æ¸…ç†å®Œæˆ")
        except Exception as e:
            print(f"âŒ [ç¼“å­˜æ¸…ç†] å†…å­˜çŠ¶æ€æ¸…ç†å¤±è´¥: {e}")
        
        return {
            "success": True,
            "message": "ç¼“å­˜æ¸…ç†å®Œæˆ",
            "details": results
        }
        
    except Exception as e:
        return {
            "success": False,
            "message": f"ç¼“å­˜æ¸…ç†å¤±è´¥: {str(e)}",
            "details": results
        }

# ============================================================================
# ç¼ºå¤±çš„APIè·¯ç”± - ä»complete_mock_backend.pyç§»æ¤
# ============================================================================

# å·¥å…·å¯ç”¨æ€§API
@app.get("/api/tools/available")
async def get_available_tools():
    """è·å–å¯ç”¨å·¥å…·åˆ—è¡¨"""
    return {
        "tools": [
            {
                "id": "document_parser",
                "name": "æ–‡æ¡£è§£æå™¨",
                "description": "è§£æWordæ–‡æ¡£"
            },
            {
                "id": "mindmap_generator", 
                "name": "æ€ç»´å¯¼å›¾ç”Ÿæˆå™¨",
                "description": "ç”Ÿæˆæ€ç»´å¯¼å›¾"
            },
            {
                "id": "workflow_builder",
                "name": "å·¥ä½œæµæ„å»ºå™¨", 
                "description": "æ„å»ºè‡ªåŠ¨åŒ–å·¥ä½œæµ"
            }
        ]
    }

# æ–‡ä»¶å¤¹æ‰«æAPI
@app.get("/api/folder/scan")
async def scan_folder(path: str):
    """æ‰«ææ–‡ä»¶å¤¹ä¸­çš„Wordæ–‡æ¡£"""
    try:
        import os
        import glob
        
        logger.info(f"ğŸ” æ‰«ææ–‡ä»¶å¤¹: {path}")
        
        # æ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨
        if not os.path.exists(path):
            return {"files": [], "message": f"è·¯å¾„ä¸å­˜åœ¨: {path}"}
        
        # æ‰«æWordæ–‡æ¡£
        pattern = os.path.join(path, "*.docx")
        docx_files = glob.glob(pattern)
        
        files = []
        for file_path in docx_files:
            try:
                file_name = os.path.basename(file_path)
                
                # è¿‡æ»¤æ‰Wordä¸´æ—¶æ–‡ä»¶ï¼ˆä»¥~$å¼€å¤´çš„æ–‡ä»¶ï¼‰
                if file_name.startswith('~$'):
                    logger.info(f"âš ï¸ è·³è¿‡Wordä¸´æ—¶æ–‡ä»¶: {file_name}")
                    continue
                
                file_stat = os.stat(file_path)
                file_info = {
                    "name": file_name,
                    "path": file_path,
                    "size": file_stat.st_size,
                    "modified": file_stat.st_mtime
                }
                files.append(file_info)
                logger.info(f"ğŸ“„ æ‰¾åˆ°æ–‡æ¡£: {file_info['name']} ({file_info['size']} bytes)")
            except Exception as e:
                logger.warning(f"âš ï¸ æ— æ³•è¯»å–æ–‡ä»¶ä¿¡æ¯: {file_path} - {e}")
        
        result = {
            "files": files,
            "path": path,
            "total": len(files),
            "message": f"æ‰¾åˆ° {len(files)} ä¸ªWordæ–‡æ¡£"
        }
        
        logger.info(f"âœ… æ‰«æå®Œæˆ: {result}")
        return result
        
    except Exception as e:
        logger.error(f"âŒ æ–‡ä»¶å¤¹æ‰«æå¤±è´¥: {e}")
        return {"files": [], "error": str(e), "message": "æ–‡ä»¶å¤¹æ‰«æå¤±è´¥"}

# é€šè¿‡æ–‡ä»¶è·¯å¾„è§£ææ–‡æ¡£çš„API
@app.get("/api/document/parse-path")
async def parse_document_by_path(file_path: str):
    """é€šè¿‡æ–‡ä»¶è·¯å¾„è§£æWordæ–‡æ¡£"""
    logger.info(f"ğŸ” æ”¶åˆ°æ–‡ä»¶è·¯å¾„è§£æè¯·æ±‚: {file_path}")
    
    try:
        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        if not os.path.exists(file_path):
            raise HTTPException(status_code=404, detail=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file_path.endswith('.docx'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒ.docxæ ¼å¼çš„æ–‡ä»¶")
        
        # è·å–æ–‡ä»¶ä¿¡æ¯
        file_stat = os.stat(file_path)
        logger.info(f"ğŸ” æ–‡ä»¶å¤§å°: {file_stat.st_size} bytes")
        
        # å°è¯•å¯¼å…¥docxæ¨¡å—
        try:
            from docx import Document
        except ImportError:
            logger.warning("python-docxæœªå®‰è£…ï¼Œè¿”å›åŸºæœ¬æ–‡ä»¶ä¿¡æ¯")
            return {
                "filename": os.path.basename(file_path),
                "content": [],
                "total_paragraphs": 0,
                "message": "æ–‡æ¡£ä¿¡æ¯è·å–æˆåŠŸï¼ˆéœ€è¦å®‰è£…python-docxè¿›è¡Œè¯¦ç»†è§£æï¼‰",
                "file_size": file_stat.st_size
            }
        
        # è§£ææ–‡ä»¶
        doc = Document(file_path)
        
        content_list = []
        run_id = 0
        
        for para_idx, paragraph in enumerate(doc.paragraphs):
            if paragraph.text.strip():  # åªå¤„ç†éç©ºæ®µè½
                logger.info(f"ğŸ“„ å¤„ç†æ®µè½ {para_idx}: '{paragraph.text[:50]}...'")
                para_content = {
                    "paragraph_id": para_idx,
                    "runs": []
                }
                
                char_offset = 0
                for run in paragraph.runs:
                    if run.text:
                        run_info = {
                            "run_id": run_id,
                            "text": run.text,
                            "font_name": run.font.name if run.font.name else "é»˜è®¤å­—ä½“",
                            "start_char": char_offset,
                            "end_char": char_offset + len(run.text)
                        }
                        para_content["runs"].append(run_info)
                        char_offset += len(run.text)
                        run_id += 1
            
                if para_content["runs"]:
                    content_list.append(para_content)
        
        result = {
            "filename": os.path.basename(file_path),
            "content": content_list,
            "total_paragraphs": len(content_list),
            "message": "æ–‡æ¡£è§£ææˆåŠŸ"
        }
        
        logger.info(f"âœ… æ–‡æ¡£è§£æå®Œæˆ: {len(content_list)} ä¸ªæ®µè½")
        return result
        
    except Exception as e:
        logger.error(f"Error parsing document at path {file_path}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")

# æ–‡æ¡£ä¸Šä¼ è§£æAPI
@app.post("/api/document/parse-upload")
async def parse_uploaded_document(file: UploadFile = File(...)):
    """è§£æç”¨æˆ·ä¸Šä¼ çš„Wordæ–‡æ¡£"""
    logger.info(f"ğŸ” æ”¶åˆ°æ–‡æ¡£è§£æè¯·æ±‚: {file.filename}")
    logger.info(f"ğŸ” æ–‡ä»¶å¤§å°: {file.size} bytes")
    logger.info(f"ğŸ” æ–‡ä»¶ç±»å‹: {file.content_type}")
    
    temp_file_path = None
    try:
        # éªŒè¯æ–‡ä»¶ç±»å‹
        if not file.filename.endswith('.docx'):
            raise HTTPException(status_code=400, detail="åªæ”¯æŒ.docxæ ¼å¼çš„æ–‡ä»¶")
        
        # å°è¯•å¯¼å…¥docxæ¨¡å—
        try:
            from docx import Document
            import tempfile
        except ImportError:
            logger.warning("python-docxæœªå®‰è£…ï¼Œè¿”å›åŸºæœ¬æ–‡ä»¶ä¿¡æ¯")
            content = await file.read()
            return {
                "filename": file.filename,
                "content": [],
                "total_paragraphs": 0,
                "message": "æ–‡æ¡£ä¿¡æ¯è·å–æˆåŠŸï¼ˆéœ€è¦å®‰è£…python-docxè¿›è¡Œè¯¦ç»†è§£æï¼‰",
                "file_size": len(content)
            }
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å¹¶å†™å…¥å†…å®¹
        with tempfile.NamedTemporaryFile(delete=False, suffix='.docx') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file.flush()
            temp_file_path = temp_file.name
        
        # ç°åœ¨æ–‡ä»¶å·²å…³é—­ï¼Œå¯ä»¥å®‰å…¨åœ°ç”¨Documentæ‰“å¼€
        doc = Document(temp_file_path)
        
        content_list = []
        run_id = 0
        
        for para_idx, paragraph in enumerate(doc.paragraphs):
            logger.info(f"ğŸ“„ å¤„ç†æ®µè½ {para_idx}: '{paragraph.text[:50]}...'")
            if paragraph.text.strip():
                para_content = {
                    "paragraph_id": para_idx,
                    "runs": []
                }
                
                char_offset = 0
                for run in paragraph.runs:
                    if run.text:
                        run_info = {
                            "run_id": run_id,
                            "text": run.text,
                            "font_name": run.font.name if run.font.name else "é»˜è®¤å­—ä½“",
                            "start_char": char_offset,
                            "end_char": char_offset + len(run.text)
                        }
                        para_content["runs"].append(run_info)
                        char_offset += len(run.text)
                        run_id += 1
                
                if para_content["runs"]:
                    content_list.append(para_content)
        
        result = {
            "filename": file.filename,
            "content": content_list,
            "total_paragraphs": len(content_list),
            "message": "æ–‡æ¡£è§£ææˆåŠŸ"
        }
        
        logger.info(f"âœ… æ–‡æ¡£è§£æå®Œæˆ: {len(content_list)} ä¸ªæ®µè½")
        return result
        
    except Exception as e:
        logger.error(f"Error parsing uploaded document {file.filename}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ–‡æ¡£è§£æå¤±è´¥: {str(e)}")
    finally:
        # ç¡®ä¿æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.unlink(temp_file_path)
                logger.info(f"ğŸ—‘ï¸ ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {temp_file_path}")
            except Exception as e:
                logger.warning(f"âš ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

